{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T10:55:21.630329Z",
     "start_time": "2023-12-04T10:55:04.468005Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import clip\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import trimesh\n",
    "import time\n",
    "import optuna\n",
    "from torchvision.transforms import Compose, Normalize, Resize, CenterCrop, InterpolationMode\n",
    "import multiprocess\n",
    "import math\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f2c42d0c43ee3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T10:55:24.302469Z",
     "start_time": "2023-12-04T10:55:24.202882Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Number of cpus: ', multiprocess.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e57945907ff5f2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Rendering Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc5fac445bb6f43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T10:55:27.165597Z",
     "start_time": "2023-12-04T10:55:27.140470Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# --- Hyperparameter Setup ---\n",
    "\n",
    "hparams = {\n",
    "    # Camera\n",
    "    \"camera_distance_factor\": 0.45,\n",
    "    \"camera_angle\": 55.,\n",
    "    \"focal_length\": 3.15,\n",
    "    # Image\n",
    "    \"resolution\": 300,\n",
    "    # Phong\n",
    "    \"ambient_coeff\": 0.67,\n",
    "    \"diffuse_coeff\": 0.79,\n",
    "    \"specular_coeff\": 0.63,\n",
    "    \"shininess\": 1.0,\n",
    "    # Colors\n",
    "    \"object_color\": torch.tensor([0.63, 0.17, 0.78]),\n",
    "    \"background_color\": torch.tensor([0.35, 0.94, 0.26]),\n",
    "    # Light\n",
    "    \"amb_light_color\": torch.tensor([0.57, 0.07, 0.69]),\n",
    "    # light 1\n",
    "    \"light_intensity_1\": 2.247, \n",
    "    \"light_color\": torch.tensor([0.88, 0.99, 0.74]), \n",
    "    \"light_dir_1\": torch.tensor([-0.410, -0.506, -0.759]), \n",
    "    # light p\n",
    "    \"light_intensity_p\": 0.,\n",
    "    \"light_radius_p\": 2., \n",
    "    \"light_u1_p\": 0.5,\n",
    "    \"light_u2_p\": 0.5\n",
    "}\n",
    "\n",
    "# --- Rendering Function ---\n",
    "\n",
    "def phong_model(sdf, points, camera_position, phong_params, light_params, mesh_path, index_tri=None):\n",
    "    \n",
    "    # Option 1: Use SDF\n",
    "    #normals = estimate_normals(sdf, points)\n",
    "    # Option 2: Use Mesh\n",
    "    normals = mesh_normals(mesh_path, index_tri)\n",
    "    view_dirs = points - camera_position \n",
    "    light_dir_1 = light_params[\"light_dir_1\"].repeat(points.shape[0], 1) \n",
    "    light_dir_p = points - light_params[\"light_pos_p\"].repeat(points.shape[0], 1)\n",
    "    \n",
    "    # Normalize all vectors\n",
    "    normals = (normals.T / torch.norm(normals, dim=-1)).T\n",
    "    light_dir_norm_1 = (light_dir_1.T / torch.norm(light_dir_1, dim=-1)).T\n",
    "    light_dir_norm_p = (light_dir_p.T / torch.norm(light_dir_p, dim=-1)).T\n",
    "    view_dir_norm = (view_dirs.T / torch.norm(view_dirs, dim=-1)).T\n",
    "    \n",
    "    # Ambient\n",
    "    ambient = phong_params[\"ambient_coeff\"] * light_params[\"amb_light_color\"] \n",
    "    ambient_refl = ambient.repeat(points.shape[0], 1)\n",
    "    \n",
    "    # Area light\n",
    "    diffuse_1 = phong_params[\"diffuse_coeff\"] * torch.clamp(torch.sum(-light_dir_norm_1 * normals, dim=-1), min=0.0) * light_params[\"light_intensity_1\"] # [N]\n",
    "    diffuse_refl_1 = torch.matmul(diffuse_1.unsqueeze(1), light_params[\"light_color_1\"].unsqueeze(0)) # [N, 3]\n",
    "    reflect_dir_1 = light_dir_norm_1 + (2 * normals.T * torch.clamp(torch.sum(-light_dir_norm_1 * normals, dim=-1), min=0.0)).T\n",
    "    specular_1 = phong_params[\"specular_coeff\"] * torch.pow(torch.clamp(torch.sum(reflect_dir_1 * -view_dir_norm, dim=-1), min=0.0), phong_params[\"shininess\"]) * light_params[\"light_intensity_1\"] # [N]\n",
    "    specular_refl_1 = torch.matmul(specular_1.unsqueeze(1), light_params[\"light_color_1\"].unsqueeze(0)) # [N, 3]\n",
    "    \n",
    "    \n",
    "    # Point light\n",
    "    diffuse_p = phong_params[\"diffuse_coeff\"] * torch.clamp(torch.sum(-light_dir_norm_p * normals, dim=-1), min=0.0) * light_params[\"light_intensity_p\"]  # [N]\n",
    "    diffuse_refl_p = torch.matmul(diffuse_p.unsqueeze(1), light_params[\"light_color_p\"].unsqueeze(0)) # [N, 3]\n",
    "    reflect_dir_p = light_dir_norm_p + (2 * normals.T * torch.clamp(torch.sum(-light_dir_norm_p * normals, dim=-1), min=0.0)).T\n",
    "    specular_p = phong_params[\"specular_coeff\"] * torch.pow(torch.clamp(torch.sum(reflect_dir_p * -view_dir_norm, dim=-1), min=0.0), phong_params[\"shininess\"]) * light_params[\"light_intensity_p\"] # [N]\n",
    "    specular_refl_p = torch.matmul(specular_p.unsqueeze(1), light_params[\"light_color_p\"].unsqueeze(0)) # [N, 3]\n",
    "\n",
    "\n",
    "    return ambient_refl + diffuse_refl_1 + specular_refl_1 + diffuse_refl_p + specular_refl_p \n",
    "\n",
    "def estimate_normals(sdf, points, epsilon=1e-3):\n",
    "    sdf_inputs = torch.concat([points,\n",
    "                              points + torch.tensor([epsilon, 0, 0]),\n",
    "                              points + torch.tensor([0, epsilon, 0]),\n",
    "                              points + torch.tensor([0, 0, epsilon])])\n",
    "\n",
    "    sdf_values = sdf(sdf_inputs).reshape(4, -1)\n",
    "\n",
    "    # Calculate the gradient using finite differences\n",
    "    gradient = sdf_values[1:] - sdf_values[0]\n",
    "\n",
    "    # Normalize the gradient to obtain the estimated normal\n",
    "    normal = gradient / torch.norm(gradient, p=2, dim=0)\n",
    "\n",
    "    return normal.T\n",
    "\n",
    "def sphere_trace(sdf, camera_position, norm_directions, max_length):\n",
    "    N = norm_directions.shape[0]\n",
    "    positions = camera_position.unsqueeze(dim=0).repeat(N, 1) # [N, 3]\n",
    "    total_distances = torch.zeros(N)\n",
    "    last_distances = torch.ones(N)\n",
    "\n",
    "    for _ in range(20):\n",
    "        #mask = torch.logical_and(total_distances < max_length, last_distances > 1e-3)\n",
    "        not_reached_max_distance = total_distances < max_length\n",
    "        not_hit_target = torch.abs(last_distances) > 1e-3\n",
    "        mask = torch.logical_and(not_reached_max_distance, not_hit_target)\n",
    "        if torch.all(torch.logical_not(mask)):\n",
    "            break\n",
    "        distances = sdf(positions[mask])\n",
    "        steps = (norm_directions[mask].T * distances).T\n",
    "        positions[mask] += steps\n",
    "        total_distances[mask] += distances\n",
    "        last_distances[mask] = distances\n",
    "\n",
    "    #positions[total_distances > max_length] *= torch.nan\n",
    "    return positions, total_distances < max_length\n",
    "\n",
    "def mesh_trace(mesh_path, ray_starts, ray_directions):\n",
    "    mesh = trimesh.load_mesh(mesh_path)\n",
    "    \n",
    "    ray_origins = ray_starts.repeat(ray_directions.shape[0], 1)\n",
    "    \n",
    "    intersections, index_ray, index_tri = mesh.ray.intersects_location(ray_origins, ray_directions, multiple_hits=False) \n",
    "    \n",
    "    mask = torch.zeros(ray_directions.shape[0], dtype=torch.bool)\n",
    "    mask[index_ray] = True\n",
    "    \n",
    "    points = torch.as_tensor(intersections, dtype=torch.float32)\n",
    "\n",
    "    return points, mask, index_tri\n",
    "\n",
    "\n",
    "def mesh_normals(mesh_path, index_tri):\n",
    "    mesh = trimesh.load_mesh(mesh_path)\n",
    "    all_normals = mesh.face_normals\n",
    "    normals = all_normals[index_tri]\n",
    "    normals_torch = torch.from_numpy(normals).to(dtype=torch.float32)\n",
    "    \n",
    "    return normals_torch\n",
    "\n",
    "def render(model, lat_rep, camera_params, phong_params, light_params, mesh_path=None):\n",
    "    \n",
    "    def sdf(positions):\n",
    "        nphm_input = torch.reshape(positions, (1, -1, 3))\n",
    "        distance, _ = model(nphm_input, torch.reshape(lat_rep, (1, 1, -1)), None)\n",
    "        return distance.squeeze()\n",
    "\n",
    "    pu = camera_params[\"resolution_x\"]\n",
    "    pv = camera_params[\"resolution_y\"]\n",
    "    image = phong_params[\"background_color\"].repeat(pu * pv, 1)\n",
    "    \n",
    "    angle_radians = torch.deg2rad_(torch.tensor(camera_params[\"camera_angle\"]))\n",
    "    camera = torch.tensor([torch.sin(angle_radians), 0, torch.cos(angle_radians)])\n",
    "    camera_position = camera * (camera_params[\"camera_distance\"] + camera_params[\"focal_length\"]) / camera.norm()\n",
    "    \n",
    "    # Normalize the xy value of the current pixel [-0.5, 0.5]\n",
    "    u_norms = ((torch.arange(pu) + 0.5) / pu - 0.5) * pu/pv\n",
    "    v_norms = 0.5 - (torch.arange(pv) + 0.5) / pv\n",
    "\n",
    "    # Calculate the ray directions for all pixels\n",
    "    directions_unn = torch.cat(torch.meshgrid(u_norms, v_norms, torch.tensor(-camera_params[\"focal_length\"]), indexing='ij'), dim=-1) \n",
    "    directions_unn = directions_unn.reshape((pu*pv, 3)) # [pu, pv, 3] --> [pu*pv, 3] (u1, v1, f)(u1, v2, f)...(u2, v1, f)...\n",
    "\n",
    "    # rotate about y-axis\n",
    "    rotation_matrix = torch.tensor([[torch.cos(angle_radians), 0, torch.sin(angle_radians)],\n",
    "                                    [0, 1, 0],\n",
    "                                    [-torch.sin(angle_radians), 0, torch.cos(angle_radians)]])\n",
    "    rotated_directions = torch.matmul(directions_unn, rotation_matrix.T)\n",
    "    \n",
    "    transposed_directions = rotated_directions.T #transpose is necessary for normalization\n",
    "    directions = (transposed_directions / transposed_directions.norm(dim=0)).T # [pu*pv, 3]\n",
    "\n",
    "    # Option 1: Use SDF\n",
    "    #hit_positions, hit_mask = sphere_trace(sdf, camera_position, directions, camera_params['max_ray_length'])\n",
    "    # Option 2: Use Mesh\n",
    "    intersections, hit_mask, index_tri = mesh_trace(mesh_path, camera_position, directions) \n",
    "    \n",
    "    # Option 1: Use SDF\n",
    "    #reflections = phong_model(sdf, hit_positions[hit_mask], camera_position, phong_params, light_params, mesh_path)\n",
    "    # Option 2: Use Mesh\n",
    "    reflections = phong_model(sdf, intersections, camera_position, phong_params, light_params, mesh_path, index_tri) # mesh alternative\n",
    "\n",
    "    # Assign a color for objects\n",
    "    image[hit_mask] = torch.mul(reflections, phong_params[\"object_color\"].repeat(reflections.shape[0], 1))\n",
    "    image = torch.clamp(image, max=1.0)\n",
    "    image = image.reshape(pu, pv, 3).transpose(0, 1)\n",
    "\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dda701d3a74cb2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Prepare Score Calculation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd21bc560b0eefe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T10:55:34.358489Z",
     "start_time": "2023-12-04T10:55:31.099580Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# --- Get Data Annotations ---\n",
    "\n",
    "path_to_head_annotations = '/Users/katharinaschmid/Text2Head/rendering_data/head_annotations.json'\n",
    "with open(path_to_head_annotations) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# --- Prepare Indices ---\n",
    "\n",
    "# Identity: Gender\n",
    "male_indices = [index for index, head in enumerate(data['heads']) if head['gender'] == 'man']\n",
    "female_indices = [index for index, head in enumerate(data['heads']) if head['gender'] == 'woman']\n",
    "\n",
    "# Identity: Age\n",
    "young_indices = [index for index, head in enumerate(data['heads']) if head['age'] == 'young']\n",
    "old_indices = [index for index, head in enumerate(data['heads']) if head['age'] == 'old']\n",
    "\n",
    "# Identity: Ethnicity\n",
    "caucasian_indices = [index for index, head in enumerate(data['heads']) if head['ethnicity'] == 'Caucasian'] \n",
    "asian_indices = [index for index, head in enumerate(data['heads']) if head['ethnicity'] == 'Asian'] \n",
    "\n",
    "# Identity: Hairstyle\n",
    "hat_indices = [index for index, head in enumerate(data['heads']) if head['hairstyle'] == 'hat']\n",
    "ponytail_indices = [index for index, head in enumerate(data['heads']) if head['hairstyle'] == 'ponytail']\n",
    "straight_hair_indices = [index for index, head in enumerate(data['heads']) if head['hairstyle'] == 'straight hair']\n",
    "curly_hair_indices = [index for index, head in enumerate(data['heads']) if head['hairstyle'] == 'curly hair']\n",
    "\n",
    "# Identity: Beard\n",
    "beard_indices = [index for index, head in enumerate(data['heads']) if head['beard'] == 'yes']\n",
    "no_beard_indices = [index for index, head in enumerate(data['heads']) if head['beard'] == 'no']\n",
    "\n",
    "# Expression: Facial Expression\n",
    "open_mouth_indices = [index for index, head in enumerate(data['heads']) if head['facial_expression'] == 'open mouth']\n",
    "smiling_indices = [index for index, head in enumerate(data['heads']) if head['facial_expression'] == 'smiling']\n",
    "closed_eyes_indices = [index for index, head in enumerate(data['heads']) if head['facial_expression'] == 'closed eyes']\n",
    "raised_brows_indices = [index for index, head in enumerate(data['heads']) if head['facial_expression'] == 'raised brows']\n",
    "puffed_cheeks_indices = [index for index, head in enumerate(data['heads']) if head['facial_expression'] == 'puffed cheeks']\n",
    "\n",
    "# Expression: Emotion \n",
    "sad_indices = [index for index, head in enumerate(data['heads']) if head['emotion'] == 'sad']\n",
    "not_sad_indices = [index for index, head in enumerate(data['heads']) if head['emotion'] != 'sad']\n",
    "happy_indices = [index for index, head in enumerate(data['heads']) if head['emotion'] == 'happy']\n",
    "not_happy_indices = [index for index, head in enumerate(data['heads']) if head['emotion'] != 'happy']\n",
    "\n",
    "# --- Precompute CLIP Text Embeddings ---\n",
    "captions = []\n",
    "captions.append('A man') # 0\n",
    "captions.append('A woman') # 1\n",
    "captions.append('A young person') # 2\n",
    "captions.append('An old person') # 3\n",
    "captions.append('A Caucasian person') # 4\n",
    "captions.append('An Asian person') # 5\n",
    "captions.append('An African person') # 6\n",
    "captions.append('A person wearing a hat') # 7\n",
    "captions.append('A person with a ponytail') # 8\n",
    "captions.append('A person with straight hair') # 9\n",
    "captions.append('A person with curly hair') # 10\n",
    "captions.append('A person with beard') # 11\n",
    "captions.append('A person with open mouth') # 12\n",
    "captions.append('A smiling person') # 13\n",
    "captions.append('A person with closed eyes') # 14\n",
    "captions.append('A frowning person') # 15\n",
    "captions.append('A person with puffed cheeks') # 16\n",
    "captions.append('A sad person') # 17\n",
    "captions.append('A happy person') # 18\n",
    "\n",
    "preprocessed_text = clip.tokenize(captions).to(device) # [num_captions, 77]\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(preprocessed_text) # [num_captions, 512]\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True) # [num_captions, 512]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2474134e2af3d1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Hyperparameter Search for Renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fe2d0b47dc321",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T11:13:50.944617Z",
     "start_time": "2023-12-04T11:13:50.854586Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# --- CLIP Preprocessing --- # \n",
    "clip_tensor_preprocessor = Compose([\n",
    "    Resize(224, interpolation=InterpolationMode.BICUBIC, antialias=None),\n",
    "    CenterCrop(224),\n",
    "    Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "\n",
    "\n",
    "# --- Penalty Function to penalize negative Signal Scores ---\n",
    "def penalty(vector, k):\n",
    "    vector[vector < 0] *= k\n",
    "    return vector\n",
    "\n",
    "# --- Image Embedding Function for Parallel Processing ---\n",
    "\n",
    "def get_image_embeddings(mesh_path, camera_params, phong_params, light_params):\n",
    "    sdf = None\n",
    "    lat_rep = None\n",
    "    rendered_image = render(sdf, lat_rep, camera_params, phong_params, light_params, mesh_path)\n",
    "    image_c_first = rendered_image.permute(2, 0, 1)\n",
    "    image_preprocessed = clip_tensor_preprocessor(image_c_first).unsqueeze(0)\n",
    "    image_features = model.encode_image(image_preprocessed) # [1, 512]\n",
    "    \n",
    "    return image_features\n",
    "\n",
    "def analyze_rendering(hparams):\n",
    "    \n",
    "    # --- Get Hyperparameters ---\n",
    "    camera_params = {\n",
    "        \"camera_distance\": hparams['camera_distance_factor'] * hparams['focal_length'], # ensures appropriate head size\n",
    "        \"camera_angle\": hparams['camera_angle'],\n",
    "        \"focal_length\": hparams['focal_length'],\n",
    "        \"max_ray_length\": (hparams['camera_distance_factor'] + 1) * hparams['focal_length'] + 1.5,\n",
    "        # Image\n",
    "        \"resolution_y\": hparams['resolution'],\n",
    "        \"resolution_x\": hparams['resolution']\n",
    "    }\n",
    "    phong_params = {\n",
    "        \"ambient_coeff\": hparams['ambient_coeff'],\n",
    "        \"diffuse_coeff\": hparams['diffuse_coeff'],\n",
    "        \"specular_coeff\": hparams['specular_coeff'],\n",
    "        \"shininess\": hparams['shininess'],\n",
    "        # Colors\n",
    "        \"object_color\": hparams['object_color'],\n",
    "        \"background_color\": hparams['background_color']\n",
    "    }\n",
    "    \n",
    "    light_params = {\n",
    "        \"amb_light_color\": hparams['amb_light_color'],\n",
    "        # light 1\n",
    "        \"light_intensity_1\": hparams['light_intensity_1'],\n",
    "        \"light_color_1\": hparams['light_color'],\n",
    "        \"light_dir_1\": hparams['light_dir_1'],\n",
    "        # light p\n",
    "        \"light_intensity_p\": hparams['light_intensity_p'],\n",
    "        \"light_color_p\": hparams['light_color'],\n",
    "        \"light_pos_p\": torch.tensor([hparams['light_radius_p'] * math.sqrt(1 - hparams['light_u1_p']**2) * math.cos(2*math.pi*hparams['light_u2_p']),\n",
    "                                     hparams['light_radius_p'] * math.sqrt(1 - hparams['light_u1_p']**2) * math.sin(2*math.pi*hparams['light_u2_p']), \n",
    "                                     hparams['light_radius_p'] * hparams['light_u1_p']])\n",
    "    }\n",
    "    \n",
    "    # --- Render and Embed Images ---\n",
    "    folder_path = '/Users/katharinaschmid/Text2Head/rendering_data/annotated_dataset/'\n",
    "    file_list = sorted(os.listdir(folder_path))\n",
    "    file_list = [file for file in file_list if not (file.startswith('.DS_Store') or file.endswith('.png'))]\n",
    "    \n",
    "    one_mesh_path = folder_path + file_list[0]\n",
    "    sdf = None\n",
    "    lat_rep = None\n",
    "    rendered_image = render(sdf, lat_rep, camera_params, phong_params, light_params, one_mesh_path)\n",
    "    plt.imshow(rendered_image.detach().numpy())\n",
    "    plt.axis('off')  # Turn off axes\n",
    "    plt.show()\n",
    "    \n",
    "    mesh_path_list = []\n",
    "    for mesh in file_list:\n",
    "        mesh_path_list.append(folder_path + mesh)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        pool = multiprocess.Pool(processes=None)\n",
    "        simplified_func = partial(get_image_embeddings, camera_params=camera_params, phong_params=phong_params, light_params=light_params) # creates a simplified version of the function 'get_image_embeddings' where certain arguments are fixed to specific values\n",
    "        image_embeddings_list = pool.map(simplified_func, mesh_path_list)\n",
    "\n",
    "        image_embeddings = torch.cat(image_embeddings_list, dim=0)\n",
    "        image_embeddings /= image_embeddings.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        # --- Get CLIP Similarity and Scores ---\n",
    "        cosine_similarity = torch.matmul(text_features, image_embeddings.T) #[num_captions*tests, num_images]\n",
    "        \n",
    "        # Identity Scores\n",
    "        i_man_score = torch.mean(cosine_similarity[0, male_indices], dim=-1) - torch.mean(cosine_similarity[0, female_indices], dim=-1)\n",
    "        i_woman_score = torch.mean(cosine_similarity[1, female_indices], dim=-1) - torch.mean(cosine_similarity[1, male_indices], dim=-1)\n",
    "        \n",
    "        i_young_score = torch.mean(cosine_similarity[2, young_indices], dim=-1) - torch.mean(cosine_similarity[2, old_indices], dim=-1)\n",
    "        i_old_score = torch.mean(cosine_similarity[3, old_indices], dim=-1) - torch.mean(cosine_similarity[3, young_indices], dim=-1)\n",
    "        i_caucasian_score = torch.mean(cosine_similarity[4, caucasian_indices], dim=-1) - torch.mean(cosine_similarity[4, (asian_indices)], dim=-1)\n",
    "        i_asian_score = torch.mean(cosine_similarity[5, asian_indices], dim=-1) - torch.mean(cosine_similarity[5, (caucasian_indices)], dim=-1)\n",
    "        # 6 skip african\n",
    "        i_hat_score = torch.mean(cosine_similarity[7, hat_indices], dim=-1) - torch.mean(cosine_similarity[7, (ponytail_indices + straight_hair_indices + curly_hair_indices)], dim=-1)\n",
    "        i_ponytail_score = torch.mean(cosine_similarity[8, ponytail_indices], dim=-1) - torch.mean(cosine_similarity[8, (hat_indices + straight_hair_indices + curly_hair_indices)], dim=-1)\n",
    "        i_straight_hair_score = torch.mean(cosine_similarity[9, straight_hair_indices], dim=-1) - torch.mean(cosine_similarity[9, curly_hair_indices], dim=-1) # ONLY curly hair as opposite!\n",
    "        i_curly_hair_score = torch.mean(cosine_similarity[10, curly_hair_indices], dim=-1) - torch.mean(cosine_similarity[10, straight_hair_indices], dim=-1) # ONLY straight hair as opposite!\n",
    "        i_beard_score = torch.mean(cosine_similarity[11, beard_indices], dim=-1) - torch.mean(cosine_similarity[11, no_beard_indices], dim=-1)\n",
    "        \n",
    "        # Expression Scores\n",
    "        e_open_mouth_score = torch.mean(cosine_similarity[12, open_mouth_indices], dim=-1) - torch.mean(cosine_similarity[12, (closed_eyes_indices + raised_brows_indices + puffed_cheeks_indices)], dim=-1) # no smiling as opposite b/c mouth can be open when grinning\n",
    "        e_smiling_score = torch.mean(cosine_similarity[13, smiling_indices], dim=-1) - torch.mean(cosine_similarity[13, (open_mouth_indices + closed_eyes_indices + raised_brows_indices + puffed_cheeks_indices)], dim=-1)\n",
    "        e_closed_eyes_score = torch.mean(cosine_similarity[14, closed_eyes_indices], dim=-1) - torch.mean(cosine_similarity[14, (open_mouth_indices + smiling_indices + raised_brows_indices + puffed_cheeks_indices)], dim=-1)\n",
    "        e_raised_brows_score = torch.mean(cosine_similarity[15, raised_brows_indices], dim=-1) - torch.mean(cosine_similarity[15, (open_mouth_indices + smiling_indices + closed_eyes_indices + puffed_cheeks_indices)], dim=-1)\n",
    "        e_puffed_cheeks_score = torch.mean(cosine_similarity[16, puffed_cheeks_indices], dim=-1) - torch.mean(cosine_similarity[16, (open_mouth_indices + smiling_indices + closed_eyes_indices + raised_brows_indices)], dim=-1)\n",
    "        e_sad_score = torch.mean(cosine_similarity[17, sad_indices], dim=-1) - torch.mean(cosine_similarity[17, not_sad_indices], dim=-1)\n",
    "        e_happy_score = torch.mean(cosine_similarity[18, happy_indices], dim=-1) - torch.mean(cosine_similarity[18, not_happy_indices], dim=-1)\n",
    "        \n",
    "        # Stack Scores, add penalty, compute average\n",
    "        i_scores = torch.stack((i_man_score, i_woman_score, i_young_score, i_old_score, i_caucasian_score, i_asian_score, i_hat_score, i_ponytail_score, i_straight_hair_score, i_curly_hair_score, i_beard_score), dim=0)\n",
    "        e_scores = torch.stack((e_open_mouth_score, e_smiling_score, e_closed_eyes_score, e_raised_brows_score, e_puffed_cheeks_score, e_sad_score, e_happy_score), dim=0)\n",
    "        print('i_scores: ', i_scores, 'e_scores: ', e_scores)\n",
    "        scores = torch.cat((i_scores, e_scores), dim=0)\n",
    "        scores_penalized = penalty(scores, 10)\n",
    "        scores_avg = torch.mean(scores_penalized, dim=0)\n",
    "        print('scores penalized: ', scores_penalized)\n",
    "        print('scores_avg: ', scores_avg)\n",
    "        \n",
    "    \n",
    "    return scores_avg\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters to tune and their search spaces\n",
    "    search_space = {\n",
    "        # Camera\n",
    "        \"camera_distance_factor\": trial.suggest_float('camera_distance_factor', 0.2, 0.5), \n",
    "        \"camera_angle\": trial.suggest_categorical('camera_angle', [30., 45., 55.]), \n",
    "        \"focal_length\": trial.suggest_float('focal_length', 1.0, 4.0),\n",
    "        # Phong\n",
    "        \"ambient_coeff\": trial.suggest_float('ambient_coeff', 0.3, 0.8), \n",
    "        \"diffuse_coeff\": trial.suggest_float('diffuse_coeff', 0.6, 1.0), \n",
    "        \"specular_coeff\": trial.suggest_float('specular_coeff', 0.4, 0.7), \n",
    "        \"shininess\": trial.suggest_categorical('shininess', [0.1, 0.5, 1.0, 3.0]),\n",
    "        # Colors\n",
    "        \"object_color\": torch.tensor([\n",
    "            trial.suggest_float('object_color_0', 0.5, 1.0),\n",
    "            trial.suggest_float('object_color_1', 0.0, 0.4),\n",
    "            trial.suggest_float('object_color_2', 0.6, 1.0)\n",
    "        ]),\n",
    "        \"background_color\": torch.tensor([\n",
    "            trial.suggest_float('background_color_0', 0.0, 0.4),\n",
    "            trial.suggest_float('background_color_1', 0.6, 1.0),\n",
    "            trial.suggest_float('background_color_2', 0.0, 0.4)\n",
    "        ]),\n",
    "        # Light\n",
    "        \"amb_light_color\": torch.tensor([\n",
    "            trial.suggest_float('amb_light_color_0', 0.3, 0.9),\n",
    "            trial.suggest_float('amb_light_color_1', 0.0, 0.3),\n",
    "            trial.suggest_float('amg_light_color_2', 0.4, 0.8)\n",
    "        ]),\n",
    "        # light 1\n",
    "        \"light_intensity_1\": trial.suggest_float('light_intensity_1', 1.0, 3.0), \n",
    "        \"light_color\": torch.tensor([\n",
    "            trial.suggest_float('light_color_0', 0.8, 1.0),\n",
    "            trial.suggest_float('light_color_1', 0.9, 1.0),\n",
    "            trial.suggest_float('light_color_2', 0.7, 0.9)\n",
    "        ]),\n",
    "        \"light_dir_1\": torch.tensor([\n",
    "            trial.suggest_float('light_dir_1_0', -0.6, -0.2),\n",
    "            trial.suggest_float('light_dir_1_1', -0.7, -0.3),\n",
    "            trial.suggest_float('light_dir_1_2', -0.95, -0.55)\n",
    "        ]),\n",
    "        # light p\n",
    "        \"light_intensity_p\": trial.suggest_float('light_intensity_p', 0., 2.), \n",
    "        \"light_radius_p\": trial.suggest_float('light_radius_p', 1., 4.), \n",
    "        \"light_u1_p\": trial.suggest_float('light_u1_p', -1.0, 1.0),\n",
    "        \"light_u2_p\": trial.suggest_float('light_u2_p', 0., 1.)\n",
    "    }\n",
    "    \n",
    "    hparams.update(search_space)\n",
    "    \n",
    "    return analyze_rendering(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8660eafbf534fc18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T12:03:14.991227Z",
     "start_time": "2023-12-04T11:54:49.586101Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    study = optuna.create_study(storage=\"sqlite:///optuna_study_renderparams.db\", study_name=\"render_params_update2\", direction='maximize', load_if_exists=True)\n",
    "    study.optimize(objective, n_trials=1)\n",
    "        \n",
    "    best_params = study.best_params\n",
    "    print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4531cb3695fa6c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
