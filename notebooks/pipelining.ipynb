{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T17:26:46.968182Z",
     "start_time": "2023-12-04T17:26:31.487569Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/langrieger/miniconda3/envs/Text2Head/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Adam\n",
    "from utils.pipeline import forward, get_latent_mean_std, get_latent_from_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T17:26:48.754399Z",
     "start_time": "2023-12-04T17:26:48.709693Z"
    }
   },
   "outputs": [],
   "source": [
    "lat_mean, lat_std = get_latent_mean_std()\n",
    "lat_rep = (torch.randn(lat_mean.shape) * lat_std * 0.85 + lat_mean).detach().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T17:26:49.411931Z",
     "start_time": "2023-12-04T17:26:49.379419Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(params=[lat_rep],\n",
    "                 lr=0.001, \n",
    "                 maximize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T17:39:06.598585Z",
     "start_time": "2023-12-04T17:39:06.522400Z"
    }
   },
   "outputs": [],
   "source": [
    "camera_params = {\n",
    "        \"camera_distance\": 1.42,\n",
    "        \"camera_angle\": 55.,\n",
    "        \"focal_length\": 3.15,\n",
    "        \"max_ray_length\": (0.5 + 1) * 3.15 + 1.5,\n",
    "        # Image\n",
    "        \"resolution_y\": 100,\n",
    "        \"resolution_x\": 100\n",
    "    }\n",
    "\n",
    "phong_params = {\n",
    "    \"ambient_coeff\": 0.67,\n",
    "    \"diffuse_coeff\": 0.79,\n",
    "    \"specular_coeff\": 0.3,  # 0.63,\n",
    "    \"shininess\": 1.,\n",
    "    # Colors\n",
    "    \"object_color\": torch.tensor([0.61, 0.61, 0.61]),  # torch.tensor([0.63, 0.17, 0.78]),\n",
    "    \"background_color\": torch.tensor([0., 0., 0.])  # torch.tensor([0.35, 0.94, 0.26])\n",
    "}\n",
    "\n",
    "light_params = {\n",
    "    \"amb_light_color\": torch.tensor([0.57, 0.07, 0.69]),\n",
    "    # light 1\n",
    "    \"light_intensity_1\": 0.,  # 1.1,\n",
    "    \"light_color_1\": torch.tensor([0.88, 0.99, 0.74]),\n",
    "    \"light_dir_1\": torch.tensor([-0.41, -0.51, -0.76]),\n",
    "    # light p\n",
    "    \"light_intensity_p\": 1.,\n",
    "    \"light_color_p\": torch.tensor([0.88, 0.99, 0.74]),\n",
    "    \"light_pos_p\": torch.tensor([2., 0., 2.])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T17:41:26.028602Z",
     "start_time": "2023-12-04T17:39:08.011236Z"
    }
   },
   "outputs": [],
   "source": [
    "score = forward(lat_rep, \"untextured render of a face\", camera_params, phong_params, light_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T17:45:05.085670Z",
     "start_time": "2023-12-04T17:42:34.071637Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score[0].backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T17:30:14.678555Z",
     "start_time": "2023-12-04T17:29:14.530220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update step 0 - score: tensor([[22.5312]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<MmBackward0>)\n",
      "lr:  0.003\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 306.00 MiB (GPU 0; 10.91 GiB total capacity; 10.18 GiB already allocated; 53.44 MiB free; 10.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/langrieger/Text2Head/notebooks/pipelining.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bniessnerlab/home/langrieger/Text2Head/notebooks/pipelining.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m latent, hist \u001b[39m=\u001b[39m get_latent_from_text(\u001b[39m\"\u001b[39;49m\u001b[39mA woman\u001b[39;49m\u001b[39m\"\u001b[39;49m, n_updates\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n",
      "File \u001b[0;32m~/Text2Head/utils/pipeline.py:131\u001b[0m, in \u001b[0;36mget_latent_from_text\u001b[0;34m(prompt, init_lat, n_updates)\u001b[0m\n\u001b[1;32m    129\u001b[0m latents\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mclone(lat_rep))\n\u001b[1;32m    130\u001b[0m images\u001b[39m.\u001b[39mappend(image)\n\u001b[0;32m--> 131\u001b[0m score\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    132\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mupdate step \u001b[39m\u001b[39m{\u001b[39;00mn\u001b[39m}\u001b[39;00m\u001b[39m - score: \u001b[39m\u001b[39m{\u001b[39;00mscore\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    133\u001b[0m \u001b[39m#plt.imshow(image.detach().numpy())\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[39m#plt.axis('off')  # Turn off axes\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[39m#plt.show()\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Text2Head/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/Text2Head/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 306.00 MiB (GPU 0; 10.91 GiB total capacity; 10.18 GiB already allocated; 53.44 MiB free; 10.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "latent, hist = get_latent_from_text(\"A woman\", n_updates=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T17:32:05.779761Z",
     "start_time": "2023-12-04T17:32:04.932225Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(hist[\"scores\"]).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T17:32:08.607684Z",
     "start_time": "2023-12-04T17:32:08.594915Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(latent, \"optim_latent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(torch.stack(hist[\"latents\"]), \"latent_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(torch.stack(hist[\"images\"]), \"render_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T17:32:12.595200Z",
     "start_time": "2023-12-04T17:32:12.474059Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(hist[\"images\"][3].detach().numpy())\n",
    "plt.axis('off')  # Turn off axes\n",
    "plt.show()\n",
    "#plt.savefig(f\"optim_img/high_lr_{i}\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T17:32:20.206671Z",
     "start_time": "2023-12-04T17:32:15.457577Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, im in enumerate(hist[\"images\"]):\n",
    "    plt.imshow(im.detach().numpy())\n",
    "    plt.axis('off')  # Turn off axes\n",
    "    #plt.show()\n",
    "    plt.savefig(f\"optim_img/high_lr_{5+i}\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nphm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
