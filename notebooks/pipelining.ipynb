{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T17:26:46.968182Z",
     "start_time": "2023-12-04T17:26:31.487569Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/schmid/Text2Head/NPHM\")\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Adam\n",
    "from utils.pipeline import forward, get_latent_mean_std, get_latent_from_text\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T17:26:48.754399Z",
     "start_time": "2023-12-04T17:26:48.709693Z"
    }
   },
   "outputs": [],
   "source": [
    "lat_mean, lat_std = get_latent_mean_std()\n",
    "lat_rep = (torch.randn(lat_mean.shape) * lat_std * 0.85 + lat_mean).detach().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T17:26:49.411931Z",
     "start_time": "2023-12-04T17:26:49.379419Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(params=[lat_rep],\n",
    "                 lr=0.001, \n",
    "                 maximize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T17:39:06.598585Z",
     "start_time": "2023-12-04T17:39:06.522400Z"
    }
   },
   "outputs": [],
   "source": [
    "camera_params = {\n",
    "        \"camera_distance\": 1.42,\n",
    "        \"camera_angle\": 55.,\n",
    "        \"focal_length\": 3.15,\n",
    "        \"max_ray_length\": (0.5 + 1) * 3.15 + 1.5,\n",
    "        # Image\n",
    "        \"resolution_y\": 100,\n",
    "        \"resolution_x\": 100\n",
    "    }\n",
    "\n",
    "phong_params = {\n",
    "    \"ambient_coeff\": 0.67,\n",
    "    \"diffuse_coeff\": 0.79,\n",
    "    \"specular_coeff\": 0.3,  # 0.63,\n",
    "    \"shininess\": 1.,\n",
    "    # Colors\n",
    "    \"object_color\": torch.tensor([0.61, 0.61, 0.61]),  # torch.tensor([0.63, 0.17, 0.78]),\n",
    "    \"background_color\": torch.tensor([0., 0., 0.])  # torch.tensor([0.35, 0.94, 0.26])\n",
    "}\n",
    "\n",
    "light_params = {\n",
    "    \"amb_light_color\": torch.tensor([0.57, 0.07, 0.69]),\n",
    "    # light 1\n",
    "    \"light_intensity_1\": 0.,  # 1.1,\n",
    "    \"light_color_1\": torch.tensor([0.88, 0.99, 0.74]),\n",
    "    \"light_dir_1\": torch.tensor([-0.41, -0.51, -0.76]),\n",
    "    # light p\n",
    "    \"light_intensity_p\": 1.,\n",
    "    \"light_color_p\": torch.tensor([0.88, 0.99, 0.74]),\n",
    "    \"light_pos_p\": torch.tensor([2., 0., 2.])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T17:41:26.028602Z",
     "start_time": "2023-12-04T17:39:08.011236Z"
    }
   },
   "outputs": [],
   "source": [
    "score = forward(lat_rep, \"untextured render of a face\", camera_params, phong_params, light_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T17:45:05.085670Z",
     "start_time": "2023-12-04T17:42:34.071637Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "score[0].backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_latent = torch.load(\"../woman_latent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T17:30:14.678555Z",
     "start_time": "2023-12-04T17:29:14.530220Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 218.00 MiB (GPU 0; 10.91 GiB total capacity; 1.56 GiB already allocated; 95.44 MiB free; 1.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/schmid/Text2Head/notebooks/pipelining.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bniessnerlab/home/schmid/Text2Head/notebooks/pipelining.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m hparams \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bniessnerlab/home/schmid/Text2Head/notebooks/pipelining.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mresolution\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m120\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bniessnerlab/home/schmid/Text2Head/notebooks/pipelining.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mn_iterations\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m50\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bniessnerlab/home/schmid/Text2Head/notebooks/pipelining.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mlr_scheduler_min_lr\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1e-5\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bniessnerlab/home/schmid/Text2Head/notebooks/pipelining.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     }\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bniessnerlab/home/schmid/Text2Head/notebooks/pipelining.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#torch.cuda.memory._record_memory_history(True)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bniessnerlab/home/schmid/Text2Head/notebooks/pipelining.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m best_latent, best_score, hist \u001b[39m=\u001b[39m get_latent_from_text(\u001b[39m\"\u001b[39;49m\u001b[39mA woman\u001b[39;49m\u001b[39m\"\u001b[39;49m, hparams)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bniessnerlab/home/schmid/Text2Head/notebooks/pipelining.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m#torch.cuda.memory._record_memory_history(False)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bniessnerlab/home/schmid/Text2Head/notebooks/pipelining.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m#torch.cuda.memory._dump_snapshot(\"../memory_snapshot.pickle\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bniessnerlab/home/schmid/Text2Head/notebooks/pipelining.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(best_score)\n",
      "File \u001b[0;32m~/Text2Head/utils/pipeline.py:141\u001b[0m, in \u001b[0;36mget_latent_from_text\u001b[0;34m(prompt, hparams, init_lat)\u001b[0m\n\u001b[1;32m    139\u001b[0m latents \u001b[39m=\u001b[39m []\n\u001b[1;32m    140\u001b[0m images \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 141\u001b[0m best_score \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m0\u001b[39m])\n\u001b[1;32m    142\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[1;32m    143\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/Text2Head/utils/pipeline.py:65\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(lat_rep, prompt, camera_params, phong_params, light_params)\u001b[0m\n\u001b[1;32m     62\u001b[0m     lat_std \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(np\u001b[39m.\u001b[39mload(env_paths\u001b[39m.\u001b[39mASSETS \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnphm_lat_std.npy\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     63\u001b[0m     \u001b[39mreturn\u001b[39;00m lat_mean, lat_std\n\u001b[0;32m---> 65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(lat_rep, prompt, camera_params, phong_params, light_params):\n\u001b[1;32m     66\u001b[0m     image \u001b[39m=\u001b[39m render(decoder_shape, lat_rep, camera_params, phong_params, light_params)\n\u001b[1;32m     68\u001b[0m     image_c_first \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mpermute(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Text2Head/utils/render.py:201\u001b[0m, in \u001b[0;36mrender\u001b[0;34m(model, lat_rep, camera_params, phong_params, light_params, mesh_path)\u001b[0m\n\u001b[1;32m    197\u001b[0m directions \u001b[39m=\u001b[39m (transposed_directions \u001b[39m/\u001b[39m transposed_directions\u001b[39m.\u001b[39mnorm(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\u001b[39m.\u001b[39mT  \u001b[39m# [pu*pv, 3]\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    200\u001b[0m \u001b[39m# Option 1: Use SDF\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m     hit_positions, hit_mask \u001b[39m=\u001b[39m two_phase_tracing(sdf, camera_position, directions, camera_params[\u001b[39m'\u001b[39;49m\u001b[39mmax_ray_length\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    202\u001b[0m \u001b[39m# Option 2: Use Mesh\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[39m# intersections, hit_mask, index_tri = mesh_trace(mesh_path, camera_position, directions)\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \n\u001b[1;32m    205\u001b[0m \u001b[39m#with torch.no_grad():\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m# Option 1: Use SDF\u001b[39;00m\n\u001b[1;32m    207\u001b[0m reflections \u001b[39m=\u001b[39m phong_model(sdf, hit_positions[hit_mask], camera_position, phong_params, light_params, mesh_path)\n",
      "File \u001b[0;32m~/Text2Head/utils/render.py:140\u001b[0m, in \u001b[0;36mtwo_phase_tracing\u001b[0;34m(sdf, camera_position, norm_directions, max_length, scale, eps)\u001b[0m\n\u001b[1;32m    138\u001b[0m N \u001b[39m=\u001b[39m norm_directions\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 140\u001b[0m     hits_1, hit_mask_1, t_1 \u001b[39m=\u001b[39m acc_sphere_trace(sdf, camera_position, norm_directions, max_length,\n\u001b[1;32m    141\u001b[0m                                                             scale\u001b[39m=\u001b[39;49m\u001b[39m2.\u001b[39;49m, eps\u001b[39m=\u001b[39;49m\u001b[39m0.025\u001b[39;49m)\n\u001b[1;32m    142\u001b[0m hits_2, hit_mask_2, t_2 \u001b[39m=\u001b[39m acc_sphere_trace(sdf, hits_1[hit_mask_1], norm_directions[hit_mask_1], \u001b[39m3.\u001b[39m,\n\u001b[1;32m    143\u001b[0m                                                         scale\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39msqrt(\u001b[39m2.\u001b[39m), eps\u001b[39m=\u001b[39m\u001b[39m0.005\u001b[39m)\n\u001b[1;32m    145\u001b[0m hit_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(N)\u001b[39m.\u001b[39mbool()\n",
      "File \u001b[0;32m~/Text2Head/utils/render.py:117\u001b[0m, in \u001b[0;36macc_sphere_trace\u001b[0;34m(sdf, init_position, norm_directions, max_length, scale, eps, init_t)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    115\u001b[0m d_curr[mask] \u001b[39m=\u001b[39m r_curr[mask] \u001b[39m+\u001b[39m scale \u001b[39m*\u001b[39m r_curr[mask] \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mnan_to_num(\n\u001b[1;32m    116\u001b[0m     (d_curr[mask] \u001b[39m-\u001b[39m r_last[mask] \u001b[39m+\u001b[39m r_curr[mask]) \u001b[39m/\u001b[39m (d_curr[mask] \u001b[39m+\u001b[39m r_last[mask] \u001b[39m-\u001b[39m r_curr[mask]))\n\u001b[0;32m--> 117\u001b[0m r_next[mask] \u001b[39m=\u001b[39m sdf(positions[mask] \u001b[39m+\u001b[39;49m ((t[mask] \u001b[39m+\u001b[39;49m d_curr[mask]) \u001b[39m*\u001b[39;49m norm_directions[mask]\u001b[39m.\u001b[39;49mT)\u001b[39m.\u001b[39;49mT)\n\u001b[1;32m    119\u001b[0m normal_tracing_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mabs(d_curr[mask]) \u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mabs(r_curr[mask]) \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mabs(r_next[mask])\n\u001b[1;32m    120\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39many(normal_tracing_mask):\n",
      "File \u001b[0;32m~/Text2Head/utils/render.py:168\u001b[0m, in \u001b[0;36mrender.<locals>.sdf\u001b[0;34m(positions, max_number)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(distances, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39msqueeze()\n\u001b[1;32m    167\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m     distance \u001b[39m=\u001b[39m model(nphm_input\u001b[39m.\u001b[39;49mto(device), lat_rep_in\u001b[39m.\u001b[39;49mto(device), \u001b[39mNone\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    169\u001b[0m     \u001b[39mreturn\u001b[39;00m distance\u001b[39m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m~/miniconda3/envs/nphm/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Text2Head/NPHM/src/NPHM/models/EnsembledDeepSDF.py:257\u001b[0m, in \u001b[0;36mFastEnsembleDeepSDFMirrored.forward\u001b[0;34m(self, xyz, lat_rep, anchors_gt)\u001b[0m\n\u001b[1;32m    254\u001b[0m coords \u001b[39m=\u001b[39m coords\u001b[39m.\u001b[39mpermute(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m) \u001b[39m# nkps x B x N x 3\u001b[39;00m\n\u001b[1;32m    255\u001b[0m cond \u001b[39m=\u001b[39m cond\u001b[39m.\u001b[39mpermute(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m) \u001b[39m# nkps x B x N x (dim_glob + dim_loc)\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m sdf_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mensembled_deep_sdf(coords, cond)\n\u001b[1;32m    259\u001b[0m \u001b[39m# hack, not sure if this is a good idea\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n",
      "File \u001b[0;32m~/miniconda3/envs/nphm/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Text2Head/NPHM/src/NPHM/models/EnsembledDeepSDF.py:106\u001b[0m, in \u001b[0;36mEnsembledDeepSDF.forward\u001b[0;34m(self, xyz, lat_rep)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, xyz, lat_rep):\n\u001b[1;32m    102\u001b[0m     \u001b[39m# xyz: A x B x nPoints x 3\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     \u001b[39m# lat_rep: A x B x nPoints x nFeats\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     A, B, nP, _ \u001b[39m=\u001b[39m xyz\u001b[39m.\u001b[39mshape\n\u001b[0;32m--> 106\u001b[0m     inp \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat([xyz, lat_rep], dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    108\u001b[0m     \u001b[39m# merge batch and point dimension\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     inp \u001b[39m=\u001b[39m inp\u001b[39m.\u001b[39mreshape(A, B\u001b[39m*\u001b[39mnP, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m# A x (B*nP) x (3+nFeats)\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 218.00 MiB (GPU 0; 10.91 GiB total capacity; 1.56 GiB already allocated; 95.44 MiB free; 1.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "hparams = {\n",
    "        'resolution': 120,\n",
    "        'n_iterations': 50,\n",
    "        'optimizer_lr': 0.002,\n",
    "        'lr_scheduler_factor': 0.5,\n",
    "        'lr_scheduler_patience': 2,\n",
    "        'lr_scheduler_min_lr': 1e-5\n",
    "    }\n",
    "#torch.cuda.memory._record_memory_history(True)\n",
    "best_latent, best_score, hist = get_latent_from_text(\"A woman\", hparams)\n",
    "#torch.cuda.memory._record_memory_history(False)\n",
    "#torch.cuda.memory._dump_snapshot(\"../memory_snapshot.pickle\")\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T17:32:05.779761Z",
     "start_time": "2023-12-04T17:32:04.932225Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(hist[\"scores\"]).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T17:32:08.607684Z",
     "start_time": "2023-12-04T17:32:08.594915Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(latent, \"../woman_latent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(torch.stack(hist[\"latents\"]), \"latent_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(torch.stack(hist[\"images\"]), \"render_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T17:32:12.595200Z",
     "start_time": "2023-12-04T17:32:12.474059Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(hist[\"images\"][-1].detach().numpy())\n",
    "plt.axis('off')  # Turn off axes\n",
    "plt.show()\n",
    "#plt.savefig(f\"optim_img/high_lr_{i}\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-04T17:32:20.206671Z",
     "start_time": "2023-12-04T17:32:15.457577Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, im in enumerate(hist[\"images\"]):\n",
    "    plt.imshow(im.detach().numpy())\n",
    "    plt.axis('off')  # Turn off axes\n",
    "    #plt.show()\n",
    "    plt.savefig(f\"../optim_img/{i}\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nphm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
