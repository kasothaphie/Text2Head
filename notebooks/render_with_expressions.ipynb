{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T10:09:51.492228Z",
     "start_time": "2023-11-30T10:09:44.302834Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from NPHM.models.deepSDF import DeepSDF, DeformationNetwork\n",
    "from NPHM.models.EnsembledDeepSDF import FastEnsembleDeepSDFMirrored\n",
    "from NPHM import env_paths\n",
    "\n",
    "import numpy as np\n",
    "import json, yaml\n",
    "import os\n",
    "import os.path as osp\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358295788d294b81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T10:09:56.731019Z",
     "start_time": "2023-11-30T10:09:56.053581Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load general config\n",
    "with open('NPHM/scripts/configs/fitting_nphm.yaml', 'r') as f:\n",
    "    print('Loading config file from: ' + 'scripts/configs/fitting_nphm.yaml')\n",
    "    CFG = yaml.safe_load(f)\n",
    "print(json.dumps(CFG, sort_keys=True, indent=4))\n",
    "\n",
    "# paths to shape and expression MLP config\n",
    "weight_dir_shape = env_paths.EXPERIMENT_DIR + '/{}/'.format(CFG['exp_name_shape'])\n",
    "weight_dir_expr = env_paths.EXPERIMENT_DIR + '/{}/'.format(CFG['exp_name_expr'])\n",
    "\n",
    "# load shape and expression MLP config\n",
    "fname_shape = weight_dir_shape + 'configs.yaml'\n",
    "with open(fname_shape, 'r') as f:\n",
    "    print('Loading config file from: ' + fname_shape)\n",
    "    CFG_shape = yaml.safe_load(f)\n",
    "fname_expr = weight_dir_expr + 'configs.yaml'\n",
    "with open(fname_expr, 'r') as f:\n",
    "    print('Loading config file from: ' + fname_expr)\n",
    "    CFG_expr = yaml.safe_load(f)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Load anchor points\n",
    "lm_inds = np.load(env_paths.ANCHOR_INDICES_PATH)\n",
    "anchors = torch.from_numpy(np.load(env_paths.ANCHOR_MEAN_PATH)).float().unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "# Initialize Shape MLP using CFG\n",
    "decoder_shape = FastEnsembleDeepSDFMirrored(\n",
    "        lat_dim_glob=CFG_shape['decoder']['decoder_lat_dim_glob'],\n",
    "        lat_dim_loc=CFG_shape['decoder']['decoder_lat_dim_loc'],\n",
    "        hidden_dim=CFG_shape['decoder']['decoder_hidden_dim'],\n",
    "        n_loc=CFG_shape['decoder']['decoder_nloc'],\n",
    "        n_symm_pairs=CFG_shape['decoder']['decoder_nsymm_pairs'],\n",
    "        anchors=anchors,\n",
    "        n_layers=CFG_shape['decoder']['decoder_nlayers'],\n",
    "        pos_mlp_dim=CFG_shape['decoder'].get('pos_mlp_dim', 256),\n",
    "    )\n",
    "\n",
    "decoder_shape = decoder_shape.to(device)\n",
    "\n",
    "# Initialize Expression MLP using CFG\n",
    "decoder_expr = DeformationNetwork(mode=CFG_expr['ex_decoder']['mode'],\n",
    "                                 lat_dim_expr=CFG_expr['ex_decoder']['decoder_lat_dim_expr'],\n",
    "                                 lat_dim_id=CFG_expr['ex_decoder']['decoder_lat_dim_id'],\n",
    "                                 lat_dim_glob_shape=CFG_expr['id_decoder']['decoder_lat_dim_glob'],\n",
    "                                 lat_dim_loc_shape=CFG_expr['id_decoder']['decoder_lat_dim_loc'],\n",
    "                                 n_loc=CFG_expr['id_decoder']['decoder_nloc'],\n",
    "                                 anchors=anchors,\n",
    "                                 hidden_dim=CFG_expr['ex_decoder']['decoder_hidden_dim'],\n",
    "                                 nlayers=CFG_expr['ex_decoder']['decoder_nlayers'],\n",
    "                                 input_dim=3, out_dim=3\n",
    "                                 )\n",
    "\n",
    "decoder_expr = decoder_expr.to(device)\n",
    "\n",
    "# Load shape MLP weights\n",
    "path = osp.join(weight_dir_shape, 'checkpoints/checkpoint_epoch_{}.tar'.format(CFG['checkpoint_shape']))\n",
    "print('Loaded checkpoint from: {}'.format(path))\n",
    "checkpoint = torch.load(path, map_location=device)\n",
    "decoder_shape.load_state_dict(checkpoint['decoder_state_dict'], strict=True)\n",
    "\n",
    "if 'latent_codes_state_dict' in checkpoint:\n",
    "    n_train_subjects = checkpoint['latent_codes_state_dict']['weight'].shape[0]\n",
    "    n_val_subjects = checkpoint['latent_codes_val_state_dict']['weight'].shape[0]\n",
    "    latent_codes_shape = torch.nn.Embedding(n_train_subjects, 512)\n",
    "    latent_codes_shape_val = torch.nn.Embedding(n_val_subjects, 512)\n",
    "    \n",
    "    latent_codes_shape.load_state_dict(checkpoint['latent_codes_state_dict'])\n",
    "    latent_codes_shape_val.load_state_dict(checkpoint['latent_codes_val_state_dict'])\n",
    "else:\n",
    "    print('no latent codes in state dict of shape decoder')\n",
    "    latent_codes_shape = None\n",
    "    latent_codes_shape_val = None\n",
    "\n",
    "# Load expression MLP weights\n",
    "path = osp.join(weight_dir_expr, 'checkpoints/checkpoint_epoch_{}.tar'.format(CFG['checkpoint_expr']))\n",
    "print('Loaded checkpoint from: {}'.format(path))\n",
    "checkpoint = torch.load(path, map_location=device)\n",
    "decoder_expr.load_state_dict(checkpoint['decoder_state_dict'], strict=True)\n",
    "if 'latent_codes_state_dict' in checkpoint:\n",
    "    latent_codes_expr = torch.nn.Embedding(checkpoint['latent_codes_state_dict']['weight'].shape[0], 200)\n",
    "    latent_codes_expr.load_state_dict(checkpoint['latent_codes_state_dict'])\n",
    "    latent_codes_expr_val = torch.nn.Embedding(checkpoint['latent_codes_val_state_dict']['weight'].shape[0], 200)\n",
    "    latent_codes_expr_val.load_state_dict(checkpoint['latent_codes_val_state_dict'])\n",
    "else:\n",
    "    print('no latent codes in state dict of exression decoder')\n",
    "    latent_codes_expr = None\n",
    "    latent_codes_expr_val = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14b9d402db23558",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T10:40:11.781329Z",
     "start_time": "2023-11-30T10:37:05.470887Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lat_mean = torch.from_numpy(np.load(env_paths.ASSETS + 'nphm_lat_mean.npy')) # [1344]\n",
    "lat_rep_id = lat_mean\n",
    "\n",
    "def phong_model(sdf, points, normals, camera_position, phong_params, light_params, mesh_path, index_tri=None):\n",
    "    \n",
    "    if normals == None:\n",
    "        # Option 1: Use SDF\n",
    "        normals = estimate_normals(sdf, points)\n",
    "        # Option 2: Use Mesh\n",
    "        #normals = mesh_normals(mesh_path, index_tri)\n",
    "        \n",
    "    view_dirs = points - camera_position \n",
    "    light_dir_1 = light_params[\"light_dir_1\"].repeat(points.shape[0], 1) \n",
    "    light_dir_2 = light_params[\"light_dir_2\"].repeat(points.shape[0], 1)\n",
    "    light_dir_3 = light_params[\"light_dir_3\"].repeat(points.shape[0], 1)\n",
    "    \n",
    "    # Normalize all vectors\n",
    "    normals = (normals.T / torch.norm(normals, dim=-1)).T\n",
    "    light_dir_norm_1 = (light_dir_1.T / torch.norm(light_dir_1, dim=-1)).T\n",
    "    light_dir_norm_2 = (light_dir_2.T / torch.norm(light_dir_2, dim=-1)).T\n",
    "    light_dir_norm_3 = (light_dir_3.T / torch.norm(light_dir_3, dim=-1)).T\n",
    "    view_dir_norm = (view_dirs.T / torch.norm(view_dirs, dim=-1)).T\n",
    "    \n",
    "    # Ambient\n",
    "    ambient = phong_params[\"ambient_coeff\"] * light_params[\"amb_light_color\"] \n",
    "    ambient_refl = ambient.repeat(points.shape[0], 1)\n",
    "    \n",
    "    # Area light 1\n",
    "    diffuse_1 = phong_params[\"diffuse_coeff\"] * torch.clamp(torch.sum(-light_dir_norm_1 * normals, dim=-1), min=0.0) * light_params[\"light_intensity_1\"] # [N]\n",
    "    diffuse_refl_1 = torch.matmul(diffuse_1.unsqueeze(1), light_params[\"light_color_1\"].unsqueeze(0)) # [N, 3]\n",
    "    reflect_dir_1 = light_dir_norm_1 + (2 * normals.T * torch.clamp(torch.sum(-light_dir_norm_1 * normals, dim=-1), min=0.0)).T\n",
    "    specular_1 = phong_params[\"specular_coeff\"] * torch.pow(torch.clamp(torch.sum(reflect_dir_1 * -view_dir_norm, dim=-1), min=0.0), phong_params[\"shininess\"]) * light_params[\"light_intensity_1\"] # [N]\n",
    "    specular_refl_1 = torch.matmul(specular_1.unsqueeze(1), light_params[\"light_color_1\"].unsqueeze(0)) # [N, 3]\n",
    "    \n",
    "    # Area light 2\n",
    "    diffuse_2 = phong_params[\"diffuse_coeff\"] * torch.clamp(torch.sum(-light_dir_norm_2 * normals, dim=-1), min=0.0) * light_params[\"light_intensity_2\"] # [N]\n",
    "    diffuse_refl_2 = torch.matmul(diffuse_2.unsqueeze(1), light_params[\"light_color_2\"].unsqueeze(0)) # [N, 3]\n",
    "    reflect_dir_2 = light_dir_norm_2 + (2 * normals.T * torch.clamp(torch.sum(-light_dir_norm_2 * normals, dim=-1), min=0.0)).T\n",
    "    specular_2 = phong_params[\"specular_coeff\"] * torch.pow(torch.clamp(torch.sum(reflect_dir_2 * -view_dir_norm, dim=-1), min=0.0), phong_params[\"shininess\"]) * light_params[\"light_intensity_2\"] # [N]\n",
    "    specular_refl_2 = torch.matmul(specular_2.unsqueeze(1), light_params[\"light_color_2\"].unsqueeze(0)) # [N, 3]\n",
    "    \n",
    "    # Area light 3\n",
    "    diffuse_3 = phong_params[\"diffuse_coeff\"] * torch.clamp(torch.sum(-light_dir_norm_3 * normals, dim=-1), min=0.0) * light_params[\"light_intensity_3\"] # [N]\n",
    "    diffuse_refl_3 = torch.matmul(diffuse_3.unsqueeze(1), light_params[\"light_color_3\"].unsqueeze(0)) # [N, 3]\n",
    "    reflect_dir_3 = light_dir_norm_3 + (2 * normals.T * torch.clamp(torch.sum(-light_dir_norm_3 * normals, dim=-1), min=0.0)).T\n",
    "    specular_3 = phong_params[\"specular_coeff\"] * torch.pow(torch.clamp(torch.sum(reflect_dir_3 * -view_dir_norm, dim=-1), min=0.0), phong_params[\"shininess\"]) * light_params[\"light_intensity_3\"] # [N]\n",
    "    specular_refl_3 = torch.matmul(specular_3.unsqueeze(1), light_params[\"light_color_3\"].unsqueeze(0)) # [N, 3]\n",
    "    \n",
    "    # point light\n",
    "    #diffuse_1 = diffuse_coeff * torch.clamp(torch.sum(-light_dir_norm_1 * normals, dim=-1), min=0.0) * light_intensity_1 # [N]\n",
    "    #diffuse_refl_1 = torch.matmul(diffuse_1.unsqueeze(1), light_color.unsqueeze(0)) # [N, 3]\n",
    "    #reflect_dir_1 = light_dir_norm_1 + (2 * normals.T * torch.clamp(torch.sum(-light_dir_norm_1 * normals, dim=-1), min=0.0)).T\n",
    "    #specular_1 = specular_coeff * torch.pow(torch.clamp(torch.sum(reflect_dir_1 * -view_dir_norm, dim=-1), min=0.0), shininess) * light_intensity_1 # [N]\n",
    "    #specular_refl_1 = torch.matmul(specular_1.unsqueeze(1), light_color.unsqueeze(0)) # [N, 3]\n",
    "\n",
    "    return ambient_refl + diffuse_refl_1 + specular_refl_1 + diffuse_refl_2 + specular_refl_2 + diffuse_refl_3 + specular_refl_3\n",
    "\n",
    "def estimate_normals(sdf, points, epsilon=1e-3):\n",
    "    sdf_inputs = torch.concat([points,\n",
    "                              points + torch.tensor([epsilon, 0, 0]),\n",
    "                              points + torch.tensor([0, epsilon, 0]),\n",
    "                              points + torch.tensor([0, 0, epsilon])])\n",
    "\n",
    "    sdf_values = sdf(sdf_inputs).reshape(4, -1)\n",
    "\n",
    "    # Calculate the gradient using finite differences\n",
    "    gradient = sdf_values[1:] - sdf_values[0]\n",
    "\n",
    "    # Normalize the gradient to obtain the estimated normal\n",
    "    normal = gradient / torch.norm(gradient, p=2, dim=0)\n",
    "\n",
    "    return normal.T\n",
    "\n",
    "def sphere_trace(sdf, camera_position, norm_directions, max_length):\n",
    "    N = norm_directions.shape[0]\n",
    "    positions = camera_position.unsqueeze(dim=0).repeat(N, 1) # [N, 3]\n",
    "    total_distances = torch.zeros(N)\n",
    "    last_distances = torch.ones(N)\n",
    "\n",
    "    for _ in range(20):\n",
    "        #mask = torch.logical_and(total_distances < max_length, last_distances > 1e-3)\n",
    "        not_reached_max_distance = total_distances < max_length\n",
    "        not_hit_target = torch.abs(last_distances) > 1e-3\n",
    "        mask = torch.logical_and(not_reached_max_distance, not_hit_target)\n",
    "        if torch.all(torch.logical_not(mask)):\n",
    "            break\n",
    "        distances = sdf(positions[mask])\n",
    "        steps = (norm_directions[mask].T * distances).T\n",
    "        positions[mask] += steps\n",
    "        total_distances[mask] += distances\n",
    "        last_distances[mask] = distances\n",
    "\n",
    "    #positions[total_distances > max_length] *= torch.nan\n",
    "    return positions, total_distances < max_length\n",
    "\n",
    "'''\n",
    "def mesh_trace(mesh_path, ray_starts, ray_directions):\n",
    "    mesh = trimesh.load_mesh(mesh_path)\n",
    "    \n",
    "    ray_origins = ray_starts.repeat(ray_directions.shape[0], 1)\n",
    "    \n",
    "    intersections, index_ray, index_tri = mesh.ray.intersects_location(ray_origins, ray_directions, multiple_hits=False) \n",
    "    \n",
    "    mask = torch.zeros(ray_directions.shape[0], dtype=torch.bool)\n",
    "    mask[index_ray] = True\n",
    "    \n",
    "    points = torch.as_tensor(intersections, dtype=torch.float32)\n",
    "\n",
    "    return points, mask, index_tri\n",
    "\n",
    "\n",
    "def mesh_normals(mesh_path, index_tri):\n",
    "    mesh = trimesh.load_mesh(mesh_path)\n",
    "    all_normals = mesh.face_normals\n",
    "    normals = all_normals[index_tri]\n",
    "    normals_torch = torch.from_numpy(normals).to(dtype=torch.float32)\n",
    "    \n",
    "    return normals_torch\n",
    "'''\n",
    "def render(model, lat_rep, camera_params, phong_params, light_params, mesh_path=None):\n",
    "    \n",
    "    def sdf(positions):\n",
    "        nphm_input = torch.reshape(positions, (1, -1, 3))\n",
    "        distance, _ = model(nphm_input, torch.reshape(lat_rep, (1, 1, -1)), None)\n",
    "        return distance.squeeze()\n",
    "\n",
    "    pu = camera_params[\"resolution_x\"]\n",
    "    pv = camera_params[\"resolution_y\"]\n",
    "    image = phong_params[\"background_color\"].repeat(pu * pv, 1)\n",
    "    \n",
    "    angle_radians = torch.deg2rad_(torch.tensor(camera_params[\"camera_angle\"]))\n",
    "    camera = torch.tensor([torch.sin(angle_radians), 0, torch.cos(angle_radians)])\n",
    "    camera_position = camera * (camera_params[\"camera_distance\"] + camera_params[\"focal_length\"]) / camera.norm()\n",
    "    \n",
    "    # Normalize the xy value of the current pixel [-0.5, 0.5]\n",
    "    u_norms = ((torch.arange(pu) + 0.5) / pu - 0.5) * pu/pv\n",
    "    v_norms = 0.5 - (torch.arange(pv) + 0.5) / pv\n",
    "\n",
    "    # Calculate the ray directions for all pixels\n",
    "    directions_unn = torch.cat(torch.meshgrid(u_norms, v_norms, torch.tensor(-camera_params[\"focal_length\"]), indexing='ij'), dim=-1) \n",
    "    directions_unn = directions_unn.reshape((pu*pv, 3)) # [pu, pv, 3] --> [pu*pv, 3] (u1, v1, f)(u1, v2, f)...(u2, v1, f)...\n",
    "\n",
    "    # rotate about y-axis\n",
    "    rotation_matrix = torch.tensor([[torch.cos(angle_radians), 0, torch.sin(angle_radians)],\n",
    "                                    [0, 1, 0],\n",
    "                                    [-torch.sin(angle_radians), 0, torch.cos(angle_radians)]])\n",
    "    rotated_directions = torch.matmul(directions_unn, rotation_matrix.T)\n",
    "    \n",
    "    transposed_directions = rotated_directions.T #transpose is necessary for normalization\n",
    "    directions = (transposed_directions / transposed_directions.norm(dim=0)).T # [pu*pv, 3]\n",
    "\n",
    "    # Option 1: Use SDF\n",
    "    hit_positions, hit_mask = sphere_trace(sdf, camera_position, directions, camera_params['max_ray_length'])\n",
    "    # Option 2: Use Mesh\n",
    "    #intersections, hit_mask, index_tri = mesh_trace(mesh_path, camera_position, directions) \n",
    "    \n",
    "    # Option 1: Use SDF\n",
    "    normals = None\n",
    "    reflections = phong_model(sdf, hit_positions[hit_mask], normals, camera_position, phong_params, light_params, mesh_path)\n",
    "    # Option 2: Use Mesh\n",
    "    #reflections = phong_model(sdf, intersections, camera_position, phong_params, light_params, mesh_path, index_tri) # mesh alternative\n",
    "\n",
    "    # Assign a color for objects\n",
    "    image[hit_mask] = torch.mul(reflections, phong_params[\"object_color\"].repeat(reflections.shape[0], 1))\n",
    "    image = torch.clamp(image, max=1.0)\n",
    "    image = image.reshape(pu, pv, 3).transpose(0, 1)\n",
    "\n",
    "\n",
    "    return image\n",
    "\n",
    "camera_params = {\n",
    "    \"camera_distance\": 0.48*3.77, #3.11, # [2, 6] distance of camera to origin\n",
    "    \"camera_angle\": 45., # [-90, 90] angle between z-axis and camera axis\n",
    "    \"focal_length\": 3.77,#6., # [2., 9.]\n",
    "    \"max_ray_length\": 3.11 + 6 + 1.5,\n",
    "    # Image\n",
    "    \"resolution_y\": 130,\n",
    "    \"resolution_x\": 130\n",
    "}\n",
    "\n",
    "phong_params = {\n",
    "    \"ambient_coeff\": 0.45,\n",
    "    \"diffuse_coeff\": 0.86,\n",
    "    \"specular_coeff\": 0.55,\n",
    "    \"shininess\": 0.5,\n",
    "    # Colors\n",
    "    \"object_color\": torch.tensor([0.81, 0.23, 0.91]),\n",
    "    \"background_color\": torch.tensor([0.1, 0.82, 0.32])\n",
    "}\n",
    "\n",
    "light_params = {\n",
    "    \"amb_light_color\": torch.tensor([0.88, 0.16, 0.73]),\n",
    "    # light 1\n",
    "    \"light_intensity_1\": 1.5, # [0, 1.5]\n",
    "    \"light_color_1\": torch.tensor([0.81, 0.9, 0.78]), # [0.5, 1]\n",
    "    \"light_dir_1\": torch.tensor([-0.61, -0.78, -0.91]), # [-1, 1]\n",
    "    # light 2\n",
    "    \"light_intensity_2\": 0.35, # [0, 1.5]\n",
    "    \"light_color_2\": torch.tensor([0.81, 0.9, 0.78]), # [0.5, 1]\n",
    "    \"light_dir_2\": torch.tensor([0, -0.48, -0.37]), # [-1, 1]\n",
    "    # light 3\n",
    "    \"light_intensity_3\": 0.13, # [0, 1.5]\n",
    "    \"light_color_3\": torch.tensor([0.81, 0.9, 0.78]), # [0.5, 1]\n",
    "    \"light_dir_3\": torch.tensor([0.14, -0.48, 0.]) # [-1, 1]\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    image = render(decoder_shape, lat_rep_id, camera_params, phong_params, light_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b812c09eea119",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "plt.imshow(image.detach().numpy())\n",
    "plt.axis('off')  # Turn off axes\n",
    "plt.show()\n",
    "\n",
    "# Save image\n",
    "#image = (image * 255).byte()  # Convert to a byte tensor (0-255)\n",
    "#image_np = image.cpu().numpy()  # Convert to a NumPy array\n",
    "#image_pil = Image.fromarray(image_np)\n",
    "\n",
    "# Save the image to a specific folder with a desired filename\n",
    "#output_folder = '/Users/katharinaschmid/Text2Head/rendering_data/'\n",
    "#output_filename = 'output_image.png'\n",
    "#image_pil.save(os.path.join(output_folder, output_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c649211cbdb2b40a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T14:12:59.538119Z",
     "start_time": "2023-11-30T14:07:25.466749Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lat_mean = torch.from_numpy(np.load(env_paths.ASSETS + 'nphm_lat_mean.npy')) # [1344]\n",
    "lat_rep_id = lat_mean\n",
    "lat_rep_expr = 0.2*torch.randn(CFG_expr['ex_decoder']['decoder_lat_dim_expr']) #[200]\n",
    "\n",
    "def render_expr(model_id, lat_rep_id, camera_params, phong_params, light_params, mesh_path=None):\n",
    "    \n",
    "    def sdf(positions):\n",
    "        nphm_input = torch.reshape(positions, (1, -1, 3))\n",
    "        distance, _ = model_id(nphm_input, torch.reshape(lat_rep_id, (1, 1, -1)), None)\n",
    "        return distance.squeeze()\n",
    "    print('ok')\n",
    "    pu = camera_params[\"resolution_x\"]\n",
    "    pv = camera_params[\"resolution_y\"]\n",
    "    image = phong_params[\"background_color\"].repeat(pu * pv, 1)\n",
    "    print(image.shape)\n",
    "    \n",
    "    angle_radians = torch.deg2rad_(torch.tensor(camera_params[\"camera_angle\"]))\n",
    "    camera = torch.tensor([torch.sin(angle_radians), 0, torch.cos(angle_radians)])\n",
    "    camera_position = camera * (camera_params[\"camera_distance\"] + camera_params[\"focal_length\"]) / camera.norm()\n",
    "    \n",
    "    # Normalize the xy value of the current pixel [-0.5, 0.5]\n",
    "    u_norms = ((torch.arange(pu) + 0.5) / pu - 0.5) * pu/pv\n",
    "    v_norms = 0.5 - (torch.arange(pv) + 0.5) / pv\n",
    "\n",
    "    # Calculate the ray directions for all pixels\n",
    "    directions_unn = torch.cat(torch.meshgrid(u_norms, v_norms, torch.tensor(-camera_params[\"focal_length\"]), indexing='ij'), dim=-1) \n",
    "    directions_unn = directions_unn.reshape((pu*pv, 3)) # [pu, pv, 3] --> [pu*pv, 3] (u1, v1, f)(u1, v2, f)...(u2, v1, f)...\n",
    "\n",
    "    # rotate about y-axis\n",
    "    rotation_matrix = torch.tensor([[torch.cos(angle_radians), 0, torch.sin(angle_radians)],\n",
    "                                    [0, 1, 0],\n",
    "                                    [-torch.sin(angle_radians), 0, torch.cos(angle_radians)]])\n",
    "    rotated_directions = torch.matmul(directions_unn, rotation_matrix.T)\n",
    "    \n",
    "    transposed_directions = rotated_directions.T #transpose is necessary for normalization\n",
    "    directions = (transposed_directions / transposed_directions.norm(dim=0)).T # [pu*pv, 3]\n",
    "\n",
    "    # get surface points in identity space\n",
    "    hit_positions, hit_mask = sphere_trace(sdf, camera_position, directions, camera_params['max_ray_length'])\n",
    "    x_c = hit_positions[hit_mask] # [N, 3]\n",
    "    \n",
    "    # get normals of surface points in identity space\n",
    "    normals_c = estimate_normals(sdf, x_c)\n",
    "    # get points to simulate normals\n",
    "    epsilon = 3e-3\n",
    "    n_c = x_c + epsilon * normals_c\n",
    "    points_c = torch.cat((x_c, n_c), dim=0).unsqueeze(0) # [B, 2N, 3]\n",
    "\n",
    "    # map surface points to expression space\n",
    "    glob_cond = torch.cat([lat_rep_id, lat_rep_expr], dim=-1) # [dim_id + dim_expr]\n",
    "    glob_cond = glob_cond.repeat(1, points_c.shape[1], 1) # [B, N, D]\n",
    "    delta, _ = decoder_expr(points_c, glob_cond, anchors.repeat(1, points_c.shape[1], 1, 1))\n",
    "    points_p = points_c + delta.squeeze() # [B, 2N, 3]\n",
    "    \n",
    "    # transform points from global to camera coord. system\n",
    "    points_p_camera = torch.matmul(points_p, rotation_matrix) - torch.tensor([0, 0, (camera_params['camera_distance'] + camera_params['focal_length'])]).unsqueeze(0) # [B, N, 3]\n",
    "    points_p_camera = points_p_camera.squeeze(0) # [2N, 3]\n",
    "    points_p_camera = points_p_camera.reshape(2, -1, 3)\n",
    "    x_p_camera = points_p_camera[0, :, :] # [N, 3]\n",
    "    normals_camera = points_p_camera[1, :, :] - points_p_camera[0, :, :] # [N, 3]\n",
    "\n",
    "    # perspective projection\n",
    "    x_image = camera_params['focal_length'] * x_p_camera[:, 0] / x_p_camera[:, 2] \n",
    "    y_image = camera_params['focal_length'] * x_p_camera[:, 1] / x_p_camera[:, 2] \n",
    "    \n",
    "    # pure magic: nach meinem Verständnis müsste das genau andersrum sein\n",
    "    u_indices = torch.floor_divide((1 - (x_image + 0.5)), (1/pu))\n",
    "    v_indices = torch.floor_divide((y_image + 0.5), (1/pv))\n",
    "    indices = torch.stack((u_indices, v_indices, x_image, y_image), dim=1) # [N, 4]\n",
    "    indices = torch.cat((indices, normals_camera, x_p_camera), dim=1) # [N, 4+3+3]\n",
    "    print(indices.shape)\n",
    "    # remove indices that are out of bound\n",
    "    indices_normals_red = indices[(indices[:, 0] < pu) & (indices[:, 1] < pv)] # [N, 2]\n",
    "    indices_red = indices_normals_red[:, :2] # [N, 2]\n",
    "    image_coord_red = indices_normals_red[:, 2:4] # [N, 2]\n",
    "    normals_red = indices_normals_red[:, 4:7] # [N, 3]\n",
    "    x_p_camera_red = indices_normals_red[:, 7:] # [N, 3]\n",
    "    \n",
    "    grid_cell_indices = indices_red[:, 0] * pv + indices_red[:, 1] # [N]\n",
    "    print(grid_cell_indices.shape)\n",
    "    \n",
    "    # Find Indices of potentially relevant but currently not included grid cells\n",
    "    min_index = torch.min(grid_cell_indices)\n",
    "    max_index = torch.max(grid_cell_indices)\n",
    "    relevant_indices = torch.arange(min_index, max_index)\n",
    "    missing_indices = relevant_indices[~torch.isin(relevant_indices, grid_cell_indices)].long()\n",
    "    print(missing_indices.shape)\n",
    "    \n",
    "    \n",
    "    r = []\n",
    "    for index in missing_indices:\n",
    "        # Achtung, Code ist nicht für Randbereiche gemacht --> funktioniert nur, wenn Kopf kleiner als Bild\n",
    "        # get surrounding indices\n",
    "        surrounding = torch.tensor([index-pu-1, index-pu, index-pu+1, index-1, index+1, index+pu-1, index+pu, index+pu+1])\n",
    "        positions = []\n",
    "        for value in surrounding:\n",
    "            if value.item() in grid_cell_indices.tolist():\n",
    "                position = (grid_cell_indices == value).nonzero(as_tuple=True)[0][0].item()\n",
    "                positions.append(position)\n",
    "        positions = torch.tensor(positions)\n",
    "        if positions.shape[0] >=4:\n",
    "            r.append(index)\n",
    "  \n",
    "            u_index_middle = index // pv\n",
    "            v_index_middle = index % pv\n",
    "            x_middle = 0.5 - u_index_middle * pu \n",
    "            y_middle = v_index_middle * pv - 0.5\n",
    "            middle = torch.tensor([x_middle, y_middle])\n",
    "            \n",
    "            distances = torch.norm(image_coord_red[positions, :] - middle, dim=1)\n",
    "            weights = 1 / distances\n",
    "            weights /= torch.sum(weights)\n",
    "            interpolated_normal = torch.sum(weights.unsqueeze(-1) * normals_red[positions], dim=0)\n",
    "            interpolated_z = torch.sum(weights * x_p_camera_red[positions, 2], dim=0)\n",
    "            print(interpolated_z.shape)\n",
    "            x_p_interpolated = torch.tensor([x_middle*interpolated_z/camera_params['focal_length'], y_middle*interpolated_z/camera_params['focal_length'], interpolated_z])\n",
    "            \n",
    "            grid_cell_indices = torch.cat((grid_cell_indices, index.unsqueeze(0)), dim=0)\n",
    "            image_coord_red = torch.cat((image_coord_red, middle.unsqueeze(0)), dim=0)\n",
    "            normals_red = torch.cat((normals_red, interpolated_normal.unsqueeze(0)), dim=0)\n",
    "            x_p_camera_red = torch.cat((x_p_camera_red, x_p_interpolated.unsqueeze(0)), dim=0)\n",
    "    print(x_p_camera_red.shape, normals_red.shape)\n",
    "    \n",
    "    r = torch.tensor(r).long()\n",
    "    grid_cell_indices = grid_cell_indices.long()\n",
    "    ##\n",
    "    unique_values, counts = torch.unique(grid_cell_indices, return_counts=True)\n",
    "    print(counts)\n",
    "    # Find the index of the maximum count\n",
    "    max_count_index = torch.argmax(counts)\n",
    "    occurrences = counts[max_count_index].item()\n",
    "    print(occurrences)\n",
    "    ##\n",
    "    \n",
    "    reflections = phong_model(sdf, x_p_camera_red, normals_red, camera_position, phong_params, light_params, mesh_path)\n",
    "    \n",
    "    image[grid_cell_indices] = torch.mul(reflections, phong_params[\"object_color\"].repeat(reflections.shape[0], 1))\n",
    "    #image[r] = torch.tensor([0, 0.5, 0]).repeat(r.shape[0], 1)\n",
    "    image = torch.clamp(image, max=1.0)\n",
    "    image = torch.clamp(image, max=1.0)\n",
    "    image = image.reshape(pu, pv, 3).transpose(0, 1)\n",
    "    print(image.shape)\n",
    "    \n",
    "    return image\n",
    "\n",
    "camera_params = {\n",
    "    \"camera_distance\": 0.4, #3.11, # [2, 6] distance of camera to origin\n",
    "    \"camera_angle\": 20., # [-90, 90] angle between z-axis and camera axis\n",
    "    \"focal_length\": 2.,#6., # [2., 9.]\n",
    "    \"max_ray_length\": 3.11 + 6 + 1.5,\n",
    "    # Image\n",
    "    \"resolution_y\": 150,\n",
    "    \"resolution_x\": 150\n",
    "}\n",
    "\n",
    "phong_params = {\n",
    "    \"ambient_coeff\": 0.5,\n",
    "    \"diffuse_coeff\": 0.7,\n",
    "    \"specular_coeff\": 0.0,\n",
    "    \"shininess\": 3.0,\n",
    "    # Colors\n",
    "    \"object_color\": torch.tensor([0.61, 0.61, 0.61]),\n",
    "    \"background_color\": torch.tensor([0.46, 0, 0])\n",
    "}\n",
    "\n",
    "light_params = {\n",
    "    \"amb_light_color\": torch.tensor([0.15, 0, 0]),\n",
    "    # light 1\n",
    "    \"light_intensity_1\": 1., # [0, 1.5]\n",
    "    \"light_color_1\": torch.tensor([1.0, 1.0, 1.0]), # [0.5, 1]\n",
    "    \"light_dir_1\": torch.tensor([-1., -1., -1.]), # [-1, 1]\n",
    "    # light 2\n",
    "    \"light_intensity_2\": 1., # [0, 1.5]\n",
    "    \"light_color_2\": torch.tensor([1.0, 1.0, 1.0]), # [0.5, 1]\n",
    "    \"light_dir_2\": torch.tensor([0., 0., -1.]), # [-1, 1]\n",
    "    # light 3\n",
    "    \"light_intensity_3\": 0., # [0, 1.5]\n",
    "    \"light_color_3\": torch.tensor([1.0, 1.0, 1.0]), # [0.5, 1]\n",
    "    \"light_dir_3\": torch.tensor([0., -1., 0.]) # [-1, 1]\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    image = render_expr(decoder_shape, lat_rep_id, camera_params, phong_params, light_params)\n",
    "    \n",
    "plt.imshow(image.detach().numpy())\n",
    "plt.axis('off')  # Turn off axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192d813771b294e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T16:08:34.071684Z",
     "start_time": "2023-11-29T16:08:34.014234Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = torch.tensor([[1, 2, 3], [2, 2, 3], [3, 2, 3]])\n",
    "list = torch.tensor([1, 2])\n",
    "print(a[list, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27901b42864b661c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
