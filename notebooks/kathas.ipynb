{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T06:49:13.870237Z",
     "start_time": "2023-10-27T06:49:05.158350Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from NPHM.models.deepSDF import DeepSDF, DeformationNetwork\n",
    "from NPHM.models.EnsembledDeepSDF import FastEnsembleDeepSDFMirrored\n",
    "from NPHM import env_paths\n",
    "from NPHM.utils.reconstruction import create_grid_points_from_bounds, mesh_from_logits\n",
    "from NPHM.models.reconstruction import deform_mesh, get_logits, get_logits_backward\n",
    "from NPHM.models.fitting import inference_iterative_root_finding_joint, inference_identity_space\n",
    "from NPHM.data.manager import DataManager\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "import json, yaml\n",
    "import os\n",
    "import os.path as osp\n",
    "import torch\n",
    "import pyvista as pv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913c0d883af35f3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T06:49:13.926591Z",
     "start_time": "2023-10-27T06:49:13.878573Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resolution = 35\n",
    "\n",
    "with open('NPHM/scripts/configs/fitting_nphm.yaml', 'r') as f:\n",
    "    print('Loading config file from: ' + 'scripts/configs/fitting_nphm.yaml')\n",
    "    CFG = yaml.safe_load(f)\n",
    "\n",
    "print(json.dumps(CFG, sort_keys=True, indent=4))\n",
    "\n",
    "weight_dir_shape = env_paths.EXPERIMENT_DIR + '/{}/'.format(CFG['exp_name_shape'])\n",
    "\n",
    "# load config files\n",
    "fname_shape = weight_dir_shape + 'configs.yaml'\n",
    "with open(fname_shape, 'r') as f:\n",
    "    print('Loading config file from: ' + fname_shape)\n",
    "    CFG_shape = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f4a9b3f9a858d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T06:49:13.949789Z",
     "start_time": "2023-10-27T06:49:13.914981Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b95c7271da459bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T06:49:14.039300Z",
     "start_time": "2023-10-27T06:49:13.940804Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('###########################################################################')\n",
    "print('####################     Shape Model Configs     #############################')\n",
    "print('###########################################################################')\n",
    "print(json.dumps(CFG_shape, sort_keys=True, indent=4))\n",
    "\n",
    "lm_inds = np.load(env_paths.ANCHOR_INDICES_PATH)\n",
    "anchors = torch.from_numpy(np.load(env_paths.ANCHOR_MEAN_PATH)).float().unsqueeze(0).unsqueeze(0).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fe99f912dbd8da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T06:49:14.348217Z",
     "start_time": "2023-10-27T06:49:13.971819Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoder_shape = FastEnsembleDeepSDFMirrored(\n",
    "        lat_dim_glob=CFG_shape['decoder']['decoder_lat_dim_glob'],\n",
    "        lat_dim_loc=CFG_shape['decoder']['decoder_lat_dim_loc'],\n",
    "        hidden_dim=CFG_shape['decoder']['decoder_hidden_dim'],\n",
    "        n_loc=CFG_shape['decoder']['decoder_nloc'],\n",
    "        n_symm_pairs=CFG_shape['decoder']['decoder_nsymm_pairs'],\n",
    "        anchors=anchors,\n",
    "        n_layers=CFG_shape['decoder']['decoder_nlayers'],\n",
    "        pos_mlp_dim=CFG_shape['decoder'].get('pos_mlp_dim', 256),\n",
    "    )\n",
    "\n",
    "decoder_shape = decoder_shape.to(device)\n",
    "\n",
    "path = osp.join(weight_dir_shape, 'checkpoints/checkpoint_epoch_{}.tar'.format(CFG['checkpoint_shape']))\n",
    "print('Loaded checkpoint from: {}'.format(path))\n",
    "checkpoint = torch.load(path, map_location=device)\n",
    "decoder_shape.load_state_dict(checkpoint['decoder_state_dict'], strict=True)\n",
    "\n",
    "if 'latent_codes_state_dict' in checkpoint:\n",
    "    n_train_subjects = checkpoint['latent_codes_state_dict']['weight'].shape[0]\n",
    "    n_val_subjects = checkpoint['latent_codes_val_state_dict']['weight'].shape[0]\n",
    "    latent_codes_shape = torch.nn.Embedding(n_train_subjects, 512)\n",
    "    latent_codes_shape_val = torch.nn.Embedding(n_val_subjects, 512)\n",
    "    \n",
    "    latent_codes_shape.load_state_dict(checkpoint['latent_codes_state_dict'])\n",
    "    latent_codes_shape_val.load_state_dict(checkpoint['latent_codes_val_state_dict'])\n",
    "else:\n",
    "    print('no latent codes in state dict')\n",
    "    latent_codes_shape = None\n",
    "    latent_codes_shape_val = None\n",
    "\n",
    "decoder_expr = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acc7fb1d2843184",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T06:49:14.419400Z",
     "start_time": "2023-10-27T06:49:14.312237Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lat_mean = torch.from_numpy(np.load(env_paths.ASSETS + 'nphm_lat_mean.npy'))\n",
    "lat_std = torch.from_numpy(np.load(env_paths.ASSETS + 'nphm_lat_std.npy'))\n",
    "\n",
    "lat_rep = (torch.randn(lat_mean.shape) * lat_std * 0.85 + lat_mean)\n",
    "print(lat_rep.shape) #40*32+64\n",
    "\n",
    "mini = [-.55, -.5, -.95]\n",
    "maxi = [0.55, 0.75, 0.4]\n",
    "\n",
    "grid_points = create_grid_points_from_bounds(mini, maxi, resolution)\n",
    "print(grid_points)\n",
    "grid_points = torch.from_numpy(grid_points).to(device, dtype=torch.float)\n",
    "grid_points = torch.reshape(grid_points, (1, len(grid_points), 3)).to(device)\n",
    "print(grid_points.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8122d38cf8eedae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T06:49:22.113215Z",
     "start_time": "2023-10-27T06:49:16.357691Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logits = get_logits(decoder_shape, lat_rep, grid_points, nbatch_points=100)\n",
    "print('starting mcubes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7399f21412db51ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T06:44:24.207808Z",
     "start_time": "2023-10-27T06:44:22.695748Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mesh = mesh_from_logits(logits, mini, maxi, resolution)\n",
    "print('done mcubes')\n",
    "\n",
    "pl = pv.Plotter(off_screen=True)\n",
    "pl.add_mesh(mesh)\n",
    "pl.reset_camera()\n",
    "pl.camera.position = (0, 0, 3)\n",
    "pl.camera.zoom(1.4)\n",
    "pl.set_viewup((0, 1, 0)) #vertical direction of camera = +Y axis\n",
    "pl.camera.view_plane_normal = (-0, -0, 1) #camera is looking at XY plane\n",
    "pl.show()\n",
    "#pl.show(screenshot=out_dir + '/step_{:04d}.png'.format(step))\n",
    "#mesh.export(out_dir + '/mesh_{:04d}.ply'.format(step))\n",
    "print(pl.camera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203c876657988fdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T11:04:32.622022Z",
     "start_time": "2023-10-27T11:04:20.553768Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from render import render\n",
    "\n",
    "# Define rendering parameters\n",
    "camera_position = torch.tensor([0.0, 0.0, 2.0])\n",
    "max_ray_length = 4.\n",
    "\n",
    "# Define phong model constants\n",
    "ambient_coeff = 0.1\n",
    "diffuse_coeff = 0.6\n",
    "specular_coeff = 0.3\n",
    "shininess = 32.0\n",
    "\n",
    "# Define light inputs\n",
    "light_position = torch.tensor([2.0, 1.0, 3.0])\n",
    "\n",
    "def sdf_nphm(positions):\n",
    "    nphm_input = torch.reshape(positions, (1, -1, 3))\n",
    "    distance, _ = decoder_shape(nphm_input, torch.reshape(lat_rep, (1, 1, -1)), None)\n",
    "    return distance.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab188098",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = render(sdf_nphm, 50, camera_position, light_position, ambient_coeff, diffuse_coeff, specular_coeff, shininess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f4603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rendering parameters\n",
    "res = 50\n",
    "camera_position = torch.tensor([0.0, 0.0, 3.0])\n",
    "max_ray_length = 4 - 2.3\n",
    "\n",
    "# Define phong model constants\n",
    "ambient_coeff = 0.1\n",
    "diffuse_coeff = 0.6\n",
    "specular_coeff = 0.3\n",
    "shininess = 32.0\n",
    "\n",
    "# Define light inputs\n",
    "light_position = torch.tensor([2.0, 1.0, 3.0])\n",
    "\n",
    "# Create an empty image\n",
    "image = torch.zeros((res, res, 3))\n",
    "#image = 0.01 * torch.ones((res, res, 3))\n",
    "\n",
    "def phong_model(normal, light_dir, view_dir):\n",
    "    # Normalize all vectors\n",
    "    normal = normal / torch.norm(normal, dim=-1)\n",
    "    light_dir = light_dir / torch.norm(light_dir, dim=-1)\n",
    "    view_dir = view_dir / torch.norm(view_dir, dim=-1)\n",
    "    \n",
    "    ambient = ambient_coeff\n",
    "    diffuse = diffuse_coeff * torch.clamp(torch.sum(light_dir * normal, dim=-1), min=0.0)\n",
    "    reflect_dir = light_dir - 2 * normal * torch.clamp(torch.sum(light_dir * normal, dim=-1), min=0.0)\n",
    "    specular = specular_coeff * torch.pow(torch.clamp(torch.sum(reflect_dir * view_dir, dim=-1), min=0.0), shininess)\n",
    "\n",
    "    return ambient + diffuse + specular\n",
    "\n",
    "def estimate_normal(sdf, point, epsilon=1e-3):\n",
    "    # Calculate the SDF value at the given point\n",
    "    sdf_value = sdf(point)\n",
    "\n",
    "    # Calculate SDF values at neighboring points\n",
    "    sdf_dx = sdf(point + torch.tensor([epsilon, 0, 0]))\n",
    "    sdf_dy = sdf(point + torch.tensor([0, epsilon, 0]))\n",
    "    sdf_dz = sdf(point + torch.tensor([0, 0, epsilon]))\n",
    "\n",
    "    # Calculate the gradient using finite differences\n",
    "    gradient = torch.tensor([sdf_dx - sdf_value, sdf_dy - sdf_value, sdf_dz - sdf_value])\n",
    "\n",
    "    # Normalize the gradient to obtain the estimated normal\n",
    "    normal = gradient / torch.norm(gradient, p=2)\n",
    "    \n",
    "    return normal\n",
    "\n",
    "def sdf_sphere(position, radius=0.75):\n",
    "    return torch.norm(position, dim=-1) - radius\n",
    "\n",
    "def sdf_nphm(position):\n",
    "    position = position.unsqueeze(0).unsqueeze(0) # [1, N, 3], lat_rep [lat_dim]\n",
    "    distance, _ = decoder_shape(position, lat_rep.repeat(1, position.shape[1], 1), None)\n",
    "    return distance\n",
    "\n",
    "def ray_march(camera_position, direction, max_length):\n",
    "    position = camera_position + 2.3 * direction\n",
    "    step_size = 0.01\n",
    "\n",
    "    for _ in range(int(max_length / step_size)):\n",
    "        #distance = sdf_sphere(position)  # Replace with your SDF function\n",
    "        distance = sdf_nphm(position)\n",
    "        if distance < 0.01:\n",
    "            return position  # Ray hits the surface\n",
    "\n",
    "        position += step_size * direction\n",
    "\n",
    "    return None  # Ray misses the scene\n",
    "\n",
    "# Rendering loop\n",
    "for v in range(res):\n",
    "    for u in range(res):\n",
    "        # Normalize the xy value of the current pixel [-1, 1]\n",
    "        u_norm = (2.0 * (u + 0.5) / res - 1.0)\n",
    "        v_norm = 1.0 - 2.0 * (v + 0.5) / res\n",
    "        u_norm = torch.tensor([u_norm])\n",
    "        v_norm = torch.tensor([v_norm])\n",
    "         # Calculate the ray direction for the current pixel\n",
    "        direction_unn = torch.tensor([u_norm, v_norm, -3.0])\n",
    "        direction = direction_unn / torch.norm(direction_unn, dim=-1)\n",
    "\n",
    "        # Perform ray marching\n",
    "        hit_position = ray_march(camera_position, direction, max_ray_length)\n",
    "\n",
    "        # Color the pixel based on whether the ray hits an object\n",
    "        if hit_position is not None:\n",
    "            normal = estimate_normal(sdf_sphere, hit_position)\n",
    "            light_dir = - (hit_position - light_position) # umdrehen, damit L*V >0\n",
    "            view_dir = - (camera_position - hit_position) # umdrehen, damit dot product nicht kleienr null?\n",
    "            reflection = phong_model(normal, light_dir, view_dir)\n",
    "            # Assign a color for objects\n",
    "            image[v, u] = reflection * torch.tensor([1.0, 1.0, 1.0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86b8f400259555",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T11:04:37.406375Z",
     "start_time": "2023-10-27T11:04:37.271427Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display the image using Matplotlib\n",
    "plt.imshow(image.detach().numpy())\n",
    "plt.axis('off')  # Turn off axes\n",
    "plt.show()\n",
    "print(image[:, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f61807e95407c21",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
