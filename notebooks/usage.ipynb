{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nphm_tum import env_paths as mono_env_paths\n",
    "from nphm_tum.models.neural3dmm import construct_n3dmm, load_checkpoint\n",
    "from utils.pipeline import get_image_clip_embedding, get_latent_from_text\n",
    "import json, yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "weight_dir_shape = mono_env_paths.EXPERIMENT_DIR_REMOTE + '/'\n",
    "fname_shape = weight_dir_shape + 'configs.yaml'\n",
    "with open(fname_shape, 'r') as f:\n",
    "    CFG = yaml.safe_load(f)\n",
    "\n",
    "# load participant IDs that were used for training\n",
    "fname_subject_index = f\"{weight_dir_shape}/subject_train_index.json\"\n",
    "with open(fname_subject_index, 'r') as f:\n",
    "    subject_index = json.load(f)\n",
    "\n",
    "# load expression indices that were used for training\n",
    "fname_subject_index = f\"{weight_dir_shape}/expression_train_index.json\"\n",
    "with open(fname_subject_index, 'r') as f:\n",
    "    expression_index = json.load(f)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "modalities = ['geo', 'exp', 'app']\n",
    "n_lats = [len(subject_index), len(expression_index), len(subject_index)]\n",
    "\n",
    "_, latent_codes = construct_n3dmm(\n",
    "    cfg=CFG,\n",
    "    modalities=modalities,\n",
    "    n_latents=n_lats,\n",
    "    device=device,\n",
    "    include_color_branch=True\n",
    "    )\n",
    "\n",
    "def get_latent_mean():\n",
    "    geo_mean = latent_codes.codebook['geo'].embedding.weight.mean(dim=0)\n",
    "    geo_std = latent_codes.codebook['geo'].embedding.weight.std(dim=0)\n",
    "    exp_mean = latent_codes.codebook['exp'].embedding.weight.mean(dim=0)\n",
    "    exp_std = latent_codes.codebook['exp'].embedding.weight.std(dim=0)\n",
    "    app_mean = latent_codes.codebook['app'].embedding.weight.mean(dim=0).detach()\n",
    "    app_std = latent_codes.codebook['app'].embedding.weight.std(dim=0).detach()\n",
    "\n",
    "    lat_rep = [geo_mean, exp_mean, app_mean]\n",
    "\n",
    "    print('mean', app_mean.shape)\n",
    "    print('std', app_std.shape)\n",
    "    print('geo', geo_mean.shape)\n",
    "    print('exp', exp_mean.shape)\n",
    "\n",
    "    return lat_rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter prompt here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Kate Winslet'\n",
    "\n",
    "hparams = {\n",
    "        'exp_name': 'test',\n",
    "        'resolution': 180,\n",
    "        'n_iterations': 50,\n",
    "        'lambda_geo': 0.6,\n",
    "        'lambda_app': 0.6,\n",
    "        'gamma_geo': 0., \n",
    "        'gamma_app': 0.,\n",
    "        'alpha': 0.1,\n",
    "        'color_prob': 0.3,\n",
    "        'optimizer_lr': 0.2,  \n",
    "        'optimizer_lr_app': 0.2, \n",
    "        'optimizer_beta1': 0.9,\n",
    "        'batch_size': 10,\n",
    "        'lr_scheduler_factor': 0.95,\n",
    "        'lr_scheduler_patience': 2, \n",
    "        'lr_scheduler_min_lr': 0.01,\n",
    "    }\n",
    "\n",
    "lat_mean = get_latent_mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_latent, _ = get_latent_from_text(prompt, hparams, init_lat=lat_mean, CLIP_gt=None, DINO_gt=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 700\n",
    "\n",
    "camera_params = {\n",
    "            \"camera_distance\": 0.21 * 2.57,\n",
    "            \"camera_angle_rho\": 0.,\n",
    "            \"camera_angle_theta\": 0.,\n",
    "            \"focal_length\": 2.57,\n",
    "            \"max_ray_length\": 3.5,\n",
    "            # Image\n",
    "            \"resolution_y\": resolution,\n",
    "            \"resolution_x\": resolution\n",
    "        }\n",
    "\n",
    "phong_params = {\n",
    "            \"ambient_coeff\": 0.32,\n",
    "            \"diffuse_coeff\": 0.85,\n",
    "            \"specular_coeff\": 0.34,\n",
    "            \"shininess\": 25,\n",
    "            # Colors\n",
    "            \"background_color\": torch.tensor([1., 1., 1.])\n",
    "        }\n",
    "\n",
    "light_params = {\n",
    "            \"amb_light_color\": torch.tensor([0.65, 0.65, 0.65]),\n",
    "            # light 1\n",
    "            \"light_intensity_1\": 1.69,\n",
    "            \"light_color_1\": torch.tensor([1., 1., 1.]),\n",
    "            \"light_dir_1\": torch.tensor([0, -0.18, -0.8]),\n",
    "            # light p\n",
    "            \"light_intensity_p\": 0.52,\n",
    "            \"light_color_p\": torch.tensor([1., 1., 1.]),\n",
    "            \"light_pos_p\": torch.tensor([0.17, 2.77, -2.25])\n",
    "    }\n",
    "\n",
    "_, image = get_image_clip_embedding(best_latent, camera_params, phong_params, light_params, with_app_grad=False, color=True)\n",
    "plt.imshow(image.detach().numpy())\n",
    "plt.axis('off')  # Turn off axes\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
