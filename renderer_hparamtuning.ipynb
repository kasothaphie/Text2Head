{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-27T12:42:51.181186Z",
     "start_time": "2023-11-27T12:42:26.801625Z"
    }
   },
   "outputs": [],
   "source": [
    "import clip\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import trimesh\n",
    "import time\n",
    "import optuna\n",
    "from torchvision.transforms import Compose, Normalize, Resize, CenterCrop, InterpolationMode\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Rendering Setup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98e57945907ff5f2"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# --- Hyperparameter Setup ---\n",
    "\n",
    "hparams = {\n",
    "    # Camera\n",
    "    \"camera_distance_factor\": 3.11/6.,\n",
    "    \"camera_angle\": 45.,\n",
    "    \"focal_length\": 6.,\n",
    "    # Image\n",
    "    \"resolution\": 224,\n",
    "    # Phong\n",
    "    \"ambient_coeff\": 0.5,\n",
    "    \"diffuse_coeff\": 0.7,\n",
    "    \"specular_coeff\": 0.0,\n",
    "    \"shininess\": 3.0,\n",
    "    # Colors\n",
    "    \"object_color\": torch.tensor([0.61, 0.61, 0.61]),\n",
    "    \"background_color\": torch.tensor([0.46, 0, 0]),\n",
    "    # Light\n",
    "    \"amb_light_color\": torch.tensor([0.15, 0, 0]),\n",
    "    # light 1\n",
    "    \"light_intensity_1\": 1., \n",
    "    \"light_color\": torch.tensor([1.0, 1.0, 1.0]), \n",
    "    \"light_dir_1\": torch.tensor([-1., -1., -1.]), \n",
    "    # light 2\n",
    "    \"light_intensity_2\": 1.,\n",
    "    \"light_dir_2\": torch.tensor([0., 0., -1.]), \n",
    "    # light 3\n",
    "    \"light_intensity_3\": 0., \n",
    "    \"light_dir_3\": torch.tensor([0., -1., 0.])\n",
    "}\n",
    "\n",
    "# --- Rendering Function ---\n",
    "\n",
    "def phong_model(sdf, points, camera_position, phong_params, light_params, mesh_path, index_tri=None):\n",
    "    \n",
    "    # Option 1: Use SDF\n",
    "    #normals = estimate_normals(sdf, points)\n",
    "    # Option 2: Use Mesh\n",
    "    normals = mesh_normals(mesh_path, index_tri)\n",
    "    view_dirs = points - camera_position \n",
    "    light_dir_1 = light_params[\"light_dir_1\"].repeat(points.shape[0], 1) \n",
    "    light_dir_2 = light_params[\"light_dir_2\"].repeat(points.shape[0], 1)\n",
    "    light_dir_3 = light_params[\"light_dir_3\"].repeat(points.shape[0], 1)\n",
    "    \n",
    "    # Normalize all vectors\n",
    "    normals = (normals.T / torch.norm(normals, dim=-1)).T\n",
    "    light_dir_norm_1 = (light_dir_1.T / torch.norm(light_dir_1, dim=-1)).T\n",
    "    light_dir_norm_2 = (light_dir_2.T / torch.norm(light_dir_2, dim=-1)).T\n",
    "    light_dir_norm_3 = (light_dir_3.T / torch.norm(light_dir_3, dim=-1)).T\n",
    "    view_dir_norm = (view_dirs.T / torch.norm(view_dirs, dim=-1)).T\n",
    "    \n",
    "    # Ambient\n",
    "    ambient = phong_params[\"ambient_coeff\"] * light_params[\"amb_light_color\"] \n",
    "    ambient_refl = ambient.repeat(points.shape[0], 1)\n",
    "    \n",
    "    # Area light 1\n",
    "    diffuse_1 = phong_params[\"diffuse_coeff\"] * torch.clamp(torch.sum(-light_dir_norm_1 * normals, dim=-1), min=0.0) * light_params[\"light_intensity_1\"] # [N]\n",
    "    diffuse_refl_1 = torch.matmul(diffuse_1.unsqueeze(1), light_params[\"light_color_1\"].unsqueeze(0)) # [N, 3]\n",
    "    reflect_dir_1 = light_dir_norm_1 + (2 * normals.T * torch.clamp(torch.sum(-light_dir_norm_1 * normals, dim=-1), min=0.0)).T\n",
    "    specular_1 = phong_params[\"specular_coeff\"] * torch.pow(torch.clamp(torch.sum(reflect_dir_1 * -view_dir_norm, dim=-1), min=0.0), phong_params[\"shininess\"]) * light_params[\"light_intensity_1\"] # [N]\n",
    "    specular_refl_1 = torch.matmul(specular_1.unsqueeze(1), light_params[\"light_color_1\"].unsqueeze(0)) # [N, 3]\n",
    "    \n",
    "    # Area light 2\n",
    "    diffuse_2 = phong_params[\"diffuse_coeff\"] * torch.clamp(torch.sum(-light_dir_norm_2 * normals, dim=-1), min=0.0) * light_params[\"light_intensity_2\"] # [N]\n",
    "    diffuse_refl_2 = torch.matmul(diffuse_2.unsqueeze(1), light_params[\"light_color_2\"].unsqueeze(0)) # [N, 3]\n",
    "    reflect_dir_2 = light_dir_norm_2 + (2 * normals.T * torch.clamp(torch.sum(-light_dir_norm_2 * normals, dim=-1), min=0.0)).T\n",
    "    specular_2 = phong_params[\"specular_coeff\"] * torch.pow(torch.clamp(torch.sum(reflect_dir_2 * -view_dir_norm, dim=-1), min=0.0), phong_params[\"shininess\"]) * light_params[\"light_intensity_2\"] # [N]\n",
    "    specular_refl_2 = torch.matmul(specular_2.unsqueeze(1), light_params[\"light_color_2\"].unsqueeze(0)) # [N, 3]\n",
    "    \n",
    "    # Area light 3\n",
    "    diffuse_3 = phong_params[\"diffuse_coeff\"] * torch.clamp(torch.sum(-light_dir_norm_3 * normals, dim=-1), min=0.0) * light_params[\"light_intensity_3\"] # [N]\n",
    "    diffuse_refl_3 = torch.matmul(diffuse_3.unsqueeze(1), light_params[\"light_color_3\"].unsqueeze(0)) # [N, 3]\n",
    "    reflect_dir_3 = light_dir_norm_3 + (2 * normals.T * torch.clamp(torch.sum(-light_dir_norm_3 * normals, dim=-1), min=0.0)).T\n",
    "    specular_3 = phong_params[\"specular_coeff\"] * torch.pow(torch.clamp(torch.sum(reflect_dir_3 * -view_dir_norm, dim=-1), min=0.0), phong_params[\"shininess\"]) * light_params[\"light_intensity_3\"] # [N]\n",
    "    specular_refl_3 = torch.matmul(specular_3.unsqueeze(1), light_params[\"light_color_3\"].unsqueeze(0)) # [N, 3]\n",
    "    \n",
    "    # point light\n",
    "    #diffuse_1 = diffuse_coeff * torch.clamp(torch.sum(-light_dir_norm_1 * normals, dim=-1), min=0.0) * light_intensity_1 # [N]\n",
    "    #diffuse_refl_1 = torch.matmul(diffuse_1.unsqueeze(1), light_color.unsqueeze(0)) # [N, 3]\n",
    "    #reflect_dir_1 = light_dir_norm_1 + (2 * normals.T * torch.clamp(torch.sum(-light_dir_norm_1 * normals, dim=-1), min=0.0)).T\n",
    "    #specular_1 = specular_coeff * torch.pow(torch.clamp(torch.sum(reflect_dir_1 * -view_dir_norm, dim=-1), min=0.0), shininess) * light_intensity_1 # [N]\n",
    "    #specular_refl_1 = torch.matmul(specular_1.unsqueeze(1), light_color.unsqueeze(0)) # [N, 3]\n",
    "\n",
    "    return ambient_refl + diffuse_refl_1 + specular_refl_1 + diffuse_refl_2 + specular_refl_2 + diffuse_refl_3 + specular_refl_3\n",
    "\n",
    "def estimate_normals(sdf, points, epsilon=1e-3):\n",
    "    sdf_inputs = torch.concat([points,\n",
    "                              points + torch.tensor([epsilon, 0, 0]),\n",
    "                              points + torch.tensor([0, epsilon, 0]),\n",
    "                              points + torch.tensor([0, 0, epsilon])])\n",
    "\n",
    "    sdf_values = sdf(sdf_inputs).reshape(4, -1)\n",
    "\n",
    "    # Calculate the gradient using finite differences\n",
    "    gradient = sdf_values[1:] - sdf_values[0]\n",
    "\n",
    "    # Normalize the gradient to obtain the estimated normal\n",
    "    normal = gradient / torch.norm(gradient, p=2, dim=0)\n",
    "\n",
    "    return normal.T\n",
    "\n",
    "def sphere_trace(sdf, camera_position, norm_directions, max_length):\n",
    "    N = norm_directions.shape[0]\n",
    "    positions = camera_position.unsqueeze(dim=0).repeat(N, 1) # [N, 3]\n",
    "    total_distances = torch.zeros(N)\n",
    "    last_distances = torch.ones(N)\n",
    "\n",
    "    for _ in range(20):\n",
    "        #mask = torch.logical_and(total_distances < max_length, last_distances > 1e-3)\n",
    "        not_reached_max_distance = total_distances < max_length\n",
    "        not_hit_target = torch.abs(last_distances) > 1e-3\n",
    "        mask = torch.logical_and(not_reached_max_distance, not_hit_target)\n",
    "        if torch.all(torch.logical_not(mask)):\n",
    "            break\n",
    "        distances = sdf(positions[mask])\n",
    "        steps = (norm_directions[mask].T * distances).T\n",
    "        positions[mask] += steps\n",
    "        total_distances[mask] += distances\n",
    "        last_distances[mask] = distances\n",
    "\n",
    "    #positions[total_distances > max_length] *= torch.nan\n",
    "    return positions, total_distances < max_length\n",
    "\n",
    "def mesh_trace(mesh_path, ray_starts, ray_directions):\n",
    "    mesh = trimesh.load_mesh(mesh_path)\n",
    "    \n",
    "    ray_origins = ray_starts.repeat(ray_directions.shape[0], 1)\n",
    "    \n",
    "    intersections, index_ray, index_tri = mesh.ray.intersects_location(ray_origins, ray_directions, multiple_hits=False) \n",
    "    \n",
    "    mask = torch.zeros(ray_directions.shape[0], dtype=torch.bool)\n",
    "    mask[index_ray] = True\n",
    "    \n",
    "    points = torch.as_tensor(intersections, dtype=torch.float32)\n",
    "\n",
    "    return points, mask, index_tri\n",
    "\n",
    "\n",
    "def mesh_normals(mesh_path, index_tri):\n",
    "    mesh = trimesh.load_mesh(mesh_path)\n",
    "    all_normals = mesh.face_normals\n",
    "    normals = all_normals[index_tri]\n",
    "    normals_torch = torch.from_numpy(normals).to(dtype=torch.float32)\n",
    "    \n",
    "    return normals_torch\n",
    "\n",
    "def render(sdf, camera_params, phong_params, light_params, mesh_path=None):\n",
    "    t_s = time.time()\n",
    "    pu = camera_params[\"resolution_x\"]\n",
    "    pv = camera_params[\"resolution_y\"]\n",
    "    image = phong_params[\"background_color\"].repeat(pu * pv, 1)\n",
    "    \n",
    "    angle_radians = torch.deg2rad_(torch.tensor(camera_params[\"camera_angle\"]))\n",
    "    camera = torch.tensor([torch.sin(angle_radians), 0, torch.cos(angle_radians)])\n",
    "    camera_position = camera * (camera_params[\"camera_distance\"] + camera_params[\"focal_length\"]) / camera.norm()\n",
    "    \n",
    "    # Normalize the xy value of the current pixel [-0.5, 0.5]\n",
    "    u_norms = ((torch.arange(pu) + 0.5) / pu - 0.5) * pu/pv\n",
    "    v_norms = 0.5 - (torch.arange(pv) + 0.5) / pv\n",
    "\n",
    "    # Calculate the ray directions for all pixels\n",
    "    directions_unn = torch.cat(torch.meshgrid(u_norms, v_norms, torch.tensor(-camera_params[\"focal_length\"])), dim=-1) \n",
    "    directions_unn = directions_unn.reshape((pu*pv, 3)) # [pu, pv, 3] --> [pu*pv, 3] (u1, v1, f)(u1, v2, f)...(u2, v1, f)...\n",
    "\n",
    "    # rotate about y-axis\n",
    "    rotation_matrix = torch.tensor([[torch.cos(angle_radians), 0, torch.sin(angle_radians)],\n",
    "                                    [0, 1, 0],\n",
    "                                    [-torch.sin(angle_radians), 0, torch.cos(angle_radians)]])\n",
    "    rotated_directions = torch.matmul(directions_unn, rotation_matrix.T)\n",
    "    \n",
    "    transposed_directions = rotated_directions.T #transpose is necessary for normalization\n",
    "    directions = (transposed_directions / transposed_directions.norm(dim=0)).T # [pu*pv, 3]\n",
    "\n",
    "    # Option 1: Use SDF\n",
    "    #hit_positions, hit_mask = sphere_trace(sdf, camera_position, directions, max_ray_length)\n",
    "    # Option 2: Use Mesh\n",
    "    intersections, hit_mask, index_tri = mesh_trace(mesh_path, camera_position, directions) \n",
    "    t_mesh = time.time()\n",
    "    \n",
    "    # Option 1: Use SDF\n",
    "    #reflections = phong_model(sdf, hit_positions[hit_mask], camera_position, ambient_coeff, diffuse_coeff, specular_coeff, shininess, mesh_path, amb_light_color, light_intensity_1, light_color_1, light_dir_1, light_intensity_2, light_color_2, light_dir_2, light_intensity_3, light_color_3, light_dir_3, index_tri=None)\n",
    "    # Option 2: Use Mesh\n",
    "    reflections = phong_model(sdf, intersections, camera_position, phong_params, light_params, mesh_path, index_tri) # mesh alternative\n",
    "    t_phong = time.time()\n",
    "\n",
    "    # Assign a color for objects\n",
    "    image[hit_mask] = torch.mul(reflections, phong_params[\"object_color\"].repeat(reflections.shape[0], 1))\n",
    "    image = torch.clamp(image, max=1.0)\n",
    "    image = image.reshape(pu, pv, 3).transpose(0, 1)\n",
    "\n",
    "\n",
    "    return image"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T12:42:54.112979Z",
     "start_time": "2023-11-27T12:42:53.890258Z"
    }
   },
   "id": "fcc5fac445bb6f43"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare Score Calculation Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62dda701d3a74cb2"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# --- Get Data Annotations ---\n",
    "\n",
    "path_to_head_annotations = '/Users/katharinaschmid/Text2Head/rendering_data/head_annotations.json'\n",
    "with open(path_to_head_annotations) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# --- Prepare Indices ---\n",
    "\n",
    "# Identity: Gender\n",
    "male_indices = [index for index, head in enumerate(data['heads']) if head['gender'] == 'man']\n",
    "female_indices = [index for index, head in enumerate(data['heads']) if head['gender'] == 'woman']\n",
    "\n",
    "# Identity: Age\n",
    "young_indices = [index for index, head in enumerate(data['heads']) if head['age'] == 'young']\n",
    "old_indices = [index for index, head in enumerate(data['heads']) if head['age'] == 'old']\n",
    "\n",
    "# Identity: Ethnicity\n",
    "caucasian_indices = [index for index, head in enumerate(data['heads']) if head['ethnicity'] == 'Caucasian'] \n",
    "asian_indices = [index for index, head in enumerate(data['heads']) if head['ethnicity'] == 'Asian'] \n",
    "\n",
    "# Identity: Hairstyle\n",
    "hat_indices = [index for index, head in enumerate(data['heads']) if head['hairstyle'] == 'hat']\n",
    "ponytail_indices = [index for index, head in enumerate(data['heads']) if head['hairstyle'] == 'ponytail']\n",
    "straight_hair_indices = [index for index, head in enumerate(data['heads']) if head['hairstyle'] == 'straight hair']\n",
    "curly_hair_indices = [index for index, head in enumerate(data['heads']) if head['hairstyle'] == 'curly hair']\n",
    "\n",
    "# Identity: Beard\n",
    "beard_indices = [index for index, head in enumerate(data['heads']) if head['beard'] == 'yes']\n",
    "no_beard_indices = [index for index, head in enumerate(data['heads']) if head['beard'] == 'no']\n",
    "\n",
    "# Expression: Facial Expression\n",
    "open_mouth_indices = [index for index, head in enumerate(data['heads']) if head['facial_expression'] == 'open mouth']\n",
    "smiling_indices = [index for index, head in enumerate(data['heads']) if head['facial_expression'] == 'smiling']\n",
    "closed_eyes_indices = [index for index, head in enumerate(data['heads']) if head['facial_expression'] == 'closed eyes']\n",
    "raised_brows_indices = [index for index, head in enumerate(data['heads']) if head['facial_expression'] == 'raised brows']\n",
    "puffed_cheeks_indices = [index for index, head in enumerate(data['heads']) if head['facial_expression'] == 'puffed cheeks']\n",
    "\n",
    "# Expression: Emotion \n",
    "sad_indices = [index for index, head in enumerate(data['heads']) if head['emotion'] == 'sad']\n",
    "not_sad_indices = [index for index, head in enumerate(data['heads']) if head['emotion'] != 'sad']\n",
    "happy_indices = [index for index, head in enumerate(data['heads']) if head['emotion'] == 'happy']\n",
    "not_happy_indices = [index for index, head in enumerate(data['heads']) if head['emotion'] != 'happy']\n",
    "\n",
    "# --- Precompute CLIP Text Embeddings ---\n",
    "captions = []\n",
    "captions.append('A man') # 0\n",
    "captions.append('A woman') # 1\n",
    "captions.append('A young person') # 2\n",
    "captions.append('An old person') # 3\n",
    "captions.append('A European person') # 4\n",
    "captions.append('An Asian person') # 5\n",
    "captions.append('An African person') # 6\n",
    "captions.append('A person wearing a hat') # 7\n",
    "captions.append('A person with a ponytail') # 8\n",
    "captions.append('A person with straight hair') # 9\n",
    "captions.append('A person with curly hair') # 10\n",
    "captions.append('A person with beard') # 11\n",
    "captions.append('A person with open mouth') # 12\n",
    "captions.append('A smiling person') # 13\n",
    "captions.append('A person with closed eyes') # 14\n",
    "captions.append('A frowning person') # 15\n",
    "captions.append('A person with puffed cheeks') # 16\n",
    "captions.append('A sad person') # 17\n",
    "captions.append('A happy person') # 18\n",
    "\n",
    "preprocessed_text = clip.tokenize(captions).to(device) # [num_captions, 77]\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(preprocessed_text) # [num_captions, 512]\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True) # [num_captions, 512]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T15:46:36.926478Z",
     "start_time": "2023-11-27T15:46:34.565306Z"
    }
   },
   "id": "ccd21bc560b0eefe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameter Search for Renderer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff2474134e2af3d1"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# --- CLIP Preprocessing --- # TODO: to be verified: kann man das so machen?\n",
    "clip_tensor_preprocessor = Compose([\n",
    "    Resize(224, interpolation=InterpolationMode.BICUBIC),\n",
    "    CenterCrop(224),\n",
    "    Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "\n",
    "\n",
    "# --- Penalty Function to penalize negative Signal Scores ---\n",
    "def penalty(vector, k):\n",
    "    vector[vector < 0] *= k\n",
    "    return vector\n",
    "\n",
    "def analyze_rendering(hparams):\n",
    "    \n",
    "    # --- Get Hyperparameters ---\n",
    "    camera_params = {\n",
    "        \"camera_distance\": hparams['camera_distance_factor'] * hparams['focal_length'], # ensures appropriate head size\n",
    "        \"camera_angle\": hparams['camera_angle'],\n",
    "        \"focal_length\": hparams['focal_length'],\n",
    "        \"max_ray_length\": (hparams['camera_distance_factor'] + 1) * hparams['focal_length'] + 1.5,\n",
    "        # Image\n",
    "        \"resolution_y\": hparams['resolution'],\n",
    "        \"resolution_x\": hparams['resolution']\n",
    "    }\n",
    "    phong_params = {\n",
    "        \"ambient_coeff\": hparams['ambient_coeff'],\n",
    "        \"diffuse_coeff\": hparams['diffuse_coeff'],\n",
    "        \"specular_coeff\": hparams['specular_coeff'],\n",
    "        \"shininess\": hparams['shininess'],\n",
    "        # Colors\n",
    "        \"object_color\": hparams['object_color'],\n",
    "        \"background_color\": hparams['background_color']\n",
    "    }\n",
    "    \n",
    "    light_params = {\n",
    "        \"amb_light_color\": hparams['amb_light_color'],\n",
    "        # light 1\n",
    "        \"light_intensity_1\": hparams['light_intensity_1'],\n",
    "        \"light_color_1\": hparams['light_color'],\n",
    "        \"light_dir_1\": hparams['light_dir_1'],\n",
    "        # light 2\n",
    "        \"light_intensity_2\": hparams['light_intensity_2'],\n",
    "        \"light_color_2\": hparams['light_color'],\n",
    "        \"light_dir_2\": hparams['light_dir_2'],\n",
    "        # light 3\n",
    "        \"light_intensity_3\": hparams['light_intensity_3'],\n",
    "        \"light_color_3\": hparams['light_color'],\n",
    "        \"light_dir_3\": hparams['light_dir_3'],\n",
    "    }\n",
    "    \n",
    "    # --- Render and Embed Images ---\n",
    "    folder_path = '/Users/katharinaschmid/Text2Head/rendering_data/annotated_dataset/'\n",
    "    file_list = sorted(os.listdir(folder_path))\n",
    "    file_list = [file for file in file_list if not (file.startswith('.DS_Store') or file.endswith('.png'))]\n",
    "    \n",
    "    image_embeddings_list = []\n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        for mesh in file_list:\n",
    "            print(i, mesh)\n",
    "            mesh_path = folder_path + mesh\n",
    "            sdf = None\n",
    "            t_start = time.time()\n",
    "            rendered_image = render(sdf, camera_params, phong_params, light_params, mesh_path)\n",
    "            t_image = time.time()\n",
    "            image_c_first = rendered_image.permute(2, 0, 1)\n",
    "            image_preprocessed = clip_tensor_preprocessor(image_c_first).unsqueeze(0)\n",
    "            t_simon = time.time()\n",
    "            image_features = model.encode_image(image_preprocessed) # [1, 512]\n",
    "            image_embeddings_list.append(image_features)\n",
    "            t_clip = time.time()\n",
    "            i += 1\n",
    "\n",
    "        image_embeddings = torch.cat(image_embeddings_list, dim=0)\n",
    "        image_embeddings /= image_embeddings.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        # --- Get CLIP Similarity and Scores ---\n",
    "        cosine_similarity = torch.matmul(text_features, image_embeddings.T) #[num_captions*tests, num_images]\n",
    "        \n",
    "        # Identity Scores\n",
    "        i_man_score = torch.mean(cosine_similarity[0, male_indices], dim=-1) - torch.mean(cosine_similarity[0, female_indices], dim=-1)\n",
    "        i_woman_score = torch.mean(cosine_similarity[1, female_indices], dim=-1) - torch.mean(cosine_similarity[1, male_indices], dim=-1)\n",
    "        \n",
    "        i_young_score = torch.mean(cosine_similarity[2, young_indices], dim=-1) - torch.mean(cosine_similarity[2, old_indices], dim=-1)\n",
    "        i_old_score = torch.mean(cosine_similarity[3, old_indices], dim=-1) - torch.mean(cosine_similarity[3, young_indices], dim=-1)\n",
    "        i_caucasian_score = torch.mean(cosine_similarity[4, caucasian_indices], dim=-1) - torch.mean(cosine_similarity[4, (asian_indices)], dim=-1)\n",
    "        i_asian_score = torch.mean(cosine_similarity[5, asian_indices], dim=-1) - torch.mean(cosine_similarity[5, (caucasian_indices)], dim=-1)\n",
    "        # 6 skip african\n",
    "        i_hat_score = torch.mean(cosine_similarity[7, hat_indices], dim=-1) - torch.mean(cosine_similarity[7, (ponytail_indices + straight_hair_indices + curly_hair_indices)], dim=-1)\n",
    "        i_ponytail_score = torch.mean(cosine_similarity[8, ponytail_indices], dim=-1) - torch.mean(cosine_similarity[8, (hat_indices + straight_hair_indices + curly_hair_indices)], dim=-1)\n",
    "        i_straight_hair_score = torch.mean(cosine_similarity[9, straight_hair_indices], dim=-1) - torch.mean(cosine_similarity[9, curly_hair_indices], dim=-1) # ONLY curly hair as opposite!\n",
    "        i_curly_hair_score = torch.mean(cosine_similarity[10, curly_hair_indices], dim=-1) - torch.mean(cosine_similarity[10, straight_hair_indices], dim=-1) # ONLY straight hair as opposite!\n",
    "        i_beard_score = torch.mean(cosine_similarity[11, beard_indices], dim=-1) - torch.mean(cosine_similarity[11, no_beard_indices], dim=-1)\n",
    "        \n",
    "        # Expression Scores\n",
    "        e_open_mouth_score = torch.mean(cosine_similarity[12, open_mouth_indices], dim=-1) - torch.mean(cosine_similarity[12, (closed_eyes_indices + raised_brows_indices + puffed_cheeks_indices)], dim=-1) # no smiling as opposite b/c mouth can be open when grinning\n",
    "        e_smiling_score = torch.mean(cosine_similarity[13, smiling_indices], dim=-1) - torch.mean(cosine_similarity[13, (open_mouth_indices + closed_eyes_indices + raised_brows_indices + puffed_cheeks_indices)], dim=-1)\n",
    "        e_closed_eyes_score = torch.mean(cosine_similarity[14, closed_eyes_indices], dim=-1) - torch.mean(cosine_similarity[14, (open_mouth_indices + smiling_indices + raised_brows_indices + puffed_cheeks_indices)], dim=-1)\n",
    "        e_raised_brows_score = torch.mean(cosine_similarity[15, raised_brows_indices], dim=-1) - torch.mean(cosine_similarity[15, (open_mouth_indices + smiling_indices + closed_eyes_indices + puffed_cheeks_indices)], dim=-1)\n",
    "        e_puffed_cheeks_score = torch.mean(cosine_similarity[16, puffed_cheeks_indices], dim=-1) - torch.mean(cosine_similarity[16, (open_mouth_indices + smiling_indices + closed_eyes_indices + raised_brows_indices)], dim=-1)\n",
    "        e_sad_score = torch.mean(cosine_similarity[17, sad_indices], dim=-1) - torch.mean(cosine_similarity[17, not_sad_indices], dim=-1)\n",
    "        e_happy_score = torch.mean(cosine_similarity[18, happy_indices], dim=-1) - torch.mean(cosine_similarity[18, not_happy_indices], dim=-1)\n",
    "        \n",
    "        # Stack Scores, add penalty, compute average\n",
    "        i_scores = torch.stack((i_man_score, i_woman_score, i_young_score, i_old_score, i_caucasian_score, i_asian_score, i_hat_score, i_ponytail_score, i_straight_hair_score, i_curly_hair_score, i_beard_score), dim=0)\n",
    "        e_scores = torch.stack((e_open_mouth_score, e_smiling_score, e_closed_eyes_score, e_raised_brows_score, e_puffed_cheeks_score, e_sad_score, e_happy_score), dim=0)\n",
    "        print('i_scores: ', i_scores, 'e_scores: ', e_scores)\n",
    "        scores = torch.cat((i_scores, e_scores), dim=0)\n",
    "        scores_penalized = penalty(scores, 10)\n",
    "        scores_avg = torch.mean(scores_penalized, dim=0)\n",
    "        print('scores penalized: ', scores_penalized)\n",
    "        print('scores_avg: ', scores_avg)\n",
    "        \n",
    "    \n",
    "    return scores_avg\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters to tune and their search spaces\n",
    "    search_space = {\n",
    "        # Camera\n",
    "        \"camera_distance_factor\": trial.suggest_float('camera_distance_factor', 0.1, 0.8), \n",
    "        \"camera_angle\": trial.suggest_categorical('camera_angle', [0., 15., 30., 45., 55.]), \n",
    "        \"focal_length\": trial.suggest_float('focal_length', 2.0, 9.0),\n",
    "        # Image\n",
    "        \"resolution\": trial.suggest_categorical('resolution', [150, 200, 224, 300]), \n",
    "        # Phong\n",
    "        \"ambient_coeff\": trial.suggest_float('ambient_coeff', 0.0, 1.0), \n",
    "        \"diffuse_coeff\": trial.suggest_float('diffuse_coeff', 0.0, 1.0), \n",
    "        \"specular_coeff\": trial.suggest_float('specular_coeff', 0.0, 1.0), \n",
    "        \"shininess\": trial.suggest_categorical('shininess', [0.1, 0.5, 1.0, 3.0, 10.]),\n",
    "        # Colors\n",
    "        \"object_color\": torch.tensor([\n",
    "            trial.suggest_float('object_color_0', 0.0, 1.0),\n",
    "            trial.suggest_float('object_color_1', 0.0, 1.0),\n",
    "            trial.suggest_float('object_color_2', 0.0, 1.0)\n",
    "        ]),\n",
    "        \"background_color\": torch.tensor([\n",
    "            trial.suggest_float('background_color_0', 0.0, 1.0),\n",
    "            trial.suggest_float('background_color_1', 0.0, 1.0),\n",
    "            trial.suggest_float('background_color_2', 0.0, 1.0)\n",
    "        ]),\n",
    "        # Light\n",
    "        \"amb_light_color\": torch.tensor([\n",
    "            trial.suggest_float('amb_light_color_0', 0.0, 1.0),\n",
    "            trial.suggest_float('amb_light_color_1', 0.0, 1.0),\n",
    "            trial.suggest_float('amg_light_color_2', 0.0, 1.0)\n",
    "        ]),\n",
    "        # light 1\n",
    "        \"light_intensity_1\": trial.suggest_float('light_intensity_1', 0.5, 1.5), \n",
    "        \"light_color\": torch.tensor([\n",
    "            trial.suggest_float('light_color_0', 0.7, 1.0),\n",
    "            trial.suggest_float('light_color_1', 0.7, 1.0),\n",
    "            trial.suggest_float('light_color_2', 0.7, 1.0)\n",
    "        ]),\n",
    "        \"light_dir_1\": torch.tensor([\n",
    "            trial.suggest_float('light_dir_1_0', -1.0, 0.0),\n",
    "            trial.suggest_float('light_dir_1_1', -1.0, 0.0),\n",
    "            trial.suggest_float('light_dir_1_2', -1.0, 0.0)\n",
    "        ]),\n",
    "        # light 2\n",
    "        \"light_intensity_2\": trial.suggest_float('light_intensity_2', 0.0, 1.), \n",
    "        \"light_dir_2\": torch.tensor([\n",
    "            0.0,\n",
    "            trial.suggest_float('light_dir_2_1', -1.0, 0.0),\n",
    "            trial.suggest_float('light_dir_2_2', -1.0, 0.0)\n",
    "        ]),\n",
    "        # light 3\n",
    "        \"light_intensity_3\": trial.suggest_float('light_intensity_3', 0.0, 0.7), \n",
    "        \"light_dir_3\": torch.tensor([\n",
    "            trial.suggest_float('light_dir_2_0', -1.0, 0.5),\n",
    "            trial.suggest_float('light_dir_2_1', -1.0, 0.0),\n",
    "            0.0\n",
    "        ]),\n",
    "    }\n",
    "    \n",
    "    hparams.update(search_space)\n",
    "    \n",
    "    return analyze_rendering(hparams)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T15:46:40.645454Z",
     "start_time": "2023-11-27T15:46:40.569118Z"
    }
   },
   "id": "18fe2d0b47dc321"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-27 16:46:55,103] A new study created in RDB with name: test_17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 003_000.ply\n",
      "1 004_000.ply\n",
      "2 005_000.ply\n",
      "3 006_000.ply\n",
      "4 007_000.ply\n",
      "5 008_000.ply\n",
      "6 009_000.ply\n",
      "7 010_000.ply\n",
      "8 011_000.ply\n",
      "9 012_000.ply\n",
      "10 013_000.ply\n",
      "11 014_000.ply\n",
      "12 015_000.ply\n",
      "13 016_000.ply\n",
      "14 017_000.ply\n",
      "15 017_018.ply\n",
      "16 018_000.ply\n",
      "17 018_017.ply\n",
      "18 019_000.ply\n",
      "19 020_000.ply\n",
      "20 021_000.ply\n",
      "21 022_000.ply\n",
      "22 023_000.ply\n",
      "23 024_000.ply\n",
      "24 025_000.ply\n",
      "25 055_017.ply\n",
      "26 056_014.ply\n",
      "27 057_006.ply\n",
      "28 058_016.ply\n",
      "29 059_017.ply\n",
      "30 060_018.ply\n",
      "31 061_006.ply\n",
      "32 062_017.ply\n",
      "33 064_016.ply\n",
      "34 065_018.ply\n",
      "35 067_017.ply\n",
      "36 068_015.ply\n",
      "37 069_015.ply\n",
      "38 070_014.ply\n",
      "39 071_006.ply\n",
      "40 072_017.ply\n",
      "41 073_014.ply\n",
      "42 074_015.ply\n",
      "43 075_017.ply\n",
      "44 076_014.ply\n",
      "45 077_017.ply\n",
      "46 078_006.ply\n",
      "47 079_018.ply\n",
      "48 080_017.ply\n",
      "49 081_016.ply\n",
      "50 082_017.ply\n",
      "51 083_017.ply\n",
      "52 085_006.ply\n",
      "53 086_014.ply\n",
      "54 087_017.ply\n",
      "55 088_014.ply\n",
      "56 089_006.ply\n",
      "57 090_015.ply\n",
      "58 091_015.ply\n",
      "59 092_014.ply\n",
      "60 096_017.ply\n",
      "61 097_015.ply\n",
      "62 098_006.ply\n",
      "63 099_014.ply\n",
      "64 100_006.ply\n",
      "65 102_014.ply\n",
      "66 103_006.ply\n",
      "67 104_014.ply\n",
      "68 105_015.ply\n",
      "69 106_017.ply\n",
      "70 108_014.ply\n",
      "71 109_014.ply\n",
      "72 110_014.ply\n",
      "73 111_017.ply\n",
      "74 112_018.ply\n",
      "75 113_006.ply\n",
      "76 114_016.ply\n",
      "77 115_016.ply\n",
      "78 117_014.ply\n",
      "79 118_017.ply\n",
      "80 120_014.ply\n",
      "81 121_016.ply\n",
      "82 122_015.ply\n",
      "83 123_018.ply\n",
      "84 124_018.ply\n",
      "85 125_015.ply\n",
      "86 126_015.ply\n",
      "87 127_016.ply\n",
      "88 128_014.ply\n",
      "89 129_017.ply\n",
      "90 130_014.ply\n",
      "91 131_006.ply\n",
      "92 132_016.ply\n",
      "93 133_017.ply\n",
      "94 134_012.ply\n",
      "95 135_006.ply\n",
      "96 136_006.ply\n",
      "97 137_019.ply\n",
      "98 138_017.ply\n",
      "99 140_019.ply\n",
      "100 141_006.ply\n",
      "101 142_014.ply\n",
      "102 143_015.ply\n",
      "103 144_015.ply\n",
      "104 145_015.ply\n",
      "105 146_018.ply\n",
      "106 147_025.ply\n",
      "107 148_015.ply\n",
      "108 149_014.ply\n",
      "109 150_006.ply\n",
      "110 151_014.ply\n",
      "111 162_004.ply\n",
      "112 163_006.ply\n",
      "113 164_016.ply\n",
      "114 165_007.ply\n",
      "115 167_004.ply\n",
      "116 174_014.ply\n",
      "117 179_017.ply\n",
      "118 180_014.ply\n",
      "119 181_017.ply\n",
      "120 182_014.ply\n",
      "121 183_015.ply\n",
      "122 184_017.ply\n",
      "123 185_016.ply\n",
      "124 186_006.ply\n",
      "125 187_014.ply\n",
      "126 188_018.ply\n",
      "127 189_015.ply\n",
      "128 190_017.ply\n",
      "129 191_016.ply\n",
      "130 193_015.ply\n",
      "131 194_020.ply\n",
      "132 195_014.ply\n",
      "133 196_014.ply\n",
      "134 198_017.ply\n",
      "135 199_014.ply\n",
      "136 200_006.ply\n",
      "137 201_006.ply\n",
      "138 202_015.ply\n",
      "139 204_016.ply\n",
      "140 206_016.ply\n",
      "141 207_014.ply\n",
      "142 209_016.ply\n",
      "143 210_014.ply\n",
      "144 211_006.ply\n",
      "145 212_016.ply\n",
      "146 213_016.ply\n",
      "147 214_014.ply\n",
      "148 215_014.ply\n",
      "149 216_018.ply\n",
      "150 217_018.ply\n",
      "151 218_006.ply\n",
      "152 220_014.ply\n",
      "153 221_014.ply\n",
      "154 223_017.ply\n",
      "155 224_017.ply\n",
      "156 226_018.ply\n",
      "157 227_006.ply\n",
      "158 228_014.ply\n",
      "159 229_015.ply\n",
      "160 231_015.ply\n",
      "161 232_015.ply\n",
      "162 233_015.ply\n",
      "163 234_018.ply\n",
      "164 235_018.ply\n",
      "165 237_018.ply\n",
      "166 238_014.ply\n",
      "167 239_018.ply\n",
      "168 240_018.ply\n",
      "169 241_015.ply\n",
      "170 242_018.ply\n",
      "171 243_016.ply\n",
      "172 244_017.ply\n",
      "173 245_017.ply\n",
      "174 246_018.ply\n",
      "175 247_007.ply\n",
      "176 248_017.ply\n",
      "177 249_014.ply\n",
      "178 250_014.ply\n",
      "179 251_014.ply\n",
      "180 252_006.ply\n",
      "181 254_006.ply\n",
      "i_scores:  tensor([ 0.0299,  0.0265,  0.0086,  0.0088, -0.0027,  0.0406,  0.0398,  0.0207,\n",
      "         0.0044,  0.0170,  0.0192]) e_scores:  tensor([ 0.0363,  0.0395,  0.0072, -0.0045,  0.0145,  0.0051,  0.0339])\n",
      "scores penalized:  tensor([ 0.0299,  0.0265,  0.0086,  0.0088, -0.0268,  0.0406,  0.0398,  0.0207,\n",
      "         0.0044,  0.0170,  0.0192,  0.0363,  0.0395,  0.0072, -0.0450,  0.0145,\n",
      "         0.0051,  0.0339])\n",
      "scores_avg:  tensor(0.0156)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-27 17:07:15,520] Trial 0 finished with value: 0.015562496148049831 and parameters: {'camera_distance_factor': 0.2980528913511912, 'camera_angle': 55.0, 'focal_length': 6.809747373092, 'resolution': 300, 'ambient_coeff': 0.1311455834535462, 'diffuse_coeff': 0.862161949931783, 'specular_coeff': 0.5956114920052035, 'shininess': 3.0, 'object_color_0': 0.7959385319885227, 'object_color_1': 0.308060958405587, 'object_color_2': 0.5401671219949975, 'background_color_0': 0.6900229873921443, 'background_color_1': 0.6303689268348766, 'background_color_2': 0.056969276310662065, 'amb_light_color_0': 0.8889335998354496, 'amb_light_color_1': 0.943713394325207, 'amg_light_color_2': 0.18796328702196496, 'light_intensity_1': 0.5414075825837699, 'light_color_0': 0.7298494606716501, 'light_color_1': 0.8049414784732534, 'light_color_2': 0.8375662249102482, 'light_dir_1_0': -0.31112004206972677, 'light_dir_1_1': -0.08556084913508344, 'light_dir_1_2': -0.15517782986851714, 'light_intensity_2': 0.08650403973293141, 'light_dir_2_1': -0.34696477876365706, 'light_dir_2_2': -0.45481826923846436, 'light_intensity_3': 0.4579281361746623, 'light_dir_2_0': 0.04843519925168516}. Best is trial 0 with value: 0.015562496148049831.\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'camera_distance_factor': 0.2980528913511912,\n 'camera_angle': 55.0,\n 'focal_length': 6.809747373092,\n 'resolution': 300,\n 'ambient_coeff': 0.1311455834535462,\n 'diffuse_coeff': 0.862161949931783,\n 'specular_coeff': 0.5956114920052035,\n 'shininess': 3.0,\n 'object_color_0': 0.7959385319885227,\n 'object_color_1': 0.308060958405587,\n 'object_color_2': 0.5401671219949975,\n 'background_color_0': 0.6900229873921443,\n 'background_color_1': 0.6303689268348766,\n 'background_color_2': 0.056969276310662065,\n 'amb_light_color_0': 0.8889335998354496,\n 'amb_light_color_1': 0.943713394325207,\n 'amg_light_color_2': 0.18796328702196496,\n 'light_intensity_1': 0.5414075825837699,\n 'light_color_0': 0.7298494606716501,\n 'light_color_1': 0.8049414784732534,\n 'light_color_2': 0.8375662249102482,\n 'light_dir_1_0': -0.31112004206972677,\n 'light_dir_1_1': -0.08556084913508344,\n 'light_dir_1_2': -0.15517782986851714,\n 'light_intensity_2': 0.08650403973293141,\n 'light_dir_2_1': -0.34696477876365706,\n 'light_dir_2_2': -0.45481826923846436,\n 'light_intensity_3': 0.4579281361746623,\n 'light_dir_2_0': 0.04843519925168516}"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study = optuna.create_study(storage=\"sqlite:///optuna_study.db\", study_name=\"test_17\", direction='maximize')\n",
    "study.optimize(objective, n_trials=1)\n",
    "\n",
    "study.best_params"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-27T16:07:16.115778Z",
     "start_time": "2023-11-27T15:46:54.889136Z"
    }
   },
   "id": "8660eafbf534fc18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e83ec109884803bd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
