{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-24T08:41:46.565478Z",
     "start_time": "2023-11-24T08:41:26.658629Z"
    }
   },
   "outputs": [],
   "source": [
    "import clip\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import os.path as osp\n",
    "import torch\n",
    "import pyvista as pv\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import trimesh\n",
    "import time\n",
    "import optuna\n",
    "from torchvision.transforms import Compose, Normalize, Resize, CenterCrop, InterpolationMode\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Rendering Setup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98e57945907ff5f2"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "# --- Hyperparameter Setup ---\n",
    "\n",
    "hparams = {\n",
    "    # Camera\n",
    "    \"camera_distance_factor\": 3.11/6.,\n",
    "    \"camera_angle\": 45.,\n",
    "    \"focal_length\": 6.,\n",
    "    # Image\n",
    "    \"resolution\": 224,\n",
    "    # Phong\n",
    "    \"ambient_coeff\": 0.5,\n",
    "    \"diffuse_coeff\": 0.7,\n",
    "    \"specular_coeff\": 0.0,\n",
    "    \"shininess\": 3.0,\n",
    "    # Colors\n",
    "    \"object_color\": torch.tensor([0.61, 0.61, 0.61]),\n",
    "    \"background_color\": torch.tensor([0.46, 0, 0]),\n",
    "    # Light\n",
    "    \"amb_light_color\": torch.tensor([0.15, 0, 0]),\n",
    "    # light 1\n",
    "    \"light_intensity_1\": 1., \n",
    "    \"light_color\": torch.tensor([1.0, 1.0, 1.0]), \n",
    "    \"light_dir_1\": torch.tensor([-1., -1., -1.]), \n",
    "    # light 2\n",
    "    \"light_intensity_2\": 1.,\n",
    "    \"light_dir_2\": torch.tensor([0., 0., -1.]), \n",
    "    # light 3\n",
    "    \"light_intensity_3\": 0., \n",
    "    \"light_dir_3\": torch.tensor([0., -1., 0.])\n",
    "}\n",
    "\n",
    "# --- Rendering Function ---\n",
    "\n",
    "def phong_model(sdf, points, camera_position, phong_params, light_params, mesh_path, index_tri=None):\n",
    "    \n",
    "    # Option 1: Use SDF\n",
    "    #normals = estimate_normals(sdf, points)\n",
    "    # Option 2: Use Mesh\n",
    "    normals = mesh_normals(mesh_path, index_tri)\n",
    "    view_dirs = points - camera_position \n",
    "    light_dir_1 = light_params[\"light_dir_1\"].repeat(points.shape[0], 1) \n",
    "    light_dir_2 = light_params[\"light_dir_2\"].repeat(points.shape[0], 1)\n",
    "    light_dir_3 = light_params[\"light_dir_3\"].repeat(points.shape[0], 1)\n",
    "    \n",
    "    # Normalize all vectors\n",
    "    normals = (normals.T / torch.norm(normals, dim=-1)).T\n",
    "    light_dir_norm_1 = (light_dir_1.T / torch.norm(light_dir_1, dim=-1)).T\n",
    "    light_dir_norm_2 = (light_dir_2.T / torch.norm(light_dir_2, dim=-1)).T\n",
    "    light_dir_norm_3 = (light_dir_3.T / torch.norm(light_dir_3, dim=-1)).T\n",
    "    view_dir_norm = (view_dirs.T / torch.norm(view_dirs, dim=-1)).T\n",
    "    \n",
    "    # Ambient\n",
    "    ambient = phong_params[\"ambient_coeff\"] * light_params[\"amb_light_color\"] \n",
    "    ambient_refl = ambient.repeat(points.shape[0], 1)\n",
    "    \n",
    "    # Area light 1\n",
    "    diffuse_1 = phong_params[\"diffuse_coeff\"] * torch.clamp(torch.sum(-light_dir_norm_1 * normals, dim=-1), min=0.0) * light_params[\"light_intensity_1\"] # [N]\n",
    "    diffuse_refl_1 = torch.matmul(diffuse_1.unsqueeze(1), light_params[\"light_color_1\"].unsqueeze(0)) # [N, 3]\n",
    "    reflect_dir_1 = light_dir_norm_1 + (2 * normals.T * torch.clamp(torch.sum(-light_dir_norm_1 * normals, dim=-1), min=0.0)).T\n",
    "    specular_1 = phong_params[\"specular_coeff\"] * torch.pow(torch.clamp(torch.sum(reflect_dir_1 * -view_dir_norm, dim=-1), min=0.0), phong_params[\"shininess\"]) * light_params[\"light_intensity_1\"] # [N]\n",
    "    specular_refl_1 = torch.matmul(specular_1.unsqueeze(1), light_params[\"light_color_1\"].unsqueeze(0)) # [N, 3]\n",
    "    \n",
    "    # Area light 2\n",
    "    diffuse_2 = phong_params[\"diffuse_coeff\"] * torch.clamp(torch.sum(-light_dir_norm_2 * normals, dim=-1), min=0.0) * light_params[\"light_intensity_2\"] # [N]\n",
    "    diffuse_refl_2 = torch.matmul(diffuse_2.unsqueeze(1), light_params[\"light_color_2\"].unsqueeze(0)) # [N, 3]\n",
    "    reflect_dir_2 = light_dir_norm_2 + (2 * normals.T * torch.clamp(torch.sum(-light_dir_norm_2 * normals, dim=-1), min=0.0)).T\n",
    "    specular_2 = phong_params[\"specular_coeff\"] * torch.pow(torch.clamp(torch.sum(reflect_dir_2 * -view_dir_norm, dim=-1), min=0.0), phong_params[\"shininess\"]) * light_params[\"light_intensity_2\"] # [N]\n",
    "    specular_refl_2 = torch.matmul(specular_2.unsqueeze(1), light_params[\"light_color_2\"].unsqueeze(0)) # [N, 3]\n",
    "    \n",
    "    # Area light 3\n",
    "    diffuse_3 = phong_params[\"diffuse_coeff\"] * torch.clamp(torch.sum(-light_dir_norm_3 * normals, dim=-1), min=0.0) * light_params[\"light_intensity_3\"] # [N]\n",
    "    diffuse_refl_3 = torch.matmul(diffuse_3.unsqueeze(1), light_params[\"light_color_3\"].unsqueeze(0)) # [N, 3]\n",
    "    reflect_dir_3 = light_dir_norm_3 + (2 * normals.T * torch.clamp(torch.sum(-light_dir_norm_3 * normals, dim=-1), min=0.0)).T\n",
    "    specular_3 = phong_params[\"specular_coeff\"] * torch.pow(torch.clamp(torch.sum(reflect_dir_3 * -view_dir_norm, dim=-1), min=0.0), phong_params[\"shininess\"]) * light_params[\"light_intensity_3\"] # [N]\n",
    "    specular_refl_3 = torch.matmul(specular_3.unsqueeze(1), light_params[\"light_color_3\"].unsqueeze(0)) # [N, 3]\n",
    "    \n",
    "    # point light\n",
    "    #diffuse_1 = diffuse_coeff * torch.clamp(torch.sum(-light_dir_norm_1 * normals, dim=-1), min=0.0) * light_intensity_1 # [N]\n",
    "    #diffuse_refl_1 = torch.matmul(diffuse_1.unsqueeze(1), light_color.unsqueeze(0)) # [N, 3]\n",
    "    #reflect_dir_1 = light_dir_norm_1 + (2 * normals.T * torch.clamp(torch.sum(-light_dir_norm_1 * normals, dim=-1), min=0.0)).T\n",
    "    #specular_1 = specular_coeff * torch.pow(torch.clamp(torch.sum(reflect_dir_1 * -view_dir_norm, dim=-1), min=0.0), shininess) * light_intensity_1 # [N]\n",
    "    #specular_refl_1 = torch.matmul(specular_1.unsqueeze(1), light_color.unsqueeze(0)) # [N, 3]\n",
    "\n",
    "    return ambient_refl + diffuse_refl_1 + specular_refl_1 + diffuse_refl_2 + specular_refl_2 + diffuse_refl_3 + specular_refl_3\n",
    "\n",
    "def estimate_normals(sdf, points, epsilon=1e-3):\n",
    "    sdf_inputs = torch.concat([points,\n",
    "                              points + torch.tensor([epsilon, 0, 0]),\n",
    "                              points + torch.tensor([0, epsilon, 0]),\n",
    "                              points + torch.tensor([0, 0, epsilon])])\n",
    "\n",
    "    sdf_values = sdf(sdf_inputs).reshape(4, -1)\n",
    "\n",
    "    # Calculate the gradient using finite differences\n",
    "    gradient = sdf_values[1:] - sdf_values[0]\n",
    "\n",
    "    # Normalize the gradient to obtain the estimated normal\n",
    "    normal = gradient / torch.norm(gradient, p=2, dim=0)\n",
    "\n",
    "    return normal.T\n",
    "\n",
    "def sphere_trace(sdf, camera_position, norm_directions, max_length):\n",
    "    N = norm_directions.shape[0]\n",
    "    positions = camera_position.unsqueeze(dim=0).repeat(N, 1) # [N, 3]\n",
    "    total_distances = torch.zeros(N)\n",
    "    last_distances = torch.ones(N)\n",
    "\n",
    "    for _ in range(20):\n",
    "        #mask = torch.logical_and(total_distances < max_length, last_distances > 1e-3)\n",
    "        not_reached_max_distance = total_distances < max_length\n",
    "        not_hit_target = torch.abs(last_distances) > 1e-3\n",
    "        mask = torch.logical_and(not_reached_max_distance, not_hit_target)\n",
    "        if torch.all(torch.logical_not(mask)):\n",
    "            break\n",
    "        distances = sdf(positions[mask])\n",
    "        steps = (norm_directions[mask].T * distances).T\n",
    "        positions[mask] += steps\n",
    "        total_distances[mask] += distances\n",
    "        last_distances[mask] = distances\n",
    "\n",
    "    #positions[total_distances > max_length] *= torch.nan\n",
    "    return positions, total_distances < max_length\n",
    "\n",
    "def mesh_trace(mesh_path, ray_starts, ray_directions):\n",
    "    mesh = trimesh.load_mesh(mesh_path)\n",
    "    \n",
    "    ray_origins = ray_starts.repeat(ray_directions.shape[0], 1)\n",
    "    \n",
    "    intersections, index_ray, index_tri = mesh.ray.intersects_location(ray_origins, ray_directions, multiple_hits=False) \n",
    "    \n",
    "    mask = torch.zeros(ray_directions.shape[0], dtype=torch.bool)\n",
    "    mask[index_ray] = True\n",
    "    \n",
    "    points = torch.as_tensor(intersections, dtype=torch.float32)\n",
    "\n",
    "    return points, mask, index_tri\n",
    "\n",
    "\n",
    "def mesh_normals(mesh_path, index_tri):\n",
    "    mesh = trimesh.load_mesh(mesh_path)\n",
    "    all_normals = mesh.face_normals\n",
    "    normals = all_normals[index_tri]\n",
    "    normals_torch = torch.from_numpy(normals).to(dtype=torch.float32)\n",
    "    \n",
    "    return normals_torch\n",
    "\n",
    "def render(sdf, camera_params, phong_params, light_params, mesh_path=None):\n",
    "    t_s = time.time()\n",
    "    pu = camera_params[\"resolution_x\"]\n",
    "    pv = camera_params[\"resolution_y\"]\n",
    "    image = phong_params[\"background_color\"].repeat(pu * pv, 1)\n",
    "    \n",
    "    angle_radians = torch.deg2rad_(torch.tensor(camera_params[\"camera_angle\"]))\n",
    "    camera = torch.tensor([torch.sin(angle_radians), 0, torch.cos(angle_radians)])\n",
    "    camera_position = camera * (camera_params[\"camera_distance\"] + camera_params[\"focal_length\"]) / camera.norm()\n",
    "    \n",
    "    # Normalize the xy value of the current pixel [-0.5, 0.5]\n",
    "    u_norms = ((torch.arange(pu) + 0.5) / pu - 0.5) * pu/pv\n",
    "    v_norms = 0.5 - (torch.arange(pv) + 0.5) / pv\n",
    "\n",
    "    # Calculate the ray directions for all pixels\n",
    "    directions_unn = torch.cat(torch.meshgrid(u_norms, v_norms, torch.tensor(-camera_params[\"focal_length\"])), dim=-1) \n",
    "    directions_unn = directions_unn.reshape((pu*pv, 3)) # [pu, pv, 3] --> [pu*pv, 3] (u1, v1, f)(u1, v2, f)...(u2, v1, f)...\n",
    "\n",
    "    # rotate about y-axis\n",
    "    rotation_matrix = torch.tensor([[torch.cos(angle_radians), 0, torch.sin(angle_radians)],\n",
    "                                    [0, 1, 0],\n",
    "                                    [-torch.sin(angle_radians), 0, torch.cos(angle_radians)]])\n",
    "    rotated_directions = torch.matmul(directions_unn, rotation_matrix.T)\n",
    "    \n",
    "    transposed_directions = rotated_directions.T #transpose is necessary for normalization\n",
    "    directions = (transposed_directions / transposed_directions.norm(dim=0)).T # [pu*pv, 3]\n",
    "\n",
    "    # Option 1: Use SDF\n",
    "    #hit_positions, hit_mask = sphere_trace(sdf, camera_position, directions, max_ray_length)\n",
    "    # Option 2: Use Mesh\n",
    "    intersections, hit_mask, index_tri = mesh_trace(mesh_path, camera_position, directions) \n",
    "    t_mesh = time.time()\n",
    "    \n",
    "    # Option 1: Use SDF\n",
    "    #reflections = phong_model(sdf, hit_positions[hit_mask], camera_position, ambient_coeff, diffuse_coeff, specular_coeff, shininess, mesh_path, amb_light_color, light_intensity_1, light_color_1, light_dir_1, light_intensity_2, light_color_2, light_dir_2, light_intensity_3, light_color_3, light_dir_3, index_tri=None)\n",
    "    # Option 2: Use Mesh\n",
    "    reflections = phong_model(sdf, intersections, camera_position, phong_params, light_params, mesh_path, index_tri) # mesh alternative\n",
    "    t_phong = time.time()\n",
    "\n",
    "    # Assign a color for objects\n",
    "    image[hit_mask] = torch.mul(reflections, phong_params[\"object_color\"].repeat(reflections.shape[0], 1))\n",
    "    image = torch.clamp(image, max=1.0)\n",
    "    image = image.reshape(pu, pv, 3).transpose(0, 1)\n",
    "    \n",
    "    print('mesh: ', t_mesh-t_s)\n",
    "    print('phong: ', t_phong-t_mesh)\n",
    "\n",
    "    return image"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T09:58:34.748409Z",
     "start_time": "2023-11-24T09:58:34.528587Z"
    }
   },
   "id": "fcc5fac445bb6f43"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare Score Calculation Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62dda701d3a74cb2"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "# --- Get Data Annotations ---\n",
    "\n",
    "path_to_head_annotations = '/Users/katharinaschmid/Text2Head/rendering_data/head_annotations.json'\n",
    "with open(path_to_head_annotations) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# --- Prepare Indices ---\n",
    "\n",
    "# Identity: Gender\n",
    "male_indices = [index for index, head in enumerate(data['heads']) if head['gender'] == 'man']\n",
    "female_indices = [index for index, head in enumerate(data['heads']) if head['gender'] == 'woman']\n",
    "\n",
    "# Identity: Age\n",
    "young_indices = [index for index, head in enumerate(data['heads']) if head['age'] == 'young']\n",
    "old_indices = [index for index, head in enumerate(data['heads']) if head['age'] == 'old']\n",
    "\n",
    "# Identity: Ethnicity\n",
    "caucasian_indices = [index for index, head in enumerate(data['heads']) if head['ethnicity'] == 'Caucasian'] \n",
    "asian_indices = [index for index, head in enumerate(data['heads']) if head['ethnicity'] == 'Asian'] \n",
    "\n",
    "# Identity: Hairstyle\n",
    "hat_indices = [index for index, head in enumerate(data['heads']) if head['hairstyle'] == 'hat']\n",
    "ponytail_indices = [index for index, head in enumerate(data['heads']) if head['hairstyle'] == 'ponytail']\n",
    "straight_hair_indices = [index for index, head in enumerate(data['heads']) if head['hairstyle'] == 'straight hair']\n",
    "curly_hair_indices = [index for index, head in enumerate(data['heads']) if head['hairstyle'] == 'curly hair']\n",
    "\n",
    "# Identity: Beard\n",
    "beard_indices = [index for index, head in enumerate(data['heads']) if head['beard'] == 'yes']\n",
    "no_beard_indices = [index for index, head in enumerate(data['heads']) if head['beard'] == 'no']\n",
    "\n",
    "# Expression: Facial Expression\n",
    "open_mouth_indices = [index for index, head in enumerate(data['heads']) if head['facial_expression'] == 'open mouth']\n",
    "smiling_indices = [index for index, head in enumerate(data['heads']) if head['facial_expression'] == 'smiling']\n",
    "closed_eyes_indices = [index for index, head in enumerate(data['heads']) if head['facial_expression'] == 'closed eyes']\n",
    "raised_brows_indices = [index for index, head in enumerate(data['heads']) if head['facial_expression'] == 'raised brows']\n",
    "puffed_cheeks_indices = [index for index, head in enumerate(data['heads']) if head['facial_expression'] == 'puffed_cheeks']\n",
    "\n",
    "# Expression: Emotion \n",
    "sad_indices = [index for index, head in enumerate(data['heads']) if head['emotion'] == 'sad']\n",
    "not_sad_indices = [index for index, head in enumerate(data['heads']) if head['emotion'] != 'sad']\n",
    "happy_indices = [index for index, head in enumerate(data['heads']) if head['emotion'] == 'happy']\n",
    "not_happy_indices = [index for index, head in enumerate(data['heads']) if head['emotion'] != 'happy']\n",
    "\n",
    "# --- Precompute CLIP Text Embeddings ---\n",
    "captions = []\n",
    "captions.append('A man') # 0\n",
    "captions.append('A woman') # 1\n",
    "captions.append('A young person') # 2\n",
    "captions.append('An old person') # 3\n",
    "captions.append('A Caucasian person') # 4\n",
    "captions.append('An Asian person') # 5\n",
    "captions.append('An African person') # 6\n",
    "captions.append('A person wearing a hat') # 7\n",
    "captions.append('A person with a ponytail') # 8\n",
    "captions.append('A person with straight hair') # 9\n",
    "captions.append('A person with curly hair') # 10\n",
    "captions.append('A person with beard') # 11\n",
    "captions.append('A person with open mouth') # 12\n",
    "captions.append('A smiling person') # 13\n",
    "captions.append('A person with closed eyes') # 14\n",
    "captions.append('A person with raised brows') # 15\n",
    "captions.append('A person with puffed cheeks') # 16\n",
    "captions.append('A sad person') # 17\n",
    "captions.append('A happy person') # 18\n",
    "\n",
    "preprocessed_text = clip.tokenize(captions).to(device) # [num_captions, 77]\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(preprocessed_text) # [num_captions, 512]\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True) # [num_captions, 512]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T09:58:43.127142Z",
     "start_time": "2023-11-24T09:58:36.922935Z"
    }
   },
   "id": "ccd21bc560b0eefe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameter Search for Renderer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff2474134e2af3d1"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "# --- CLIP Preprocessing --- # TODO: to be verified: kann man das so machen?\n",
    "clip_tensor_preprocessor = Compose([\n",
    "    Resize(224, interpolation=InterpolationMode.BICUBIC),\n",
    "    CenterCrop(224),\n",
    "    Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "\n",
    "\n",
    "# --- Penalty Function to penalize negative Signal Scores ---\n",
    "def penalty(vector, k):\n",
    "    vector[vector < 0] *= k\n",
    "    return vector\n",
    "\n",
    "def analyze_rendering(hparams):\n",
    "    \n",
    "    # --- Get Hyperparameters ---\n",
    "    camera_params = {\n",
    "        \"camera_distance\": hparams['camera_distance_factor'] * hparams['focal_length'], # ensures appropriate head size\n",
    "        \"camera_angle\": hparams['camera_angle'],\n",
    "        \"focal_length\": hparams['focal_length'],\n",
    "        \"max_ray_length\": (hparams['camera_distance_factor'] + 1) * hparams['focal_length'] + 1.5,\n",
    "        # Image\n",
    "        \"resolution_y\": hparams['resolution'],\n",
    "        \"resolution_x\": hparams['resolution']\n",
    "    }\n",
    "    phong_params = {\n",
    "        \"ambient_coeff\": hparams['ambient_coeff'],\n",
    "        \"diffuse_coeff\": hparams['diffuse_coeff'],\n",
    "        \"specular_coeff\": hparams['specular_coeff'],\n",
    "        \"shininess\": hparams['shininess'],\n",
    "        # Colors\n",
    "        \"object_color\": hparams['object_color'],\n",
    "        \"background_color\": hparams['background_color']\n",
    "    }\n",
    "    \n",
    "    light_params = {\n",
    "        \"amb_light_color\": hparams['amb_light_color'],\n",
    "        # light 1\n",
    "        \"light_intensity_1\": hparams['light_intensity_1'],\n",
    "        \"light_color_1\": hparams['light_color'],\n",
    "        \"light_dir_1\": hparams['light_dir_1'],\n",
    "        # light 2\n",
    "        \"light_intensity_2\": hparams['light_intensity_2'],\n",
    "        \"light_color_2\": hparams['light_color'],\n",
    "        \"light_dir_2\": hparams['light_dir_2'],\n",
    "        # light 3\n",
    "        \"light_intensity_3\": hparams['light_intensity_3'],\n",
    "        \"light_color_3\": hparams['light_color'],\n",
    "        \"light_dir_3\": hparams['light_dir_3'],\n",
    "    }\n",
    "    \n",
    "    # --- Render and Embed Images ---\n",
    "    folder_path = '/Users/katharinaschmid/Text2Head/rendering_data/annotated_dataset/'\n",
    "    file_list = sorted(os.listdir(folder_path))\n",
    "    file_list = [file for file in file_list if not file.startswith('.DS_Store')]\n",
    "    \n",
    "    image_embeddings_list = []\n",
    "    with torch.no_grad():\n",
    "        for mesh in file_list:\n",
    "            mesh_path = folder_path + mesh\n",
    "            sdf = None\n",
    "            t_start = time.time()\n",
    "            rendered_image = render(sdf, camera_params, phong_params, light_params, mesh_path)\n",
    "            t_image = time.time()\n",
    "            image_c_first = rendered_image.permute(2, 0, 1)\n",
    "            image_preprocessed = clip_tensor_preprocessor(image_c_first).unsqueeze(0)\n",
    "            t_simon = time.time()\n",
    "            image_features = model.encode_image(image_preprocessed) # [1, 512]\n",
    "            image_embeddings_list.append(image_features)\n",
    "            t_clip = time.time()\n",
    "            print('render: ', t_image-t_start)\n",
    "            print('simon: ', t_simon-t_image)\n",
    "            print('clip: ', t_clip-t_simon)\n",
    "        image_embeddings = torch.cat(image_embeddings_list, dim=0)\n",
    "        image_embeddings /= image_embeddings.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        # --- Get CLIP Similarity and Scores ---\n",
    "        cosine_similarity = torch.matmul(text_features, image_embeddings.T) #[num_captions*tests, num_images]\n",
    "        \n",
    "        # Identity Scores\n",
    "        i_man_score = torch.mean(cosine_similarity[0, male_indices], dim=-1) - torch.mean(cosine_similarity[0, female_indices], dim=-1)\n",
    "        i_woman_score = torch.mean(cosine_similarity[1, female_indices], dim=-1) - torch.mean(cosine_similarity[1, male_indices], dim=-1)\n",
    "        '''\n",
    "        i_young_score = torch.mean(cosine_similarity[2, young_indices], dim=-1) - torch.mean(cosine_similarity[2, old_indices], dim=-1)\n",
    "        i_old_score = torch.mean(cosine_similarity[3, old_indices], dim=-1) - torch.mean(cosine_similarity[3, young_indices], dim=-1)\n",
    "        i_caucasian_score = torch.mean(cosine_similarity[4, caucasian_indices], dim=-1) - torch.mean(cosine_similarity[4, (asian_indices)], dim=-1)\n",
    "        i_asian_score = torch.mean(cosine_similarity[5, asian_indices], dim=-1) - torch.mean(cosine_similarity[5, (caucasian_indices)], dim=-1)\n",
    "        # 6 skip african\n",
    "        i_hat_score = torch.mean(cosine_similarity[7, hat_indices], dim=-1) - torch.mean(cosine_similarity[7, (ponytail_indices + straight_hair_indices + curly_hair_indices)], dim=-1)\n",
    "        i_ponytail_score = torch.mean(cosine_similarity[8, ponytail_indices], dim=-1) - torch.mean(cosine_similarity[8, (hat_indices + straight_hair_indices + curly_hair_indices)], dim=-1)\n",
    "        i_straight_hair_score = torch.mean(cosine_similarity[9, straight_hair_indices], dim=-1) - torch.mean(cosine_similarity[9, curly_hair_indices], dim=-1) # ONLY curly hair as opposite!\n",
    "        i_curly_hair_score = torch.mean(cosine_similarity[10, curly_hair_indices], dim=-1) - torch.mean(cosine_similarity[10, straight_hair_indices], dim=-1) # ONLY straight hair as opposite!\n",
    "        i_beard_score = torch.mean(cosine_similarity[11, beard_indices], dim=-1) - torch.mean(cosine_similarity[11, no_beard_indices], dim=-1)\n",
    "        \n",
    "        # Expression Scores\n",
    "        e_open_mouth_score = torch.mean(cosine_similarity[12, open_mouth_indices], dim=-1) - torch.mean(cosine_similarity[12, (closed_eyes_indices, raised_brows_indices, puffed_cheeks_indices)], dim=-1) # no smiling as opposite b/c mouth can be open when grinning\n",
    "        e_smiling_score = torch.mean(cosine_similarity[13, smiling_indices], dim=-1) - torch.mean(cosine_similarity[13, (open_mouth_indices, closed_eyes_indices, raised_brows_indices, puffed_cheeks_indices)], dim=-1)\n",
    "        e_closed_eyes_score = torch.mean(cosine_similarity[14, closed_eyes_indices], dim=-1) - torch.mean(cosine_similarity[14, (open_mouth_indices, smiling_indices, raised_brows_indices, puffed_cheeks_indices)], dim=-1)\n",
    "        e_raised_brows_score = torch.mean(cosine_similarity[15, raised_brows_indices], dim=-1) - torch.mean(cosine_similarity[15, (open_mouth_indices, smiling_indices, closed_eyes_indices, puffed_cheeks_indices)], dim=-1)\n",
    "        e_puffed_cheeks_score = torch.mean(cosine_similarity[16, puffed_cheeks_indices], dim=-1) - torch.mean(cosine_similarity[16, (open_mouth_indices, smiling_indices, closed_eyes_indices, raised_brows_indices)], dim=-1)\n",
    "        e_sad_score = torch.mean(cosine_similarity[17, sad_indices], dim=-1) - torch.mean(cosine_similarity[17, not_sad_indices], dim=-1)\n",
    "        e_happy_score = torch.mean(cosine_similarity[18, happy_indices], dim=-1) - torch.mean(cosine_similarity[18, not_happy_indices], dim=-1)\n",
    "        \n",
    "        # Stack Scores, add penalty, compute average\n",
    "        i_scores = torch.stack((i_man_score, i_woman_score, i_young_score, i_old_score, i_caucasian_score, i_asian_score, i_hat_score, i_ponytail_score, i_straight_hair_score, i_curly_hair_score, i_beard_score), dim=0)\n",
    "        e_scores = torch.stack((e_open_mouth_score, e_smiling_score, e_closed_eyes_score, e_raised_brows_score, e_puffed_cheeks_score, e_sad_score, e_happy_score), dim=0)\n",
    "        print('i_scores: ', i_scores, 'e_scores: ', e_scores)\n",
    "        scores = torch.stack((i_scores, e_scores), dim=0)\n",
    "        scores_penalized = penalty(scores, 10)\n",
    "        scores_avg = torch.mean(scores_penalized, dim=0)\n",
    "        '''\n",
    "    \n",
    "    return i_man_score # scores_avg\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters to tune and their search spaces\n",
    "    search_space = {\n",
    "        # Camera\n",
    "        \"camera_distance_factor\": trial.suggest_float('camera_distance_factor', 0.1, 0.8), \n",
    "        \"camera_angle\": trial.suggest_categorical('camera_angle', [0., 15., 30., 45., 55.]), \n",
    "        \"focal_length\": trial.suggest_float('focal_length', 2.0, 9.0),\n",
    "        # Image\n",
    "        \"resolution\": trial.suggest_categorical('resolution', [150, 200, 224, 300]), \n",
    "        # Phong\n",
    "        \"ambient_coeff\": trial.suggest_float('ambient_coeff', 0.0, 1.0), \n",
    "        \"diffuse_coeff\": trial.suggest_float('diffuse_coeff', 0.0, 1.0), \n",
    "        \"specular_coeff\": trial.suggest_float('specular_coeff', 0.0, 1.0), \n",
    "        \"shininess\": trial.suggest_categorical('shininess', [0.1, 0.5, 1.0, 3.0, 10.]),\n",
    "        # Colors\n",
    "        \"object_color\": torch.tensor([\n",
    "            trial.suggest_float('object_color_0', 0.0, 1.0),\n",
    "            trial.suggest_float('object_color_1', 0.0, 1.0),\n",
    "            trial.suggest_float('object_color_2', 0.0, 1.0)\n",
    "        ]),\n",
    "        \"background_color\": torch.tensor([\n",
    "            trial.suggest_float('background_color_0', 0.0, 1.0),\n",
    "            trial.suggest_float('background_color_1', 0.0, 1.0),\n",
    "            trial.suggest_float('background_color_2', 0.0, 1.0)\n",
    "        ]),\n",
    "        # Light\n",
    "        \"amb_light_color\": torch.tensor([\n",
    "            trial.suggest_float('amb_light_color_0', 0.0, 1.0),\n",
    "            trial.suggest_float('amb_light_color_1', 0.0, 1.0),\n",
    "            trial.suggest_float('amg_light_color_2', 0.0, 1.0)\n",
    "        ]),\n",
    "        # light 1\n",
    "        \"light_intensity_1\": trial.suggest_float('light_intensity_1', 0.5, 1.5), \n",
    "        \"light_color\": torch.tensor([\n",
    "            trial.suggest_float('light_color_0', 0.7, 1.0),\n",
    "            trial.suggest_float('light_color_1', 0.7, 1.0),\n",
    "            trial.suggest_float('light_color_2', 0.7, 1.0)\n",
    "        ]),\n",
    "        \"light_dir_1\": torch.tensor([\n",
    "            trial.suggest_float('light_dir_1_0', -1.0, 0.0),\n",
    "            trial.suggest_float('light_dir_1_1', -1.0, 0.0),\n",
    "            trial.suggest_float('light_dir_1_2', -1.0, 0.0)\n",
    "        ]),\n",
    "        # light 2\n",
    "        \"light_intensity_2\": trial.suggest_float('light_intensity_2', 0.0, 1.), \n",
    "        \"light_dir_2\": torch.tensor([\n",
    "            0.0,\n",
    "            trial.suggest_float('light_dir_2_1', -1.0, 0.0),\n",
    "            trial.suggest_float('light_dir_2_2', -1.0, 0.0)\n",
    "        ]),\n",
    "        # light 3\n",
    "        \"light_intensity_3\": trial.suggest_float('light_intensity_3', 0.0, 0.7), \n",
    "        \"light_dir_3\": torch.tensor([\n",
    "            trial.suggest_float('light_dir_2_0', -1.0, 0.5),\n",
    "            trial.suggest_float('light_dir_2_1', -1.0, 0.0),\n",
    "            0.0\n",
    "        ]),\n",
    "    }\n",
    "    \n",
    "    hparams.update(search_space)\n",
    "    \n",
    "    return analyze_rendering(hparams)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T10:01:07.088286Z",
     "start_time": "2023-11-24T10:01:06.799972Z"
    }
   },
   "id": "18fe2d0b47dc321"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 11:01:12,785] A new study created in RDB with name: test_9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mesh:  8.87585997581482\n",
      "phong:  2.564879894256592\n",
      "render:  -11.442860126495361\n",
      "katha:  -0.01046299934387207\n",
      "simon:  -0.0034399032592773438\n",
      "clip:  0.3289031982421875\n",
      "mesh:  3.6026413440704346\n",
      "phong:  2.6684398651123047\n",
      "render:  -6.2727978229522705\n",
      "katha:  -0.012449026107788086\n",
      "simon:  -0.005248069763183594\n",
      "clip:  0.3634378910064697\n",
      "mesh:  2.449267864227295\n",
      "phong:  1.9240641593933105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-24 11:01:38,976] Trial 0 finished with value: 0.0015973001718521118 and parameters: {'camera_distance_factor': 0.7903597653060517, 'camera_angle': 15.0, 'focal_length': 2.179374614805111, 'resolution': 200, 'ambient_coeff': 0.7146025384876417, 'diffuse_coeff': 0.2913364154805379, 'specular_coeff': 0.12108495122994334, 'shininess': 10.0, 'object_color_0': 0.0790365954970973, 'object_color_1': 0.2667472661838203, 'object_color_2': 0.5589730574693859, 'background_color_0': 0.5470725765589803, 'background_color_1': 0.19383136388068611, 'background_color_2': 0.5576085692255932, 'amb_light_color_0': 0.7157691518299166, 'amb_light_color_1': 0.3865288912661503, 'amg_light_color_2': 0.49244144031585046, 'light_intensity_1': 0.8585998411570825, 'light_color_0': 0.8920148052192268, 'light_color_1': 0.8711492633731552, 'light_color_2': 0.7633590677022483, 'light_dir_1_0': -0.5755090155831443, 'light_dir_1_1': -0.5452185603199955, 'light_dir_1_2': -0.7882542173257472, 'light_intensity_2': 0.42351961294002893, 'light_dir_2_1': -0.1725047266190407, 'light_dir_2_2': -0.4322611020156053, 'light_intensity_3': 0.4816493418593561, 'light_dir_2_0': 0.1261006260933648}. Best is trial 0 with value: 0.0015973001718521118.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "render:  -4.374833345413208\n",
      "katha:  -0.009271621704101562\n",
      "simon:  -0.00310516357421875\n",
      "clip:  0.43361926078796387\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'camera_distance_factor': 0.7903597653060517,\n 'camera_angle': 15.0,\n 'focal_length': 2.179374614805111,\n 'resolution': 200,\n 'ambient_coeff': 0.7146025384876417,\n 'diffuse_coeff': 0.2913364154805379,\n 'specular_coeff': 0.12108495122994334,\n 'shininess': 10.0,\n 'object_color_0': 0.0790365954970973,\n 'object_color_1': 0.2667472661838203,\n 'object_color_2': 0.5589730574693859,\n 'background_color_0': 0.5470725765589803,\n 'background_color_1': 0.19383136388068611,\n 'background_color_2': 0.5576085692255932,\n 'amb_light_color_0': 0.7157691518299166,\n 'amb_light_color_1': 0.3865288912661503,\n 'amg_light_color_2': 0.49244144031585046,\n 'light_intensity_1': 0.8585998411570825,\n 'light_color_0': 0.8920148052192268,\n 'light_color_1': 0.8711492633731552,\n 'light_color_2': 0.7633590677022483,\n 'light_dir_1_0': -0.5755090155831443,\n 'light_dir_1_1': -0.5452185603199955,\n 'light_dir_1_2': -0.7882542173257472,\n 'light_intensity_2': 0.42351961294002893,\n 'light_dir_2_1': -0.1725047266190407,\n 'light_dir_2_2': -0.4322611020156053,\n 'light_intensity_3': 0.4816493418593561,\n 'light_dir_2_0': 0.1261006260933648}"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study = optuna.create_study(storage=\"sqlite:///optuna_study.db\", study_name=\"test_9\", direction='maximize')\n",
    "study.optimize(objective, n_trials=1)\n",
    "\n",
    "study.best_params"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T10:01:39.070591Z",
     "start_time": "2023-11-24T10:01:12.193325Z"
    }
   },
   "id": "8660eafbf534fc18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a30d2a40785edca"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
