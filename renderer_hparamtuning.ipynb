{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-23T12:36:57.787904Z",
     "start_time": "2023-11-23T12:36:46.375458Z"
    }
   },
   "outputs": [],
   "source": [
    "import clip\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import os.path as osp\n",
    "import torch\n",
    "import pyvista as pv\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import trimesh\n",
    "import time\n",
    "import optuna\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Rendering Setup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98e57945907ff5f2"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# --- Hyperparameter Setup ---\n",
    "\n",
    "hparams = {\n",
    "    # Camera\n",
    "    \"camera_distance_factor\": 3.11/6.,\n",
    "    \"camera_angle\": 45.,\n",
    "    \"focal_length\": 6.,\n",
    "    # Image\n",
    "    \"resolution\": 224,\n",
    "    # Phong\n",
    "    \"ambient_coeff\": 0.5,\n",
    "    \"diffuse_coeff\": 0.7,\n",
    "    \"specular_coeff\": 0.0,\n",
    "    \"shininess\": 3.0,\n",
    "    # Colors\n",
    "    \"object_color\": torch.tensor([0.61, 0.61, 0.61]),\n",
    "    \"background_color\": torch.tensor([0.46, 0, 0]),\n",
    "    # Light\n",
    "    \"amb_light_color\": torch.tensor([0.15, 0, 0]),\n",
    "    # light 1\n",
    "    \"light_intensity_1\": 1., \n",
    "    \"light_color\": torch.tensor([1.0, 1.0, 1.0]), \n",
    "    \"light_dir_1\": torch.tensor([-1., -1., -1.]), \n",
    "    # light 2\n",
    "    \"light_intensity_2\": 1.,\n",
    "    \"light_dir_2\": torch.tensor([0., 0., -1.]), \n",
    "    # light 3\n",
    "    \"light_intensity_3\": 0., \n",
    "    \"light_dir_3\": torch.tensor([0., -1., 0.])\n",
    "}\n",
    "\n",
    "# --- Rendering Function ---\n",
    "\n",
    "def phong_model(sdf, points, camera_position, phong_params, light_params, mesh_path, index_tri=None):\n",
    "    \n",
    "    # Option 1: Use SDF\n",
    "    #normals = estimate_normals(sdf, points)\n",
    "    # Option 2: Use Mesh\n",
    "    normals = mesh_normals(mesh_path, index_tri)\n",
    "    view_dirs = points - camera_position \n",
    "    light_dir_1 = light_params[\"light_dir_1\"].repeat(points.shape[0], 1) \n",
    "    light_dir_2 = light_params[\"light_dir_2\"].repeat(points.shape[0], 1)\n",
    "    light_dir_3 = light_params[\"light_dir_3\"].repeat(points.shape[0], 1)\n",
    "    \n",
    "    # Normalize all vectors\n",
    "    normals = (normals.T / torch.norm(normals, dim=-1)).T\n",
    "    light_dir_norm_1 = (light_dir_1.T / torch.norm(light_dir_1, dim=-1)).T\n",
    "    light_dir_norm_2 = (light_dir_2.T / torch.norm(light_dir_2, dim=-1)).T\n",
    "    light_dir_norm_3 = (light_dir_3.T / torch.norm(light_dir_3, dim=-1)).T\n",
    "    view_dir_norm = (view_dirs.T / torch.norm(view_dirs, dim=-1)).T\n",
    "    \n",
    "    # Ambient\n",
    "    ambient = phong_params[\"ambient_coeff\"] * light_params[\"amb_light_color\"] \n",
    "    ambient_refl = ambient.repeat(points.shape[0], 1)\n",
    "    \n",
    "    # Area light 1\n",
    "    diffuse_1 = phong_params[\"diffuse_coeff\"] * torch.clamp(torch.sum(-light_dir_norm_1 * normals, dim=-1), min=0.0) * light_params[\"light_intensity_1\"] # [N]\n",
    "    diffuse_refl_1 = torch.matmul(diffuse_1.unsqueeze(1), light_params[\"light_color_1\"].unsqueeze(0)) # [N, 3]\n",
    "    reflect_dir_1 = light_dir_norm_1 + (2 * normals.T * torch.clamp(torch.sum(-light_dir_norm_1 * normals, dim=-1), min=0.0)).T\n",
    "    specular_1 = phong_params[\"specular_coeff\"] * torch.pow(torch.clamp(torch.sum(reflect_dir_1 * -view_dir_norm, dim=-1), min=0.0), phong_params[\"shininess\"]) * light_params[\"light_intensity_1\"] # [N]\n",
    "    specular_refl_1 = torch.matmul(specular_1.unsqueeze(1), light_params[\"light_color_1\"].unsqueeze(0)) # [N, 3]\n",
    "    \n",
    "    # Area light 2\n",
    "    diffuse_2 = phong_params[\"diffuse_coeff\"] * torch.clamp(torch.sum(-light_dir_norm_2 * normals, dim=-1), min=0.0) * light_params[\"light_intensity_2\"] # [N]\n",
    "    diffuse_refl_2 = torch.matmul(diffuse_2.unsqueeze(1), light_params[\"light_color_2\"].unsqueeze(0)) # [N, 3]\n",
    "    reflect_dir_2 = light_dir_norm_2 + (2 * normals.T * torch.clamp(torch.sum(-light_dir_norm_2 * normals, dim=-1), min=0.0)).T\n",
    "    specular_2 = phong_params[\"specular_coeff\"] * torch.pow(torch.clamp(torch.sum(reflect_dir_2 * -view_dir_norm, dim=-1), min=0.0), phong_params[\"shininess\"]) * light_params[\"light_intensity_2\"] # [N]\n",
    "    specular_refl_2 = torch.matmul(specular_2.unsqueeze(1), light_params[\"light_color_2\"].unsqueeze(0)) # [N, 3]\n",
    "    \n",
    "    # Area light 3\n",
    "    diffuse_3 = phong_params[\"diffuse_coeff\"] * torch.clamp(torch.sum(-light_dir_norm_3 * normals, dim=-1), min=0.0) * light_params[\"light_intensity_3\"] # [N]\n",
    "    diffuse_refl_3 = torch.matmul(diffuse_3.unsqueeze(1), light_params[\"light_color_3\"].unsqueeze(0)) # [N, 3]\n",
    "    reflect_dir_3 = light_dir_norm_3 + (2 * normals.T * torch.clamp(torch.sum(-light_dir_norm_3 * normals, dim=-1), min=0.0)).T\n",
    "    specular_3 = phong_params[\"specular_coeff\"] * torch.pow(torch.clamp(torch.sum(reflect_dir_3 * -view_dir_norm, dim=-1), min=0.0), phong_params[\"shininess\"]) * light_params[\"light_intensity_3\"] # [N]\n",
    "    specular_refl_3 = torch.matmul(specular_3.unsqueeze(1), light_params[\"light_color_3\"].unsqueeze(0)) # [N, 3]\n",
    "    \n",
    "    # point light\n",
    "    #diffuse_1 = diffuse_coeff * torch.clamp(torch.sum(-light_dir_norm_1 * normals, dim=-1), min=0.0) * light_intensity_1 # [N]\n",
    "    #diffuse_refl_1 = torch.matmul(diffuse_1.unsqueeze(1), light_color.unsqueeze(0)) # [N, 3]\n",
    "    #reflect_dir_1 = light_dir_norm_1 + (2 * normals.T * torch.clamp(torch.sum(-light_dir_norm_1 * normals, dim=-1), min=0.0)).T\n",
    "    #specular_1 = specular_coeff * torch.pow(torch.clamp(torch.sum(reflect_dir_1 * -view_dir_norm, dim=-1), min=0.0), shininess) * light_intensity_1 # [N]\n",
    "    #specular_refl_1 = torch.matmul(specular_1.unsqueeze(1), light_color.unsqueeze(0)) # [N, 3]\n",
    "\n",
    "    return ambient_refl + diffuse_refl_1 + specular_refl_1 + diffuse_refl_2 + specular_refl_2 + diffuse_refl_3 + specular_refl_3\n",
    "\n",
    "def estimate_normals(sdf, points, epsilon=1e-3):\n",
    "    sdf_inputs = torch.concat([points,\n",
    "                              points + torch.tensor([epsilon, 0, 0]),\n",
    "                              points + torch.tensor([0, epsilon, 0]),\n",
    "                              points + torch.tensor([0, 0, epsilon])])\n",
    "\n",
    "    sdf_values = sdf(sdf_inputs).reshape(4, -1)\n",
    "\n",
    "    # Calculate the gradient using finite differences\n",
    "    gradient = sdf_values[1:] - sdf_values[0]\n",
    "\n",
    "    # Normalize the gradient to obtain the estimated normal\n",
    "    normal = gradient / torch.norm(gradient, p=2, dim=0)\n",
    "\n",
    "    return normal.T\n",
    "\n",
    "def sphere_trace(sdf, camera_position, norm_directions, max_length):\n",
    "    N = norm_directions.shape[0]\n",
    "    positions = camera_position.unsqueeze(dim=0).repeat(N, 1) # [N, 3]\n",
    "    total_distances = torch.zeros(N)\n",
    "    last_distances = torch.ones(N)\n",
    "\n",
    "    for _ in range(20):\n",
    "        #mask = torch.logical_and(total_distances < max_length, last_distances > 1e-3)\n",
    "        not_reached_max_distance = total_distances < max_length\n",
    "        not_hit_target = torch.abs(last_distances) > 1e-3\n",
    "        mask = torch.logical_and(not_reached_max_distance, not_hit_target)\n",
    "        if torch.all(torch.logical_not(mask)):\n",
    "            break\n",
    "        distances = sdf(positions[mask])\n",
    "        steps = (norm_directions[mask].T * distances).T\n",
    "        positions[mask] += steps\n",
    "        total_distances[mask] += distances\n",
    "        last_distances[mask] = distances\n",
    "\n",
    "    #positions[total_distances > max_length] *= torch.nan\n",
    "    return positions, total_distances < max_length\n",
    "\n",
    "def mesh_trace(mesh_path, ray_starts, ray_directions):\n",
    "\n",
    "    mesh = trimesh.load_mesh(mesh_path)\n",
    "    \n",
    "    ray_origins = ray_starts.repeat(ray_directions.shape[0], 1)\n",
    "    \n",
    "    intersections, index_ray, index_tri = mesh.ray.intersects_location(ray_origins, ray_directions, multiple_hits=False) \n",
    "    \n",
    "    mask = torch.zeros(ray_directions.shape[0], dtype=torch.bool)\n",
    "    mask[index_ray] = True\n",
    "    \n",
    "    points = torch.FloatTensor(intersections).to(dtype=torch.float32)\n",
    "\n",
    "    return points, mask, index_tri\n",
    "\n",
    "\n",
    "def mesh_normals(mesh_path, index_tri):\n",
    "    mesh = trimesh.load_mesh(mesh_path)\n",
    "    all_normals = mesh.face_normals\n",
    "    normals = all_normals[index_tri]\n",
    "    normals_torch = torch.from_numpy(normals).to(dtype=torch.float32)\n",
    "    \n",
    "    return normals_torch\n",
    "\n",
    "def render(sdf, camera_params, phong_params, light_params, mesh_path=None):\n",
    "    \n",
    "    pu = camera_params[\"resolution_x\"]\n",
    "    pv = camera_params[\"resolution_y\"]\n",
    "    image = phong_params[\"background_color\"].repeat(pu * pv, 1)\n",
    "    \n",
    "    angle_radians = torch.deg2rad_(torch.tensor(camera_params[\"camera_angle\"]))\n",
    "    camera = torch.tensor([torch.sin(angle_radians), 0, torch.cos(angle_radians)])\n",
    "    camera_position = camera * (camera_params[\"camera_distance\"] + camera_params[\"focal_length\"]) / camera.norm()\n",
    "    \n",
    "    # Normalize the xy value of the current pixel [-0.5, 0.5]\n",
    "    u_norms = ((torch.arange(pu) + 0.5) / pu - 0.5) * pu/pv\n",
    "    v_norms = 0.5 - (torch.arange(pv) + 0.5) / pv\n",
    "\n",
    "    # Calculate the ray directions for all pixels\n",
    "    directions_unn = torch.cat(torch.meshgrid(u_norms, v_norms, torch.tensor(-camera_params[\"focal_length\"])), dim=-1) \n",
    "    directions_unn = directions_unn.reshape((pu*pv, 3)) # [pu, pv, 3] --> [pu*pv, 3] (u1, v1, f)(u1, v2, f)...(u2, v1, f)...\n",
    "\n",
    "    # rotate about y-axis\n",
    "    rotation_matrix = torch.tensor([[torch.cos(angle_radians), 0, torch.sin(angle_radians)],\n",
    "                                    [0, 1, 0],\n",
    "                                    [-torch.sin(angle_radians), 0, torch.cos(angle_radians)]])\n",
    "    rotated_directions = torch.matmul(directions_unn, rotation_matrix.T)\n",
    "    \n",
    "    transposed_directions = rotated_directions.T #transpose is necessary for normalization\n",
    "    directions = (transposed_directions / transposed_directions.norm(dim=0)).T # [pu*pv, 3]\n",
    "\n",
    "    # Option 1: Use SDF\n",
    "    #hit_positions, hit_mask = sphere_trace(sdf, camera_position, directions, max_ray_length)\n",
    "    # Option 2: Use Mesh\n",
    "    intersections, hit_mask, index_tri = mesh_trace(mesh_path, camera_position, directions) \n",
    "    \n",
    "    # Option 1: Use SDF\n",
    "    #reflections = phong_model(sdf, hit_positions[hit_mask], camera_position, ambient_coeff, diffuse_coeff, specular_coeff, shininess, mesh_path, amb_light_color, light_intensity_1, light_color_1, light_dir_1, light_intensity_2, light_color_2, light_dir_2, light_intensity_3, light_color_3, light_dir_3, index_tri=None)\n",
    "    # Option 2: Use Mesh\n",
    "    reflections = phong_model(sdf, intersections, camera_position, phong_params, light_params, mesh_path, index_tri) # mesh alternative\n",
    "\n",
    "    # Assign a color for objects\n",
    "    image[hit_mask] = torch.mul(reflections, phong_params[\"object_color\"].repeat(reflections.shape[0], 1))\n",
    "    image = torch.clamp(image, max=1.0)\n",
    "    image = image.reshape(pu, pv, 3).transpose(0, 1)\n",
    "\n",
    "    return image"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T13:32:40.903987Z",
     "start_time": "2023-11-23T13:32:40.729573Z"
    }
   },
   "id": "fcc5fac445bb6f43"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare Score Calculation Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62dda701d3a74cb2"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# --- Get Data Annotations ---\n",
    "\n",
    "path_to_head_annotations = '/Users/katharinaschmid/Text2Head/rendering_data/head_annotations.json'\n",
    "with open(path_to_head_annotations) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# --- Prepare Indices ---\n",
    "\n",
    "# Identity: Gender\n",
    "male_indices = [index for index, head in enumerate(data['heads']) if head['gender'] == 'man']\n",
    "female_indices = [index for index, head in enumerate(data['heads']) if head['gender'] == 'woman']\n",
    "\n",
    "# Identity: Age\n",
    "young_indices = [index for index, head in enumerate(data['heads']) if head['age'] == 'young']\n",
    "old_indices = [index for index, head in enumerate(data['heads']) if head['age'] == 'old']\n",
    "\n",
    "# Identity: Ethnicity\n",
    "caucasian_indices = [index for index, head in enumerate(data['heads']) if head['ethnicity'] == 'Caucasian'] \n",
    "asian_indices = [index for index, head in enumerate(data['heads']) if head['ethnicity'] == 'Asian'] \n",
    "african_indices = [index for index, head in enumerate(data['heads']) if head['ethnicity'] == 'African'] \n",
    "\n",
    "# Identity: Hairstyle\n",
    "hat_indices = [index for index, head in enumerate(data['heads']) if head['hairstyle'] == 'hat']\n",
    "ponytail_indices = [index for index, head in enumerate(data['heads']) if head['hairstyle'] == 'ponytail']\n",
    "straight_hair_indices = [index for index, head in enumerate(data['heads']) if head['hairstyle'] == 'straight hair']\n",
    "curly_hair_indices = [index for index, head in enumerate(data['heads']) if head['hairstyle'] == 'curly hair']\n",
    "\n",
    "# Identity: Beard\n",
    "beard_indices = [index for index, head in enumerate(data['heads']) if head['beard'] == 'yes']\n",
    "no_beard_indices = [index for index, head in enumerate(data['heads']) if head['beard'] == 'no']\n",
    "\n",
    "# Expression: Facial Expression\n",
    "open_mouth_indices = [index for index, head in enumerate(data['heads']) if head['facial_expression'] == 'open mouth']\n",
    "smiling_indices = [index for index, head in enumerate(data['heads']) if head['facial_expression'] == 'smiling']\n",
    "closed_eyes_indices = [index for index, head in enumerate(data['heads']) if head['facial_expression'] == 'closed eyes']\n",
    "raised_brows_indices = [index for index, head in enumerate(data['heads']) if head['facial_expression'] == 'raised brows']\n",
    "puffed_cheeks_indices = [index for index, head in enumerate(data['heads']) if head['facial_expression'] == 'puffed_cheeks']\n",
    "\n",
    "# Expression: Emotion \n",
    "sad_indices = [index for index, head in enumerate(data['heads']) if head['emotion'] == 'sad']\n",
    "happy_indices = [index for index, head in enumerate(data['heads']) if head['emotion'] == 'happy']\n",
    "\n",
    "# --- Precompute CLIP Text Embeddings ---\n",
    "captions = []\n",
    "captions.append('A man') # 0\n",
    "captions.append('A woman') # 1\n",
    "captions.append('A young person') # 2\n",
    "captions.append('An old person') # 3\n",
    "captions.append('A Caucasian person') # 4\n",
    "captions.append('An Asian person') # 5\n",
    "captions.append('An African person') # 6\n",
    "captions.append('A person wearing a hat') # 7\n",
    "captions.append('A person with a ponytail') # 8\n",
    "captions.append('A person with straight hair') # 9\n",
    "captions.append('A person with curly hair') # 10\n",
    "captions.append('A person with beard') # 11\n",
    "captions.append('A person with open mouth') # 12\n",
    "captions.append('A smiling person') # 13\n",
    "captions.append('A person with closed eyes') # 14\n",
    "captions.append('A person with raised brows') # 15\n",
    "captions.append('A person with puffed cheeks') # 16\n",
    "captions.append('A sad person') # 17\n",
    "captions.append('A happy person') # 18\n",
    "\n",
    "preprocessed_text = clip.tokenize(captions).to(device) # [num_captions, 77]\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(preprocessed_text) # [num_captions, 512]\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True) # [num_captions, 512]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T13:32:48.355370Z",
     "start_time": "2023-11-23T13:32:43.099396Z"
    }
   },
   "id": "ccd21bc560b0eefe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameter Search for Renderer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff2474134e2af3d1"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# --- Penalty Function to penalize negative Signal Scores ---\n",
    "def penalty(vector, k):\n",
    "    vector[vector < 0] *= k\n",
    "    return vector\n",
    "\n",
    "def analyze_rendering(hparams):\n",
    "    \n",
    "    # --- Get Hyperparameters ---\n",
    "    camera_params = {\n",
    "        \"camera_distance\": hparams['camera_distance_factor'] * hparams['focal_length'], # ensures appropriate head size\n",
    "        \"camera_angle\": hparams['camera_angle'],\n",
    "        \"focal_length\": hparams['focal_length'],\n",
    "        \"max_ray_length\": (hparams['camera_distance_factor'] + 1) * hparams['focal_length'] + 1.5,\n",
    "        # Image\n",
    "        \"resolution_y\": hparams['resolution'],\n",
    "        \"resolution_x\": hparams['resolution']\n",
    "    }\n",
    "    phong_params = {\n",
    "        \"ambient_coeff\": hparams['ambient_coeff'],\n",
    "        \"diffuse_coeff\": hparams['diffuse_coeff'],\n",
    "        \"specular_coeff\": hparams['specular_coeff'],\n",
    "        \"shininess\": hparams['shininess'],\n",
    "        # Colors\n",
    "        \"object_color\": hparams['object_color'],\n",
    "        \"background_color\": hparams['background_color']\n",
    "    }\n",
    "    \n",
    "    light_params = {\n",
    "        \"amb_light_color\": hparams['amb_light_color'],\n",
    "        # light 1\n",
    "        \"light_intensity_1\": hparams['light_intensity_1'],\n",
    "        \"light_color_1\": hparams['light_color'],\n",
    "        \"light_dir_1\": hparams['light_dir_1'],\n",
    "        # light 2\n",
    "        \"light_intensity_2\": hparams['light_intensity_2'],\n",
    "        \"light_color_2\": hparams['light_color'],\n",
    "        \"light_dir_2\": hparams['light_dir_2'],\n",
    "        # light 3\n",
    "        \"light_intensity_3\": hparams['light_intensity_3'],\n",
    "        \"light_color_3\": hparams['light_color'],\n",
    "        \"light_dir_3\": hparams['light_dir_3'],\n",
    "    }\n",
    "    \n",
    "    # --- Render and Embed Images ---\n",
    "    folder_path = '/Users/katharinaschmid/Text2Head/rendering_data/meshes/'\n",
    "    file_list = sorted(os.listdir(folder_path))\n",
    "    file_list = [file for file in file_list if not file.startswith('.DS_Store')]\n",
    "    \n",
    "    image_embeddings_list = []\n",
    "    with torch.no_grad():\n",
    "        for mesh in file_list:\n",
    "            mesh_path = folder_path + mesh\n",
    "            sdf = None\n",
    "            rendered_image = render(sdf, camera_params, phong_params, light_params, mesh_path)\n",
    "            image = (rendered_image * 255).byte()  # Convert to a byte tensor (0-255)\n",
    "            image_np = image.cpu().numpy()\n",
    "            image_pil = Image.fromarray(image_np)\n",
    "            preprocessed_image = preprocess(image_pil).unsqueeze(0) # [1, 3, 224, 224]\n",
    "            image_features = model.encode_image(preprocessed_image) # [1, 512]\n",
    "            image_embeddings_list.append(image_features)\n",
    "        image_embeddings = torch.cat(image_embeddings_list, dim=0)\n",
    "        image_embeddings /= image_embeddings.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        # --- Get CLIP Similarity and Scores ---\n",
    "        cosine_similarity = torch.matmul(text_features, image_embeddings.T) #[num_captions*tests, num_images]\n",
    "        \n",
    "        # Identity Scores\n",
    "        i_man_score = torch.mean(cosine_similarity[0, male_indices], dim=-1) - torch.mean(cosine_similarity[0, female_indices], dim=-1)\n",
    "        i_woman_score = torch.mean(cosine_similarity[1, female_indices], dim=-1) - torch.mean(cosine_similarity[1, male_indices], dim=-1)\n",
    "        i_young_score = torch.mean(cosine_similarity[2, young_indices], dim=-1) - torch.mean(cosine_similarity[2, old_indices], dim=-1)\n",
    "        i_old_score = torch.mean(cosine_similarity[3, old_indices], dim=-1) - torch.mean(cosine_similarity[3, young_indices], dim=-1)\n",
    "        i_caucasian_score = torch.mean(cosine_similarity[4, caucasian_indices], dim=-1) - torch.mean(cosine_similarity[4, (asian_indices + african_indices)], dim=-1)\n",
    "        i_asian_score = torch.mean(cosine_similarity[5, asian_indices], dim=-1) - torch.mean(cosine_similarity[5, (caucasian_indices + african_indices)], dim=-1)\n",
    "        # 6 skip african\n",
    "        i_hat_score = torch.mean(cosine_similarity[7, hat_indices], dim=-1) - torch.mean(cosine_similarity[7, (ponytail_indices + straight_hair_indices + curly_hair_indices)], dim=-1)\n",
    "        i_ponytail_score = torch.mean(cosine_similarity[8, ponytail_indices], dim=-1) - torch.mean(cosine_similarity[8, (hat_indices + straight_hair_indices + curly_hair_indices)], dim=-1)\n",
    "        i_straight_hair_score = torch.mean(cosine_similarity[9, straight_hair_indices], dim=-1) - torch.mean(cosine_similarity[9, curly_hair_indices], dim=-1) # ONLY curly hair as opposite!\n",
    "        i_curly_hair_score = torch.mean(cosine_similarity[10, curly_hair_indices], dim=-1) - torch.mean(cosine_similarity[10, straight_hair_indices], dim=-1) # ONLY straight hair as opposite!\n",
    "        i_beard_score = torch.mean(cosine_similarity[11, beard_indices], dim=-1) - torch.mean(cosine_similarity[11, no_beard_indices], dim=-1)\n",
    "        \n",
    "        # Expression Scores\n",
    "        e_open_mouth_score = torch.mean(cosine_similarity[12, open_mouth_indices], dim=-1) - torch.mean(cosine_similarity[12, (closed_eyes_indices, raised_brows_indices, puffed_cheeks_indices)], dim=-1) # no smiling as opposite b/c mouth can be open when grinning\n",
    "        e_smiling_score = torch.mean(cosine_similarity[13, smiling_indices], dim=-1) - torch.mean(cosine_similarity[13, (open_mouth_indices, closed_eyes_indices, raised_brows_indices, puffed_cheeks_indices)], dim=-1)\n",
    "        e_closed_eyes_score = torch.mean(cosine_similarity[14, closed_eyes_indices], dim=-1) - torch.mean(cosine_similarity[14, (open_mouth_indices, smiling_indices, raised_brows_indices, puffed_cheeks_indices)], dim=-1)\n",
    "        e_raised_brows_score = torch.mean(cosine_similarity[15, raised_brows_indices], dim=-1) - torch.mean(cosine_similarity[15, (open_mouth_indices, smiling_indices, closed_eyes_indices, puffed_cheeks_indices)], dim=-1)\n",
    "        e_puffed_cheeks_score = torch.mean(cosine_similarity[16, puffed_cheeks_indices], dim=-1) - torch.mean(cosine_similarity[16, (open_mouth_indices, smiling_indices, closed_eyes_indices, raised_brows_indices)], dim=-1)\n",
    "        e_sad_score = torch.mean(cosine_similarity[17, sad_indices], dim=-1) - torch.mean(cosine_similarity[17, happy_indices], dim=-1)\n",
    "        e_happy_score = torch.mean(cosine_similarity[18, happy_indices], dim=-1) - torch.mean(cosine_similarity[18, sad_indices], dim=-1)\n",
    "        \n",
    "        # Stack Scores, add penalty, compute average\n",
    "        i_scores = torch.stack((i_man_score, i_woman_score, i_young_score, i_old_score, i_caucasian_score, i_asian_score, i_hat_score, i_ponytail_score, i_straight_hair_score, i_curly_hair_score, i_beard_score), dim=0)\n",
    "        e_scores = torch.stack((e_open_mouth_score, e_smiling_score, e_closed_eyes_score, e_raised_brows_score, e_puffed_cheeks_score, e_sad_score, e_happy_score), dim=0)\n",
    "        print('i_scores: ', i_scores, 'e_scores: ', e_scores)\n",
    "        scores = torch.stack((i_scores, e_scores), dim=0)\n",
    "        scores_penalized = penalty(scores, 10)\n",
    "        scores_avg = torch.mean(scores_penalized, dim=0)\n",
    "    \n",
    "    return scores_avg\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters to tune and their search spaces\n",
    "    search_space = {\n",
    "        # Camera\n",
    "        \"camera_distance_factor\": trial.suggest_float('camera_distance_factor', 0.1, 0.8), \n",
    "        \"camera_angle\": trial.suggest_categorical('camera_angle', [0., 15., 30., 45., 55.]), \n",
    "        \"focal_length\": trial.suggest_float('focal_length', 2.0, 9.0),\n",
    "        # Image\n",
    "        \"resolution\": trial.suggest_categorical('resolution', [150, 200, 224, 300]), \n",
    "        # Phong\n",
    "        \"ambient_coeff\": trial.suggest_float('ambient_coeff', 0.0, 1.0), \n",
    "        \"diffuse_coeff\": trial.suggest_float('diffuse_coeff', 0.0, 1.0), \n",
    "        \"specular_coeff\": trial.suggest_float('specular_coeff', 0.0, 1.0), \n",
    "        \"shininess\": trial.suggest_categorical('shininess', [0.1, 0.5, 1.0, 3.0, 10.]),\n",
    "        # Colors\n",
    "        \"object_color\": torch.tensor([\n",
    "            trial.suggest_float('object_color_0', 0.0, 1.0),\n",
    "            trial.suggest_float('object_color_1', 0.0, 1.0),\n",
    "            trial.suggest_float('object_color_2', 0.0, 1.0)\n",
    "        ]),\n",
    "        \"background_color\": torch.tensor([\n",
    "            trial.suggest_float('background_color_0', 0.0, 1.0),\n",
    "            trial.suggest_float('background_color_1', 0.0, 1.0),\n",
    "            trial.suggest_float('background_color_2', 0.0, 1.0)\n",
    "        ]),\n",
    "        # Light\n",
    "        \"amb_light_color\": torch.tensor([\n",
    "            trial.suggest_float('amb_light_color_0', 0.0, 1.0),\n",
    "            trial.suggest_float('amb_light_color_1', 0.0, 1.0),\n",
    "            trial.suggest_float('amg_light_color_2', 0.0, 1.0)\n",
    "        ]),\n",
    "        # light 1\n",
    "        \"light_intensity_1\": trial.suggest_float('light_intensity_1', 0.5, 1.5), \n",
    "        \"light_color\": torch.tensor([\n",
    "            trial.suggest_float('light_color_0', 0.7, 1.0),\n",
    "            trial.suggest_float('light_color_1', 0.7, 1.0),\n",
    "            trial.suggest_float('light_color_2', 0.7, 1.0)\n",
    "        ]),\n",
    "        \"light_dir_1\": torch.tensor([\n",
    "            trial.suggest_float('light_dir_1_0', -1.0, 0.0),\n",
    "            trial.suggest_float('light_dir_1_1', -1.0, 0.0),\n",
    "            trial.suggest_float('light_dir_1_2', -1.0, 0.0)\n",
    "        ]),\n",
    "        # light 2\n",
    "        \"light_intensity_2\": trial.suggest_float('light_intensity_2', 0.0, 1.), \n",
    "        \"light_dir_2\": torch.tensor([\n",
    "            0.0,\n",
    "            trial.suggest_float('light_dir_2_1', -1.0, 0.0),\n",
    "            trial.suggest_float('light_dir_2_2', -1.0, 0.0)\n",
    "        ]),\n",
    "        # light 3\n",
    "        \"light_intensity_3\": trial.suggest_float('light_intensity_3', 0.0, 0.7), \n",
    "        \"light_dir_3\": torch.tensor([\n",
    "            trial.suggest_float('light_dir_2_0', -1.0, 0.5),\n",
    "            trial.suggest_float('light_dir_2_1', -1.0, 0.0),\n",
    "            0.0\n",
    "        ]),\n",
    "    }\n",
    "    \n",
    "    hparams.update(search_space)\n",
    "    \n",
    "    return analyze_rendering(hparams)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T13:52:42.320644Z",
     "start_time": "2023-11-23T13:52:42.281371Z"
    }
   },
   "id": "18fe2d0b47dc321"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-23 15:02:10,052] A new study created in RDB with name: test_2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_list:  ['01.ply', '02.ply', '03.ply']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-23 15:02:28,401] Trial 0 finished with value: 0.026060357689857483 and parameters: {'camera_distance_factor': 0.4262159524767436, 'camera_angle': 45.0, 'focal_length': 2.8588476021095603, 'resolution': 224, 'ambient_coeff': 0.37326565625223007, 'diffuse_coeff': 0.38317351714198056, 'specular_coeff': 0.9486262710673882, 'shininess': 0.1, 'object_color_0': 0.42585222969783176, 'object_color_1': 0.9343647618107314, 'object_color_2': 0.40583866285955716, 'background_color_0': 0.6825755786499671, 'background_color_1': 0.12299008231283204, 'background_color_2': 0.5865344696862822, 'amb_light_color_0': 0.40382610739760083, 'amb_light_color_1': 0.5327496805841134, 'amg_light_color_2': 0.5502157681648859, 'light_intensity_1': 0.6163791478537356, 'light_color_0': 0.7305198710849781, 'light_color_1': 0.8587854743569627, 'light_color_2': 0.7135445661080153, 'light_dir_1_0': -0.3947715529507573, 'light_dir_1_1': -0.8557223172563867, 'light_dir_1_2': -0.9508653955941185, 'light_intensity_2': 0.5728753113510167, 'light_dir_2_1': -0.7932316202321615, 'light_dir_2_2': -0.7702666673424424, 'light_intensity_3': 0.3622748688964944, 'light_dir_2_0': -0.7079762311103447}. Best is trial 0 with value: 0.026060357689857483.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_list:  ['01.ply', '02.ply', '03.ply']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-23 15:02:41,808] Trial 1 finished with value: 0.024325087666511536 and parameters: {'camera_distance_factor': 0.40916219686271427, 'camera_angle': 55.0, 'focal_length': 2.8636172099657142, 'resolution': 200, 'ambient_coeff': 0.6754888567579762, 'diffuse_coeff': 0.6458600858367192, 'specular_coeff': 0.9827301028175933, 'shininess': 0.5, 'object_color_0': 0.06067822055443284, 'object_color_1': 0.6135365973137048, 'object_color_2': 0.33690484274308075, 'background_color_0': 0.3243916221990384, 'background_color_1': 0.2611450172828872, 'background_color_2': 0.3826185806237734, 'amb_light_color_0': 0.6336746585485075, 'amb_light_color_1': 0.8066451070752583, 'amg_light_color_2': 0.7765048322850088, 'light_intensity_1': 1.2118817755070204, 'light_color_0': 0.707126379242294, 'light_color_1': 0.8698593393309547, 'light_color_2': 0.9275893612939745, 'light_dir_1_0': -0.850562305767666, 'light_dir_1_1': -0.35364776575497603, 'light_dir_1_2': -0.1557939448504324, 'light_intensity_2': 0.4706549822272449, 'light_dir_2_1': -0.05998886413665139, 'light_dir_2_2': -0.5110405686254595, 'light_intensity_3': 0.06653481992666435, 'light_dir_2_0': -0.153673846868011}. Best is trial 0 with value: 0.026060357689857483.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_list:  ['01.ply', '02.ply', '03.ply']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-23 15:02:53,312] Trial 2 finished with value: 0.009080596268177032 and parameters: {'camera_distance_factor': 0.5203535063226872, 'camera_angle': 45.0, 'focal_length': 4.223318177978973, 'resolution': 150, 'ambient_coeff': 0.26537616052558866, 'diffuse_coeff': 0.19141648342999706, 'specular_coeff': 0.0040886331582056235, 'shininess': 3.0, 'object_color_0': 0.7430347947039876, 'object_color_1': 0.8954858688453009, 'object_color_2': 0.4324159471548735, 'background_color_0': 0.6019241386600762, 'background_color_1': 0.9734920031577594, 'background_color_2': 0.07589817150533162, 'amb_light_color_0': 0.1127199250647698, 'amb_light_color_1': 0.4725461944275944, 'amg_light_color_2': 0.6940832945702751, 'light_intensity_1': 0.9720505681621812, 'light_color_0': 0.9738786585469452, 'light_color_1': 0.9531762153011104, 'light_color_2': 0.9123709844572065, 'light_dir_1_0': -0.8888146283403879, 'light_dir_1_1': -0.8558659907998699, 'light_dir_1_2': -0.1659901063440461, 'light_intensity_2': 0.6335915155591864, 'light_dir_2_1': -0.9450935291643183, 'light_dir_2_2': -0.7083292342778778, 'light_intensity_3': 0.568089039110969, 'light_dir_2_0': -0.6955182364061401}. Best is trial 0 with value: 0.026060357689857483.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_list:  ['01.ply', '02.ply', '03.ply']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-23 15:03:10,405] Trial 3 finished with value: 0.00922837108373642 and parameters: {'camera_distance_factor': 0.1923653589980382, 'camera_angle': 45.0, 'focal_length': 2.4234322491816176, 'resolution': 200, 'ambient_coeff': 0.8574469092117665, 'diffuse_coeff': 0.2665545205016253, 'specular_coeff': 0.6186349859157729, 'shininess': 10.0, 'object_color_0': 0.5535036847034636, 'object_color_1': 0.09143623016410896, 'object_color_2': 0.7123098432188325, 'background_color_0': 0.5700209900515023, 'background_color_1': 0.7314130000147128, 'background_color_2': 0.4675855960476135, 'amb_light_color_0': 0.15078628803283656, 'amb_light_color_1': 0.7451763314273842, 'amg_light_color_2': 0.19540787946034832, 'light_intensity_1': 1.1065930167516442, 'light_color_0': 0.758666755431079, 'light_color_1': 0.9466546848056709, 'light_color_2': 0.7639188267987025, 'light_dir_1_0': -0.7504110357163101, 'light_dir_1_1': -0.8116313685758163, 'light_dir_1_2': -0.4645860637869802, 'light_intensity_2': 0.2929327435898801, 'light_dir_2_1': -0.960706033883435, 'light_dir_2_2': -0.9832766541438782, 'light_intensity_3': 0.6046019991015488, 'light_dir_2_0': -0.23302574156070333}. Best is trial 0 with value: 0.026060357689857483.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_list:  ['01.ply', '02.ply', '03.ply']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-23 15:03:24,101] Trial 4 finished with value: 0.017227903008461 and parameters: {'camera_distance_factor': 0.10349555969024044, 'camera_angle': 45.0, 'focal_length': 2.4172465754318275, 'resolution': 300, 'ambient_coeff': 0.7791207229562114, 'diffuse_coeff': 0.9854934777694561, 'specular_coeff': 0.29372187413055095, 'shininess': 1.0, 'object_color_0': 0.31003130955454716, 'object_color_1': 0.013351681799617099, 'object_color_2': 0.6241130229927422, 'background_color_0': 0.580713284566908, 'background_color_1': 0.6134447538817657, 'background_color_2': 0.5981856423364614, 'amb_light_color_0': 0.7862359340569264, 'amb_light_color_1': 0.6737081540603749, 'amg_light_color_2': 0.5273353990744434, 'light_intensity_1': 1.2356048775250232, 'light_color_0': 0.9487222881920638, 'light_color_1': 0.7791857400300304, 'light_color_2': 0.9198572983289934, 'light_dir_1_0': -0.06638630044248772, 'light_dir_1_1': -0.73194910614424, 'light_dir_1_2': -0.6551599388035617, 'light_intensity_2': 0.8572834592949042, 'light_dir_2_1': -0.26632734389135126, 'light_dir_2_2': -0.6448168415407286, 'light_intensity_3': 0.008320978761910435, 'light_dir_2_0': -0.6406936945876064}. Best is trial 0 with value: 0.026060357689857483.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_list:  ['01.ply', '02.ply', '03.ply']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-23 15:03:38,359] Trial 5 finished with value: 0.01942988485097885 and parameters: {'camera_distance_factor': 0.5888511697763883, 'camera_angle': 15.0, 'focal_length': 7.85397097151267, 'resolution': 300, 'ambient_coeff': 0.34044676779285943, 'diffuse_coeff': 0.6086266425188798, 'specular_coeff': 0.14715544171270478, 'shininess': 0.1, 'object_color_0': 0.1461998758434888, 'object_color_1': 0.11652747649056205, 'object_color_2': 0.8137276157423831, 'background_color_0': 0.5924087086750578, 'background_color_1': 0.29155086439791034, 'background_color_2': 0.7187457945564992, 'amb_light_color_0': 0.048377307706339634, 'amb_light_color_1': 0.19592532157240716, 'amg_light_color_2': 0.49197179673388913, 'light_intensity_1': 1.1548369185498948, 'light_color_0': 0.757853593160798, 'light_color_1': 0.8340899702073874, 'light_color_2': 0.891356548434323, 'light_dir_1_0': -0.27820410947121166, 'light_dir_1_1': -0.9473646680931688, 'light_dir_1_2': -0.6046148340910007, 'light_intensity_2': 0.9369885748386354, 'light_dir_2_1': -0.6300178379684508, 'light_dir_2_2': -0.9882523269712622, 'light_intensity_3': 0.3609160515862661, 'light_dir_2_0': -0.04079482912760246}. Best is trial 0 with value: 0.026060357689857483.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_list:  ['01.ply', '02.ply', '03.ply']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-23 15:03:50,609] Trial 6 finished with value: 0.01691555231809616 and parameters: {'camera_distance_factor': 0.6929193122976949, 'camera_angle': 55.0, 'focal_length': 7.202546277446786, 'resolution': 300, 'ambient_coeff': 0.7263547366747486, 'diffuse_coeff': 0.7854663645666349, 'specular_coeff': 0.27895052929831676, 'shininess': 1.0, 'object_color_0': 0.9299110850892701, 'object_color_1': 0.4325742077284893, 'object_color_2': 0.17395340247849533, 'background_color_0': 0.7531075103336008, 'background_color_1': 0.2639496350535222, 'background_color_2': 0.3551361693590266, 'amb_light_color_0': 0.4642324691002012, 'amb_light_color_1': 0.28900143378825516, 'amg_light_color_2': 0.7053960836791481, 'light_intensity_1': 1.2646758978175758, 'light_color_0': 0.836681160557236, 'light_color_1': 0.709270197666995, 'light_color_2': 0.7438375158500238, 'light_dir_1_0': -0.8965279047662795, 'light_dir_1_1': -0.6456740868374613, 'light_dir_1_2': -0.4999083992648672, 'light_intensity_2': 0.16373744517423627, 'light_dir_2_1': -0.9651031564266139, 'light_dir_2_2': -0.7543437232156326, 'light_intensity_3': 0.3978172239473026, 'light_dir_2_0': -0.28788727317607554}. Best is trial 0 with value: 0.026060357689857483.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_list:  ['01.ply', '02.ply', '03.ply']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-23 15:04:03,331] Trial 7 finished with value: 0.019977524876594543 and parameters: {'camera_distance_factor': 0.7243842907943521, 'camera_angle': 15.0, 'focal_length': 6.167994306458115, 'resolution': 150, 'ambient_coeff': 0.6575278546654741, 'diffuse_coeff': 0.6287109088068125, 'specular_coeff': 0.376739764412907, 'shininess': 3.0, 'object_color_0': 0.6887586320173636, 'object_color_1': 0.9899902914249716, 'object_color_2': 0.7669778960003745, 'background_color_0': 0.8222892075778881, 'background_color_1': 0.9997311701326801, 'background_color_2': 0.37113175812216515, 'amb_light_color_0': 0.8504075596439695, 'amb_light_color_1': 0.9166761654783488, 'amg_light_color_2': 0.5208341401550267, 'light_intensity_1': 1.2543635813942795, 'light_color_0': 0.7491070259000033, 'light_color_1': 0.8633477658771112, 'light_color_2': 0.8546242068738622, 'light_dir_1_0': -0.7130763450471459, 'light_dir_1_1': -0.6472379156017536, 'light_dir_1_2': -0.9465001236404305, 'light_intensity_2': 0.7993166538610748, 'light_dir_2_1': -0.9974938939570204, 'light_dir_2_2': -0.6803464264207894, 'light_intensity_3': 0.692731996732194, 'light_dir_2_0': 0.08881271370081167}. Best is trial 0 with value: 0.026060357689857483.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_list:  ['01.ply', '02.ply', '03.ply']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-23 15:04:17,222] Trial 8 finished with value: 0.01657561957836151 and parameters: {'camera_distance_factor': 0.4044343656072694, 'camera_angle': 45.0, 'focal_length': 4.0370660899713275, 'resolution': 150, 'ambient_coeff': 0.6077164985849127, 'diffuse_coeff': 0.06201789445225647, 'specular_coeff': 0.6629865885635235, 'shininess': 3.0, 'object_color_0': 0.13636314102303415, 'object_color_1': 0.7775495682271227, 'object_color_2': 0.38929077708301185, 'background_color_0': 0.4301848234777642, 'background_color_1': 0.34892478651347003, 'background_color_2': 0.9376812791242909, 'amb_light_color_0': 0.035654711432842046, 'amb_light_color_1': 0.002191032183236352, 'amg_light_color_2': 0.135841977726189, 'light_intensity_1': 1.4279032914880505, 'light_color_0': 0.8214844133102721, 'light_color_1': 0.824180171801002, 'light_color_2': 0.8599711074806171, 'light_dir_1_0': -0.33659307857528675, 'light_dir_1_1': -0.06324836874070261, 'light_dir_1_2': -0.7694446415463743, 'light_intensity_2': 0.8536373746114553, 'light_dir_2_1': -0.3115836252498193, 'light_dir_2_2': -0.3177850593560346, 'light_intensity_3': 0.09789223466754314, 'light_dir_2_0': 0.1351233839551389}. Best is trial 0 with value: 0.026060357689857483.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_list:  ['01.ply', '02.ply', '03.ply']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-23 15:04:29,220] Trial 9 finished with value: 0.02276451140642166 and parameters: {'camera_distance_factor': 0.6753698837472806, 'camera_angle': 30.0, 'focal_length': 8.756486743111676, 'resolution': 300, 'ambient_coeff': 0.06377967326707223, 'diffuse_coeff': 0.5430795717519239, 'specular_coeff': 0.7666935323799584, 'shininess': 0.5, 'object_color_0': 0.15414281757429626, 'object_color_1': 0.04261708579975654, 'object_color_2': 0.40158150794307534, 'background_color_0': 0.2380612637955294, 'background_color_1': 0.6809476463705121, 'background_color_2': 0.9612775422289118, 'amb_light_color_0': 0.3429750517171205, 'amb_light_color_1': 0.3413465259261913, 'amg_light_color_2': 0.11117924229735066, 'light_intensity_1': 1.3043296907222266, 'light_color_0': 0.7296652263396699, 'light_color_1': 0.7798716958394977, 'light_color_2': 0.8535753814128466, 'light_dir_1_0': -0.9672728878682709, 'light_dir_1_1': -0.4845872577606737, 'light_dir_1_2': -0.22091586222694437, 'light_intensity_2': 0.376421171066069, 'light_dir_2_1': -0.10331664616501468, 'light_dir_2_2': -0.4267267964305602, 'light_intensity_3': 0.1385916926494322, 'light_dir_2_0': -0.8165066701217881}. Best is trial 0 with value: 0.026060357689857483.\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'camera_distance_factor': 0.4262159524767436,\n 'camera_angle': 45.0,\n 'focal_length': 2.8588476021095603,\n 'resolution': 224,\n 'ambient_coeff': 0.37326565625223007,\n 'diffuse_coeff': 0.38317351714198056,\n 'specular_coeff': 0.9486262710673882,\n 'shininess': 0.1,\n 'object_color_0': 0.42585222969783176,\n 'object_color_1': 0.9343647618107314,\n 'object_color_2': 0.40583866285955716,\n 'background_color_0': 0.6825755786499671,\n 'background_color_1': 0.12299008231283204,\n 'background_color_2': 0.5865344696862822,\n 'amb_light_color_0': 0.40382610739760083,\n 'amb_light_color_1': 0.5327496805841134,\n 'amg_light_color_2': 0.5502157681648859,\n 'light_intensity_1': 0.6163791478537356,\n 'light_color_0': 0.7305198710849781,\n 'light_color_1': 0.8587854743569627,\n 'light_color_2': 0.7135445661080153,\n 'light_dir_1_0': -0.3947715529507573,\n 'light_dir_1_1': -0.8557223172563867,\n 'light_dir_1_2': -0.9508653955941185,\n 'light_intensity_2': 0.5728753113510167,\n 'light_dir_2_1': -0.7932316202321615,\n 'light_dir_2_2': -0.7702666673424424,\n 'light_intensity_3': 0.3622748688964944,\n 'light_dir_2_0': -0.7079762311103447}"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study = optuna.create_study(storage=\"sqlite:///optuna_study.db\", study_name=\"test_2\", direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "study.best_params"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T14:04:29.326391Z",
     "start_time": "2023-11-23T14:02:09.925991Z"
    }
   },
   "id": "8660eafbf534fc18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a30d2a40785edca"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
