{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-28T10:01:58.469992Z",
     "start_time": "2023-11-28T10:01:42.948008Z"
    }
   },
   "outputs": [],
   "source": [
    "import clip\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import trimesh\n",
    "import time\n",
    "import optuna\n",
    "from torchvision.transforms import Compose, Normalize, Resize, CenterCrop, InterpolationMode\n",
    "import multiprocess\n",
    "from functools import partial\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cpus:  4\n"
     ]
    }
   ],
   "source": [
    "print('Number of cpus: ', multiprocess.cpu_count())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T10:02:01.841242Z",
     "start_time": "2023-11-28T10:02:01.786121Z"
    }
   },
   "id": "38f2c42d0c43ee3e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Rendering Setup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98e57945907ff5f2"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# --- Hyperparameter Setup ---\n",
    "\n",
    "hparams = {\n",
    "    # Camera\n",
    "    \"camera_distance_factor\": 3.11/6.,\n",
    "    \"camera_angle\": 45.,\n",
    "    \"focal_length\": 6.,\n",
    "    # Image\n",
    "    \"resolution\": 224,\n",
    "    # Phong\n",
    "    \"ambient_coeff\": 0.5,\n",
    "    \"diffuse_coeff\": 0.7,\n",
    "    \"specular_coeff\": 0.0,\n",
    "    \"shininess\": 3.0,\n",
    "    # Colors\n",
    "    \"object_color\": torch.tensor([0.61, 0.61, 0.61]),\n",
    "    \"background_color\": torch.tensor([0.46, 0, 0]),\n",
    "    # Light\n",
    "    \"amb_light_color\": torch.tensor([0.15, 0, 0]),\n",
    "    # light 1\n",
    "    \"light_intensity_1\": 1., \n",
    "    \"light_color\": torch.tensor([1.0, 1.0, 1.0]), \n",
    "    \"light_dir_1\": torch.tensor([-1., -1., -1.]), \n",
    "    # light 2\n",
    "    \"light_intensity_2\": 1.,\n",
    "    \"light_dir_2\": torch.tensor([0., 0., -1.]), \n",
    "    # light 3\n",
    "    \"light_intensity_3\": 0., \n",
    "    \"light_dir_3\": torch.tensor([0., -1., 0.])\n",
    "}\n",
    "\n",
    "# --- Rendering Function ---\n",
    "\n",
    "def phong_model(sdf, points, camera_position, phong_params, light_params, mesh_path, index_tri=None):\n",
    "    \n",
    "    # Option 1: Use SDF\n",
    "    #normals = estimate_normals(sdf, points)\n",
    "    # Option 2: Use Mesh\n",
    "    normals = mesh_normals(mesh_path, index_tri)\n",
    "    view_dirs = points - camera_position \n",
    "    light_dir_1 = light_params[\"light_dir_1\"].repeat(points.shape[0], 1) \n",
    "    light_dir_2 = light_params[\"light_dir_2\"].repeat(points.shape[0], 1)\n",
    "    light_dir_3 = light_params[\"light_dir_3\"].repeat(points.shape[0], 1)\n",
    "    \n",
    "    # Normalize all vectors\n",
    "    normals = (normals.T / torch.norm(normals, dim=-1)).T\n",
    "    light_dir_norm_1 = (light_dir_1.T / torch.norm(light_dir_1, dim=-1)).T\n",
    "    light_dir_norm_2 = (light_dir_2.T / torch.norm(light_dir_2, dim=-1)).T\n",
    "    light_dir_norm_3 = (light_dir_3.T / torch.norm(light_dir_3, dim=-1)).T\n",
    "    view_dir_norm = (view_dirs.T / torch.norm(view_dirs, dim=-1)).T\n",
    "    \n",
    "    # Ambient\n",
    "    ambient = phong_params[\"ambient_coeff\"] * light_params[\"amb_light_color\"] \n",
    "    ambient_refl = ambient.repeat(points.shape[0], 1)\n",
    "    \n",
    "    # Area light 1\n",
    "    diffuse_1 = phong_params[\"diffuse_coeff\"] * torch.clamp(torch.sum(-light_dir_norm_1 * normals, dim=-1), min=0.0) * light_params[\"light_intensity_1\"] # [N]\n",
    "    diffuse_refl_1 = torch.matmul(diffuse_1.unsqueeze(1), light_params[\"light_color_1\"].unsqueeze(0)) # [N, 3]\n",
    "    reflect_dir_1 = light_dir_norm_1 + (2 * normals.T * torch.clamp(torch.sum(-light_dir_norm_1 * normals, dim=-1), min=0.0)).T\n",
    "    specular_1 = phong_params[\"specular_coeff\"] * torch.pow(torch.clamp(torch.sum(reflect_dir_1 * -view_dir_norm, dim=-1), min=0.0), phong_params[\"shininess\"]) * light_params[\"light_intensity_1\"] # [N]\n",
    "    specular_refl_1 = torch.matmul(specular_1.unsqueeze(1), light_params[\"light_color_1\"].unsqueeze(0)) # [N, 3]\n",
    "    \n",
    "    # Area light 2\n",
    "    diffuse_2 = phong_params[\"diffuse_coeff\"] * torch.clamp(torch.sum(-light_dir_norm_2 * normals, dim=-1), min=0.0) * light_params[\"light_intensity_2\"] # [N]\n",
    "    diffuse_refl_2 = torch.matmul(diffuse_2.unsqueeze(1), light_params[\"light_color_2\"].unsqueeze(0)) # [N, 3]\n",
    "    reflect_dir_2 = light_dir_norm_2 + (2 * normals.T * torch.clamp(torch.sum(-light_dir_norm_2 * normals, dim=-1), min=0.0)).T\n",
    "    specular_2 = phong_params[\"specular_coeff\"] * torch.pow(torch.clamp(torch.sum(reflect_dir_2 * -view_dir_norm, dim=-1), min=0.0), phong_params[\"shininess\"]) * light_params[\"light_intensity_2\"] # [N]\n",
    "    specular_refl_2 = torch.matmul(specular_2.unsqueeze(1), light_params[\"light_color_2\"].unsqueeze(0)) # [N, 3]\n",
    "    \n",
    "    # Area light 3\n",
    "    diffuse_3 = phong_params[\"diffuse_coeff\"] * torch.clamp(torch.sum(-light_dir_norm_3 * normals, dim=-1), min=0.0) * light_params[\"light_intensity_3\"] # [N]\n",
    "    diffuse_refl_3 = torch.matmul(diffuse_3.unsqueeze(1), light_params[\"light_color_3\"].unsqueeze(0)) # [N, 3]\n",
    "    reflect_dir_3 = light_dir_norm_3 + (2 * normals.T * torch.clamp(torch.sum(-light_dir_norm_3 * normals, dim=-1), min=0.0)).T\n",
    "    specular_3 = phong_params[\"specular_coeff\"] * torch.pow(torch.clamp(torch.sum(reflect_dir_3 * -view_dir_norm, dim=-1), min=0.0), phong_params[\"shininess\"]) * light_params[\"light_intensity_3\"] # [N]\n",
    "    specular_refl_3 = torch.matmul(specular_3.unsqueeze(1), light_params[\"light_color_3\"].unsqueeze(0)) # [N, 3]\n",
    "    \n",
    "    # point light\n",
    "    #diffuse_1 = diffuse_coeff * torch.clamp(torch.sum(-light_dir_norm_1 * normals, dim=-1), min=0.0) * light_intensity_1 # [N]\n",
    "    #diffuse_refl_1 = torch.matmul(diffuse_1.unsqueeze(1), light_color.unsqueeze(0)) # [N, 3]\n",
    "    #reflect_dir_1 = light_dir_norm_1 + (2 * normals.T * torch.clamp(torch.sum(-light_dir_norm_1 * normals, dim=-1), min=0.0)).T\n",
    "    #specular_1 = specular_coeff * torch.pow(torch.clamp(torch.sum(reflect_dir_1 * -view_dir_norm, dim=-1), min=0.0), shininess) * light_intensity_1 # [N]\n",
    "    #specular_refl_1 = torch.matmul(specular_1.unsqueeze(1), light_color.unsqueeze(0)) # [N, 3]\n",
    "\n",
    "    return ambient_refl + diffuse_refl_1 + specular_refl_1 + diffuse_refl_2 + specular_refl_2 + diffuse_refl_3 + specular_refl_3\n",
    "\n",
    "def estimate_normals(sdf, points, epsilon=1e-3):\n",
    "    sdf_inputs = torch.concat([points,\n",
    "                              points + torch.tensor([epsilon, 0, 0]),\n",
    "                              points + torch.tensor([0, epsilon, 0]),\n",
    "                              points + torch.tensor([0, 0, epsilon])])\n",
    "\n",
    "    sdf_values = sdf(sdf_inputs).reshape(4, -1)\n",
    "\n",
    "    # Calculate the gradient using finite differences\n",
    "    gradient = sdf_values[1:] - sdf_values[0]\n",
    "\n",
    "    # Normalize the gradient to obtain the estimated normal\n",
    "    normal = gradient / torch.norm(gradient, p=2, dim=0)\n",
    "\n",
    "    return normal.T\n",
    "\n",
    "def sphere_trace(sdf, camera_position, norm_directions, max_length):\n",
    "    N = norm_directions.shape[0]\n",
    "    positions = camera_position.unsqueeze(dim=0).repeat(N, 1) # [N, 3]\n",
    "    total_distances = torch.zeros(N)\n",
    "    last_distances = torch.ones(N)\n",
    "\n",
    "    for _ in range(20):\n",
    "        #mask = torch.logical_and(total_distances < max_length, last_distances > 1e-3)\n",
    "        not_reached_max_distance = total_distances < max_length\n",
    "        not_hit_target = torch.abs(last_distances) > 1e-3\n",
    "        mask = torch.logical_and(not_reached_max_distance, not_hit_target)\n",
    "        if torch.all(torch.logical_not(mask)):\n",
    "            break\n",
    "        distances = sdf(positions[mask])\n",
    "        steps = (norm_directions[mask].T * distances).T\n",
    "        positions[mask] += steps\n",
    "        total_distances[mask] += distances\n",
    "        last_distances[mask] = distances\n",
    "\n",
    "    #positions[total_distances > max_length] *= torch.nan\n",
    "    return positions, total_distances < max_length\n",
    "\n",
    "def mesh_trace(mesh_path, ray_starts, ray_directions):\n",
    "    mesh = trimesh.load_mesh(mesh_path)\n",
    "    \n",
    "    ray_origins = ray_starts.repeat(ray_directions.shape[0], 1)\n",
    "    \n",
    "    intersections, index_ray, index_tri = mesh.ray.intersects_location(ray_origins, ray_directions, multiple_hits=False) \n",
    "    \n",
    "    mask = torch.zeros(ray_directions.shape[0], dtype=torch.bool)\n",
    "    mask[index_ray] = True\n",
    "    \n",
    "    points = torch.as_tensor(intersections, dtype=torch.float32)\n",
    "\n",
    "    return points, mask, index_tri\n",
    "\n",
    "\n",
    "def mesh_normals(mesh_path, index_tri):\n",
    "    mesh = trimesh.load_mesh(mesh_path)\n",
    "    all_normals = mesh.face_normals\n",
    "    normals = all_normals[index_tri]\n",
    "    normals_torch = torch.from_numpy(normals).to(dtype=torch.float32)\n",
    "    \n",
    "    return normals_torch\n",
    "\n",
    "def render(model, lat_rep, camera_params, phong_params, light_params, mesh_path=None):\n",
    "    \n",
    "    def sdf(positions):\n",
    "        nphm_input = torch.reshape(positions, (1, -1, 3))\n",
    "        distance, _ = model(nphm_input, torch.reshape(lat_rep, (1, 1, -1)), None)\n",
    "        return distance.squeeze()\n",
    "\n",
    "    pu = camera_params[\"resolution_x\"]\n",
    "    pv = camera_params[\"resolution_y\"]\n",
    "    image = phong_params[\"background_color\"].repeat(pu * pv, 1)\n",
    "    \n",
    "    angle_radians = torch.deg2rad_(torch.tensor(camera_params[\"camera_angle\"]))\n",
    "    camera = torch.tensor([torch.sin(angle_radians), 0, torch.cos(angle_radians)])\n",
    "    camera_position = camera * (camera_params[\"camera_distance\"] + camera_params[\"focal_length\"]) / camera.norm()\n",
    "    \n",
    "    # Normalize the xy value of the current pixel [-0.5, 0.5]\n",
    "    u_norms = ((torch.arange(pu) + 0.5) / pu - 0.5) * pu/pv\n",
    "    v_norms = 0.5 - (torch.arange(pv) + 0.5) / pv\n",
    "\n",
    "    # Calculate the ray directions for all pixels\n",
    "    directions_unn = torch.cat(torch.meshgrid(u_norms, v_norms, torch.tensor(-camera_params[\"focal_length\"]), indexing='ij'), dim=-1) \n",
    "    directions_unn = directions_unn.reshape((pu*pv, 3)) # [pu, pv, 3] --> [pu*pv, 3] (u1, v1, f)(u1, v2, f)...(u2, v1, f)...\n",
    "\n",
    "    # rotate about y-axis\n",
    "    rotation_matrix = torch.tensor([[torch.cos(angle_radians), 0, torch.sin(angle_radians)],\n",
    "                                    [0, 1, 0],\n",
    "                                    [-torch.sin(angle_radians), 0, torch.cos(angle_radians)]])\n",
    "    rotated_directions = torch.matmul(directions_unn, rotation_matrix.T)\n",
    "    \n",
    "    transposed_directions = rotated_directions.T #transpose is necessary for normalization\n",
    "    directions = (transposed_directions / transposed_directions.norm(dim=0)).T # [pu*pv, 3]\n",
    "\n",
    "    # Option 1: Use SDF\n",
    "    #hit_positions, hit_mask = sphere_trace(sdf, camera_position, directions, camera_params['max_ray_length'])\n",
    "    # Option 2: Use Mesh\n",
    "    intersections, hit_mask, index_tri = mesh_trace(mesh_path, camera_position, directions) \n",
    "    \n",
    "    # Option 1: Use SDF\n",
    "    #reflections = phong_model(sdf, hit_positions[hit_mask], camera_position, phong_params, light_params, mesh_path)\n",
    "    # Option 2: Use Mesh\n",
    "    reflections = phong_model(sdf, intersections, camera_position, phong_params, light_params, mesh_path, index_tri) # mesh alternative\n",
    "\n",
    "    # Assign a color for objects\n",
    "    image[hit_mask] = torch.mul(reflections, phong_params[\"object_color\"].repeat(reflections.shape[0], 1))\n",
    "    image = torch.clamp(image, max=1.0)\n",
    "    image = image.reshape(pu, pv, 3).transpose(0, 1)\n",
    "\n",
    "\n",
    "    return image"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T13:42:04.896399Z",
     "start_time": "2023-11-28T13:42:04.801451Z"
    }
   },
   "id": "fcc5fac445bb6f43"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare Score Calculation Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62dda701d3a74cb2"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# --- Get Data Annotations ---\n",
    "\n",
    "path_to_head_annotations = '/Users/katharinaschmid/Text2Head/rendering_data/head_annotations.json'\n",
    "with open(path_to_head_annotations) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# --- Prepare Indices ---\n",
    "\n",
    "# Identity: Gender\n",
    "male_indices = [index for index, head in enumerate(data['heads']) if head['gender'] == 'man']\n",
    "female_indices = [index for index, head in enumerate(data['heads']) if head['gender'] == 'woman']\n",
    "\n",
    "# Identity: Age\n",
    "young_indices = [index for index, head in enumerate(data['heads']) if head['age'] == 'young']\n",
    "old_indices = [index for index, head in enumerate(data['heads']) if head['age'] == 'old']\n",
    "\n",
    "# Identity: Ethnicity\n",
    "caucasian_indices = [index for index, head in enumerate(data['heads']) if head['ethnicity'] == 'Caucasian'] \n",
    "asian_indices = [index for index, head in enumerate(data['heads']) if head['ethnicity'] == 'Asian'] \n",
    "\n",
    "# Identity: Hairstyle\n",
    "hat_indices = [index for index, head in enumerate(data['heads']) if head['hairstyle'] == 'hat']\n",
    "ponytail_indices = [index for index, head in enumerate(data['heads']) if head['hairstyle'] == 'ponytail']\n",
    "straight_hair_indices = [index for index, head in enumerate(data['heads']) if head['hairstyle'] == 'straight hair']\n",
    "curly_hair_indices = [index for index, head in enumerate(data['heads']) if head['hairstyle'] == 'curly hair']\n",
    "\n",
    "# Identity: Beard\n",
    "beard_indices = [index for index, head in enumerate(data['heads']) if head['beard'] == 'yes']\n",
    "no_beard_indices = [index for index, head in enumerate(data['heads']) if head['beard'] == 'no']\n",
    "\n",
    "# Expression: Facial Expression\n",
    "open_mouth_indices = [index for index, head in enumerate(data['heads']) if head['facial_expression'] == 'open mouth']\n",
    "smiling_indices = [index for index, head in enumerate(data['heads']) if head['facial_expression'] == 'smiling']\n",
    "closed_eyes_indices = [index for index, head in enumerate(data['heads']) if head['facial_expression'] == 'closed eyes']\n",
    "raised_brows_indices = [index for index, head in enumerate(data['heads']) if head['facial_expression'] == 'raised brows']\n",
    "puffed_cheeks_indices = [index for index, head in enumerate(data['heads']) if head['facial_expression'] == 'puffed cheeks']\n",
    "\n",
    "# Expression: Emotion \n",
    "sad_indices = [index for index, head in enumerate(data['heads']) if head['emotion'] == 'sad']\n",
    "not_sad_indices = [index for index, head in enumerate(data['heads']) if head['emotion'] != 'sad']\n",
    "happy_indices = [index for index, head in enumerate(data['heads']) if head['emotion'] == 'happy']\n",
    "not_happy_indices = [index for index, head in enumerate(data['heads']) if head['emotion'] != 'happy']\n",
    "\n",
    "# --- Precompute CLIP Text Embeddings ---\n",
    "captions = []\n",
    "captions.append('A man') # 0\n",
    "captions.append('A woman') # 1\n",
    "captions.append('A young person') # 2\n",
    "captions.append('An old person') # 3\n",
    "captions.append('A European person') # 4\n",
    "captions.append('An Asian person') # 5\n",
    "captions.append('An African person') # 6\n",
    "captions.append('A person wearing a hat') # 7\n",
    "captions.append('A person with a ponytail') # 8\n",
    "captions.append('A person with straight hair') # 9\n",
    "captions.append('A person with curly hair') # 10\n",
    "captions.append('A person with beard') # 11\n",
    "captions.append('A person with open mouth') # 12\n",
    "captions.append('A smiling person') # 13\n",
    "captions.append('A person with closed eyes') # 14\n",
    "captions.append('A frowning person') # 15\n",
    "captions.append('A person with puffed cheeks') # 16\n",
    "captions.append('A sad person') # 17\n",
    "captions.append('A happy person') # 18\n",
    "\n",
    "preprocessed_text = clip.tokenize(captions).to(device) # [num_captions, 77]\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(preprocessed_text) # [num_captions, 512]\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True) # [num_captions, 512]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T13:42:11.847157Z",
     "start_time": "2023-11-28T13:42:07.892175Z"
    }
   },
   "id": "ccd21bc560b0eefe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameter Search for Renderer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff2474134e2af3d1"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# --- CLIP Preprocessing --- # \n",
    "clip_tensor_preprocessor = Compose([\n",
    "    Resize(224, interpolation=InterpolationMode.BICUBIC, antialias=None),\n",
    "    CenterCrop(224),\n",
    "    Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "\n",
    "\n",
    "# --- Penalty Function to penalize negative Signal Scores ---\n",
    "def penalty(vector, k):\n",
    "    vector[vector < 0] *= k\n",
    "    return vector\n",
    "\n",
    "# --- Image Embedding Function for Parallel Processing ---\n",
    "\n",
    "def get_image_embeddings(mesh_path, camera_params, phong_params, light_params):\n",
    "    sdf = None\n",
    "    lat_rep = None\n",
    "    rendered_image = render(sdf, lat_rep, camera_params, phong_params, light_params, mesh_path)\n",
    "    image_c_first = rendered_image.permute(2, 0, 1)\n",
    "    image_preprocessed = clip_tensor_preprocessor(image_c_first).unsqueeze(0)\n",
    "    image_features = model.encode_image(image_preprocessed) # [1, 512]\n",
    "    \n",
    "    return image_features\n",
    "\n",
    "def analyze_rendering(hparams):\n",
    "    \n",
    "    # --- Get Hyperparameters ---\n",
    "    camera_params = {\n",
    "        \"camera_distance\": hparams['camera_distance_factor'] * hparams['focal_length'], # ensures appropriate head size\n",
    "        \"camera_angle\": hparams['camera_angle'],\n",
    "        \"focal_length\": hparams['focal_length'],\n",
    "        \"max_ray_length\": (hparams['camera_distance_factor'] + 1) * hparams['focal_length'] + 1.5,\n",
    "        # Image\n",
    "        \"resolution_y\": hparams['resolution'],\n",
    "        \"resolution_x\": hparams['resolution']\n",
    "    }\n",
    "    phong_params = {\n",
    "        \"ambient_coeff\": hparams['ambient_coeff'],\n",
    "        \"diffuse_coeff\": hparams['diffuse_coeff'],\n",
    "        \"specular_coeff\": hparams['specular_coeff'],\n",
    "        \"shininess\": hparams['shininess'],\n",
    "        # Colors\n",
    "        \"object_color\": hparams['object_color'],\n",
    "        \"background_color\": hparams['background_color']\n",
    "    }\n",
    "    \n",
    "    light_params = {\n",
    "        \"amb_light_color\": hparams['amb_light_color'],\n",
    "        # light 1\n",
    "        \"light_intensity_1\": hparams['light_intensity_1'],\n",
    "        \"light_color_1\": hparams['light_color'],\n",
    "        \"light_dir_1\": hparams['light_dir_1'],\n",
    "        # light 2\n",
    "        \"light_intensity_2\": hparams['light_intensity_2'],\n",
    "        \"light_color_2\": hparams['light_color'],\n",
    "        \"light_dir_2\": hparams['light_dir_2'],\n",
    "        # light 3\n",
    "        \"light_intensity_3\": hparams['light_intensity_3'],\n",
    "        \"light_color_3\": hparams['light_color'],\n",
    "        \"light_dir_3\": hparams['light_dir_3'],\n",
    "    }\n",
    "    \n",
    "    # --- Render and Embed Images ---\n",
    "    folder_path = '/Users/katharinaschmid/Text2Head/rendering_data/annotated_dataset/'\n",
    "    file_list = sorted(os.listdir(folder_path))\n",
    "    file_list = [file for file in file_list if not (file.startswith('.DS_Store') or file.endswith('.png'))]\n",
    "    \n",
    "    mesh_path_list = []\n",
    "    for mesh in file_list:\n",
    "        mesh_path_list.append(folder_path + mesh)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        pool = multiprocess.Pool(processes=None)\n",
    "        simplified_func = partial(get_image_embeddings, camera_params=camera_params, phong_params=phong_params, light_params=light_params) # creates a simplified version of the function 'get_image_embeddings' where certain arguments are fixed to specific values\n",
    "        image_embeddings_list = pool.map(simplified_func, mesh_path_list)\n",
    "\n",
    "        image_embeddings = torch.cat(image_embeddings_list, dim=0)\n",
    "        image_embeddings /= image_embeddings.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        # --- Get CLIP Similarity and Scores ---\n",
    "        cosine_similarity = torch.matmul(text_features, image_embeddings.T) #[num_captions*tests, num_images]\n",
    "        \n",
    "        # Identity Scores\n",
    "        i_man_score = torch.mean(cosine_similarity[0, male_indices], dim=-1) - torch.mean(cosine_similarity[0, female_indices], dim=-1)\n",
    "        i_woman_score = torch.mean(cosine_similarity[1, female_indices], dim=-1) - torch.mean(cosine_similarity[1, male_indices], dim=-1)\n",
    "        \n",
    "        i_young_score = torch.mean(cosine_similarity[2, young_indices], dim=-1) - torch.mean(cosine_similarity[2, old_indices], dim=-1)\n",
    "        i_old_score = torch.mean(cosine_similarity[3, old_indices], dim=-1) - torch.mean(cosine_similarity[3, young_indices], dim=-1)\n",
    "        i_caucasian_score = torch.mean(cosine_similarity[4, caucasian_indices], dim=-1) - torch.mean(cosine_similarity[4, (asian_indices)], dim=-1)\n",
    "        i_asian_score = torch.mean(cosine_similarity[5, asian_indices], dim=-1) - torch.mean(cosine_similarity[5, (caucasian_indices)], dim=-1)\n",
    "        # 6 skip african\n",
    "        i_hat_score = torch.mean(cosine_similarity[7, hat_indices], dim=-1) - torch.mean(cosine_similarity[7, (ponytail_indices + straight_hair_indices + curly_hair_indices)], dim=-1)\n",
    "        i_ponytail_score = torch.mean(cosine_similarity[8, ponytail_indices], dim=-1) - torch.mean(cosine_similarity[8, (hat_indices + straight_hair_indices + curly_hair_indices)], dim=-1)\n",
    "        i_straight_hair_score = torch.mean(cosine_similarity[9, straight_hair_indices], dim=-1) - torch.mean(cosine_similarity[9, curly_hair_indices], dim=-1) # ONLY curly hair as opposite!\n",
    "        i_curly_hair_score = torch.mean(cosine_similarity[10, curly_hair_indices], dim=-1) - torch.mean(cosine_similarity[10, straight_hair_indices], dim=-1) # ONLY straight hair as opposite!\n",
    "        i_beard_score = torch.mean(cosine_similarity[11, beard_indices], dim=-1) - torch.mean(cosine_similarity[11, no_beard_indices], dim=-1)\n",
    "        \n",
    "        # Expression Scores\n",
    "        e_open_mouth_score = torch.mean(cosine_similarity[12, open_mouth_indices], dim=-1) - torch.mean(cosine_similarity[12, (closed_eyes_indices + raised_brows_indices + puffed_cheeks_indices)], dim=-1) # no smiling as opposite b/c mouth can be open when grinning\n",
    "        e_smiling_score = torch.mean(cosine_similarity[13, smiling_indices], dim=-1) - torch.mean(cosine_similarity[13, (open_mouth_indices + closed_eyes_indices + raised_brows_indices + puffed_cheeks_indices)], dim=-1)\n",
    "        e_closed_eyes_score = torch.mean(cosine_similarity[14, closed_eyes_indices], dim=-1) - torch.mean(cosine_similarity[14, (open_mouth_indices + smiling_indices + raised_brows_indices + puffed_cheeks_indices)], dim=-1)\n",
    "        e_raised_brows_score = torch.mean(cosine_similarity[15, raised_brows_indices], dim=-1) - torch.mean(cosine_similarity[15, (open_mouth_indices + smiling_indices + closed_eyes_indices + puffed_cheeks_indices)], dim=-1)\n",
    "        e_puffed_cheeks_score = torch.mean(cosine_similarity[16, puffed_cheeks_indices], dim=-1) - torch.mean(cosine_similarity[16, (open_mouth_indices + smiling_indices + closed_eyes_indices + raised_brows_indices)], dim=-1)\n",
    "        e_sad_score = torch.mean(cosine_similarity[17, sad_indices], dim=-1) - torch.mean(cosine_similarity[17, not_sad_indices], dim=-1)\n",
    "        e_happy_score = torch.mean(cosine_similarity[18, happy_indices], dim=-1) - torch.mean(cosine_similarity[18, not_happy_indices], dim=-1)\n",
    "        \n",
    "        # Stack Scores, add penalty, compute average\n",
    "        i_scores = torch.stack((i_man_score, i_woman_score, i_young_score, i_old_score, i_caucasian_score, i_asian_score, i_hat_score, i_ponytail_score, i_straight_hair_score, i_curly_hair_score, i_beard_score), dim=0)\n",
    "        e_scores = torch.stack((e_open_mouth_score, e_smiling_score, e_closed_eyes_score, e_raised_brows_score, e_puffed_cheeks_score, e_sad_score, e_happy_score), dim=0)\n",
    "        print('i_scores: ', i_scores, 'e_scores: ', e_scores)\n",
    "        scores = torch.cat((i_scores, e_scores), dim=0)\n",
    "        scores_penalized = penalty(scores, 10)\n",
    "        scores_avg = torch.mean(scores_penalized, dim=0)\n",
    "        print('scores penalized: ', scores_penalized)\n",
    "        print('scores_avg: ', scores_avg)\n",
    "        \n",
    "    \n",
    "    return scores_avg\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters to tune and their search spaces\n",
    "    search_space = {\n",
    "        # Camera\n",
    "        \"camera_distance_factor\": trial.suggest_float('camera_distance_factor', 0.1, 0.8), \n",
    "        \"camera_angle\": trial.suggest_categorical('camera_angle', [0., 15., 30., 45., 55.]), \n",
    "        \"focal_length\": trial.suggest_float('focal_length', 2.0, 9.0),\n",
    "        # Image\n",
    "        \"resolution\": trial.suggest_categorical('resolution', [150, 200, 224, 300]), \n",
    "        # Phong\n",
    "        \"ambient_coeff\": trial.suggest_float('ambient_coeff', 0.0, 1.0), \n",
    "        \"diffuse_coeff\": trial.suggest_float('diffuse_coeff', 0.0, 1.0), \n",
    "        \"specular_coeff\": trial.suggest_float('specular_coeff', 0.0, 1.0), \n",
    "        \"shininess\": trial.suggest_categorical('shininess', [0.1, 0.5, 1.0, 3.0, 10.]),\n",
    "        # Colors\n",
    "        \"object_color\": torch.tensor([\n",
    "            trial.suggest_float('object_color_0', 0.0, 1.0),\n",
    "            trial.suggest_float('object_color_1', 0.0, 1.0),\n",
    "            trial.suggest_float('object_color_2', 0.0, 1.0)\n",
    "        ]),\n",
    "        \"background_color\": torch.tensor([\n",
    "            trial.suggest_float('background_color_0', 0.0, 1.0),\n",
    "            trial.suggest_float('background_color_1', 0.0, 1.0),\n",
    "            trial.suggest_float('background_color_2', 0.0, 1.0)\n",
    "        ]),\n",
    "        # Light\n",
    "        \"amb_light_color\": torch.tensor([\n",
    "            trial.suggest_float('amb_light_color_0', 0.0, 1.0),\n",
    "            trial.suggest_float('amb_light_color_1', 0.0, 1.0),\n",
    "            trial.suggest_float('amg_light_color_2', 0.0, 1.0)\n",
    "        ]),\n",
    "        # light 1\n",
    "        \"light_intensity_1\": trial.suggest_float('light_intensity_1', 0.5, 1.5), \n",
    "        \"light_color\": torch.tensor([\n",
    "            trial.suggest_float('light_color_0', 0.7, 1.0),\n",
    "            trial.suggest_float('light_color_1', 0.7, 1.0),\n",
    "            trial.suggest_float('light_color_2', 0.7, 1.0)\n",
    "        ]),\n",
    "        \"light_dir_1\": torch.tensor([\n",
    "            trial.suggest_float('light_dir_1_0', -1.0, 0.0),\n",
    "            trial.suggest_float('light_dir_1_1', -1.0, 0.0),\n",
    "            trial.suggest_float('light_dir_1_2', -1.0, 0.0)\n",
    "        ]),\n",
    "        # light 2\n",
    "        \"light_intensity_2\": trial.suggest_float('light_intensity_2', 0.0, 1.), \n",
    "        \"light_dir_2\": torch.tensor([\n",
    "            0.0,\n",
    "            trial.suggest_float('light_dir_2_1', -1.0, 0.0),\n",
    "            trial.suggest_float('light_dir_2_2', -1.0, 0.0)\n",
    "        ]),\n",
    "        # light 3\n",
    "        \"light_intensity_3\": trial.suggest_float('light_intensity_3', 0.0, 0.7), \n",
    "        \"light_dir_3\": torch.tensor([\n",
    "            trial.suggest_float('light_dir_2_0', -1.0, 0.5),\n",
    "            trial.suggest_float('light_dir_2_1', -1.0, 0.0),\n",
    "            0.0\n",
    "        ]),\n",
    "    }\n",
    "    \n",
    "    hparams.update(search_space)\n",
    "    \n",
    "    return analyze_rendering(hparams)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T13:42:14.908786Z",
     "start_time": "2023-11-28T13:42:14.853307Z"
    }
   },
   "id": "18fe2d0b47dc321"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-28 15:21:47,259] Using an existing study with name 'render_params' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_scores:  tensor([ 0.0234,  0.0244,  0.0128,  0.0008, -0.0050,  0.0359,  0.0359,  0.0163,\n",
      "         0.0043,  0.0132,  0.0160]) e_scores:  tensor([ 2.6528e-02,  3.4482e-02, -5.4926e-05, -1.4553e-03,  1.9386e-02,\n",
      "         8.2773e-03,  2.9249e-02])\n",
      "scores penalized:  tensor([ 0.0234,  0.0244,  0.0128,  0.0008, -0.0497,  0.0359,  0.0359,  0.0163,\n",
      "         0.0043,  0.0132,  0.0160,  0.0265,  0.0345, -0.0005, -0.0146,  0.0194,\n",
      "         0.0083,  0.0292])\n",
      "scores_avg:  tensor(0.0131)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-28 15:32:53,333] Trial 5 finished with value: 0.01311392243951559 and parameters: {'camera_distance_factor': 0.3362851638544545, 'camera_angle': 15.0, 'focal_length': 8.957066202829555, 'resolution': 300, 'ambient_coeff': 0.10178847196525642, 'diffuse_coeff': 0.6389529726630414, 'specular_coeff': 0.22932161385702832, 'shininess': 0.1, 'object_color_0': 0.8597033247386369, 'object_color_1': 0.8770838770784088, 'object_color_2': 0.998698108657361, 'background_color_0': 0.7591506562067979, 'background_color_1': 0.5474907471625484, 'background_color_2': 0.08182925315967948, 'amb_light_color_0': 0.717044115521322, 'amb_light_color_1': 0.370564162012496, 'amg_light_color_2': 0.12592380798272074, 'light_intensity_1': 1.4090595434898443, 'light_color_0': 0.9336982414012925, 'light_color_1': 0.7966374958239457, 'light_color_2': 0.837422865550463, 'light_dir_1_0': -0.6127075850956579, 'light_dir_1_1': -0.719539594951635, 'light_dir_1_2': -0.04058240257263379, 'light_intensity_2': 0.4163897262240758, 'light_dir_2_1': -0.24827224984322172, 'light_dir_2_2': -0.7171306416576981, 'light_intensity_3': 0.6031854191172381, 'light_dir_2_0': -0.06310475802881454}. Best is trial 3 with value: 0.016015496104955673.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_scores:  tensor([ 0.0214,  0.0152,  0.0119,  0.0076, -0.0021,  0.0153,  0.0271,  0.0066,\n",
      "        -0.0001,  0.0076,  0.0132]) e_scores:  tensor([0.0257, 0.0238, 0.0026, 0.0046, 0.0115, 0.0054, 0.0192])\n",
      "scores penalized:  tensor([ 0.0214,  0.0152,  0.0119,  0.0076, -0.0206,  0.0153,  0.0271,  0.0066,\n",
      "        -0.0011,  0.0076,  0.0132,  0.0257,  0.0238,  0.0026,  0.0046,  0.0115,\n",
      "         0.0054,  0.0192])\n",
      "scores_avg:  tensor(0.0109)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-28 15:41:34,515] Trial 6 finished with value: 0.010938220657408237 and parameters: {'camera_distance_factor': 0.7026381956661675, 'camera_angle': 30.0, 'focal_length': 2.3551088050581006, 'resolution': 150, 'ambient_coeff': 0.5113219259127193, 'diffuse_coeff': 0.09546043411302929, 'specular_coeff': 0.10875083615744996, 'shininess': 3.0, 'object_color_0': 0.7369922026292552, 'object_color_1': 0.5747373839713094, 'object_color_2': 0.6378153875235385, 'background_color_0': 0.22891349389558047, 'background_color_1': 0.5323773945852492, 'background_color_2': 0.27470468262000647, 'amb_light_color_0': 0.7250056072627092, 'amb_light_color_1': 0.20572151934070904, 'amg_light_color_2': 0.547779213469626, 'light_intensity_1': 0.8880828255665536, 'light_color_0': 0.735908080093145, 'light_color_1': 0.9273935764564314, 'light_color_2': 0.9352153027205867, 'light_dir_1_0': -0.25913714052924797, 'light_dir_1_1': -0.18869335207257254, 'light_dir_1_2': -0.8364770033086639, 'light_intensity_2': 0.12246147776390404, 'light_dir_2_1': -0.34256043930952684, 'light_dir_2_2': -0.6206900507596194, 'light_intensity_3': 0.5783260906122023, 'light_dir_2_0': -0.8852259649994003}. Best is trial 3 with value: 0.016015496104955673.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'camera_distance_factor': 0.4353541176453686, 'camera_angle': 55.0, 'focal_length': 6.315613009928316, 'resolution': 200, 'ambient_coeff': 0.433617795065394, 'diffuse_coeff': 0.9418913474409987, 'specular_coeff': 0.9285968503928825, 'shininess': 1.0, 'object_color_0': 0.4517315870564569, 'object_color_1': 0.6199719391733423, 'object_color_2': 0.23443183909162246, 'background_color_0': 0.7320434468379704, 'background_color_1': 0.4353073213945601, 'background_color_2': 0.25908668217835973, 'amb_light_color_0': 0.5423249780196012, 'amb_light_color_1': 0.5947592253616197, 'amg_light_color_2': 0.7820058417016323, 'light_intensity_1': 1.4417642033897295, 'light_color_0': 0.7052455950247074, 'light_color_1': 0.7482770738590255, 'light_color_2': 0.7893694242467788, 'light_dir_1_0': -0.854814366621506, 'light_dir_1_1': -0.6415108145970294, 'light_dir_1_2': -0.9243215892417174, 'light_intensity_2': 0.7954153059143604, 'light_dir_2_1': -0.7544718994685186, 'light_dir_2_2': -0.8880851680883526, 'light_intensity_3': 0.11189863073293933, 'light_dir_2_0': -0.8076413220647567}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    study = optuna.create_study(storage=\"sqlite:///optuna_study_renderparams.db\", study_name=\"render_params\", direction='maximize', load_if_exists=True)\n",
    "    study.optimize(objective, n_trials=2)\n",
    "        \n",
    "    best_params = study.best_params\n",
    "    print(best_params)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T14:41:34.918489Z",
     "start_time": "2023-11-28T14:21:47.041618Z"
    }
   },
   "id": "8660eafbf534fc18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7a4531cb3695fa6c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
