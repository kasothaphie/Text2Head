{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T06:49:13.870237Z",
     "start_time": "2023-10-27T06:49:05.158350Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/nphm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from NPHM.models.deepSDF import DeepSDF, DeformationNetwork\n",
    "from NPHM.models.EnsembledDeepSDF import FastEnsembleDeepSDFMirrored\n",
    "from NPHM import env_paths\n",
    "from NPHM.utils.reconstruction import create_grid_points_from_bounds, mesh_from_logits\n",
    "from NPHM.models.reconstruction import deform_mesh, get_logits, get_logits_backward\n",
    "from NPHM.models.fitting import inference_iterative_root_finding_joint, inference_identity_space\n",
    "from NPHM.data.manager import DataManager\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "import json, yaml\n",
    "import os\n",
    "import os.path as osp\n",
    "import torch\n",
    "import pyvista as pv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "913c0d883af35f3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T06:49:13.926591Z",
     "start_time": "2023-10-27T06:49:13.878573Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config file from: scripts/configs/fitting_nphm.yaml\n",
      "{\n",
      "    \"checkpoint_expr\": 2500,\n",
      "    \"checkpoint_shape\": 15000,\n",
      "    \"exp_name_expr\": \"nphm_expression_space_pretrained\",\n",
      "    \"exp_name_shape\": \"nphm_identity_space_pretrained\",\n",
      "    \"lambdas_shape\": {\n",
      "        \"reg_global\": 0.002,\n",
      "        \"reg_unobserved\": \"5.0 0\",\n",
      "        \"reg_zero\": 0.005,\n",
      "        \"surface\": 2.0,\n",
      "        \"symm_dist\": 1.0\n",
      "    },\n",
      "    \"local_expr\": false,\n",
      "    \"local_shape\": true,\n",
      "    \"mode\": \"compress\"\n",
      "}\n",
      "Loading config file from: /Users/simonlangrieger/Documents/Universität/Master/6. Semester/Praktikum/NPHM/weights/nphm_identity_space_pretrained/configs.yaml\n"
     ]
    }
   ],
   "source": [
    "resolution = 35\n",
    "\n",
    "with open('scripts/configs/fitting_nphm.yaml', 'r') as f:\n",
    "    print('Loading config file from: ' + 'scripts/configs/fitting_nphm.yaml')\n",
    "    CFG = yaml.safe_load(f)\n",
    "\n",
    "print(json.dumps(CFG, sort_keys=True, indent=4))\n",
    "\n",
    "weight_dir_shape = env_paths.EXPERIMENT_DIR + '/{}/'.format(CFG['exp_name_shape'])\n",
    "\n",
    "# load config files\n",
    "fname_shape = weight_dir_shape + 'configs.yaml'\n",
    "with open(fname_shape, 'r') as f:\n",
    "    print('Loading config file from: ' + fname_shape)\n",
    "    CFG_shape = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7f4a9b3f9a858d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T06:49:13.949789Z",
     "start_time": "2023-10-27T06:49:13.914981Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b95c7271da459bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T06:49:14.039300Z",
     "start_time": "2023-10-27T06:49:13.940804Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###########################################################################\n",
      "####################     Shape Model Configs     #############################\n",
      "###########################################################################\n",
      "{\n",
      "    \"anchors_path\": \"/cluster/daidalos/sgiebenhain/anchors_39.npy\",\n",
      "    \"data\": {\n",
      "        \"n_expressions\": 1,\n",
      "        \"n_subjects\": 300,\n",
      "        \"root\": \"/cluster/angmar/sgiebenhain/synth_data_2neutral\"\n",
      "    },\n",
      "    \"decoder\": {\n",
      "        \"decoder_hidden_dim\": 200,\n",
      "        \"decoder_lat_dim_glob\": 64,\n",
      "        \"decoder_lat_dim_loc\": 32,\n",
      "        \"decoder_nlayers\": 4,\n",
      "        \"decoder_nloc\": 39,\n",
      "        \"decoder_nsymm_pairs\": 16,\n",
      "        \"dropout\": false,\n",
      "        \"type\": \"deep_sdf_decoder\",\n",
      "        \"weight_norm\": false\n",
      "    },\n",
      "    \"encoder\": null,\n",
      "    \"lm_inds_path\": \"/cluster/daidalos/sgiebenhain/lm_inds_39.npy\",\n",
      "    \"training\": {\n",
      "        \"batch_size\": 32,\n",
      "        \"ckpt_interval\": 500,\n",
      "        \"grad_clip\": 0.1,\n",
      "        \"grad_clip_lat\": 0.1,\n",
      "        \"lambdas\": {\n",
      "            \"anchors\": 7.5,\n",
      "            \"grad\": 0.1,\n",
      "            \"lat_reg\": 0.01,\n",
      "            \"middle_dist\": 0.0,\n",
      "            \"normals\": 0.3,\n",
      "            \"space_sdf\": 0.01,\n",
      "            \"surf_sdf\": 2.0,\n",
      "            \"symm_dist\": 0.01\n",
      "        },\n",
      "        \"loss_type\": \"igr\",\n",
      "        \"lr\": 0.0005,\n",
      "        \"lr_decay_factor\": 0.5,\n",
      "        \"lr_decay_factor_lat\": 0.5,\n",
      "        \"lr_decay_interval\": 5000,\n",
      "        \"lr_decay_interval_lat\": 5000,\n",
      "        \"lr_lat\": 0.001,\n",
      "        \"mode\": \"shape_space\",\n",
      "        \"npoints_decoder\": 750,\n",
      "        \"npoints_decoder_non\": 250,\n",
      "        \"sigma_near\": 0.01,\n",
      "        \"weight_decay\": 0.01\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('###########################################################################')\n",
    "print('####################     Shape Model Configs     #############################')\n",
    "print('###########################################################################')\n",
    "print(json.dumps(CFG_shape, sort_keys=True, indent=4))\n",
    "\n",
    "lm_inds = np.load(env_paths.ANCHOR_INDICES_PATH)\n",
    "anchors = torch.from_numpy(np.load(env_paths.ANCHOR_MEAN_PATH)).float().unsqueeze(0).unsqueeze(0).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3fe99f912dbd8da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T06:49:14.348217Z",
     "start_time": "2023-10-27T06:49:13.971819Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from: /Users/simonlangrieger/Documents/Universität/Master/6. Semester/Praktikum/NPHM/weights/nphm_identity_space_pretrained/checkpoints/checkpoint_epoch_15000.tar\n",
      "no latent codes in state dict\n"
     ]
    }
   ],
   "source": [
    "decoder_shape = FastEnsembleDeepSDFMirrored(\n",
    "        lat_dim_glob=CFG_shape['decoder']['decoder_lat_dim_glob'],\n",
    "        lat_dim_loc=CFG_shape['decoder']['decoder_lat_dim_loc'],\n",
    "        hidden_dim=CFG_shape['decoder']['decoder_hidden_dim'],\n",
    "        n_loc=CFG_shape['decoder']['decoder_nloc'],\n",
    "        n_symm_pairs=CFG_shape['decoder']['decoder_nsymm_pairs'],\n",
    "        anchors=anchors,\n",
    "        n_layers=CFG_shape['decoder']['decoder_nlayers'],\n",
    "        pos_mlp_dim=CFG_shape['decoder'].get('pos_mlp_dim', 256),\n",
    "    )\n",
    "\n",
    "decoder_shape = decoder_shape.to(device)\n",
    "\n",
    "path = osp.join(weight_dir_shape, 'checkpoints/checkpoint_epoch_{}.tar'.format(CFG['checkpoint_shape']))\n",
    "print('Loaded checkpoint from: {}'.format(path))\n",
    "checkpoint = torch.load(path, map_location=device)\n",
    "decoder_shape.load_state_dict(checkpoint['decoder_state_dict'], strict=True)\n",
    "\n",
    "if 'latent_codes_state_dict' in checkpoint:\n",
    "    n_train_subjects = checkpoint['latent_codes_state_dict']['weight'].shape[0]\n",
    "    n_val_subjects = checkpoint['latent_codes_val_state_dict']['weight'].shape[0]\n",
    "    latent_codes_shape = torch.nn.Embedding(n_train_subjects, 512)\n",
    "    latent_codes_shape_val = torch.nn.Embedding(n_val_subjects, 512)\n",
    "    \n",
    "    latent_codes_shape.load_state_dict(checkpoint['latent_codes_state_dict'])\n",
    "    latent_codes_shape_val.load_state_dict(checkpoint['latent_codes_val_state_dict'])\n",
    "else:\n",
    "    print('no latent codes in state dict')\n",
    "    latent_codes_shape = None\n",
    "    latent_codes_shape_val = None\n",
    "\n",
    "decoder_expr = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6acc7fb1d2843184",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T06:49:14.419400Z",
     "start_time": "2023-10-27T06:49:14.312237Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1344])\n",
      "[[-0.55       -0.5        -0.95      ]\n",
      " [-0.55       -0.5        -0.91029412]\n",
      " [-0.55       -0.5        -0.87058824]\n",
      " ...\n",
      " [ 0.55        0.75        0.32058824]\n",
      " [ 0.55        0.75        0.36029412]\n",
      " [ 0.55        0.75        0.4       ]]\n",
      "torch.Size([1, 42875, 3])\n"
     ]
    }
   ],
   "source": [
    "lat_mean = torch.from_numpy(np.load(env_paths.ASSETS + 'nphm_lat_mean.npy'))\n",
    "lat_std = torch.from_numpy(np.load(env_paths.ASSETS + 'nphm_lat_std.npy'))\n",
    "\n",
    "lat_rep = (torch.randn(lat_mean.shape) * lat_std * 0.85 + lat_mean)\n",
    "print(lat_rep.shape) #40*32+64\n",
    "\n",
    "mini = [-.55, -.5, -.95]\n",
    "maxi = [0.55, 0.75, 0.4]\n",
    "\n",
    "grid_points = create_grid_points_from_bounds(mini, maxi, resolution)\n",
    "print(grid_points)\n",
    "grid_points = torch.from_numpy(grid_points).to(device, dtype=torch.float)\n",
    "grid_points = torch.reshape(grid_points, (1, len(grid_points), 3)).to(device)\n",
    "print(grid_points.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8122d38cf8eedae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T06:49:22.113215Z",
     "start_time": "2023-10-27T06:49:16.357691Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logits = get_logits(decoder_shape, lat_rep, grid_points, nbatch_points=100)\n",
    "print('starting mcubes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7399f21412db51ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T06:44:24.207808Z",
     "start_time": "2023-10-27T06:44:22.695748Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mesh = mesh_from_logits(logits, mini, maxi, resolution)\n",
    "print('done mcubes')\n",
    "\n",
    "pl = pv.Plotter(off_screen=True)\n",
    "pl.add_mesh(mesh)\n",
    "pl.reset_camera()\n",
    "pl.camera.position = (0, 0, 3)\n",
    "pl.camera.zoom(1.4)\n",
    "pl.set_viewup((0, 1, 0)) #vertical direction of camera = +Y axis\n",
    "pl.camera.view_plane_normal = (-0, -0, 1) #camera is looking at XY plane\n",
    "pl.show()\n",
    "#pl.show(screenshot=out_dir + '/step_{:04d}.png'.format(step))\n",
    "#mesh.export(out_dir + '/mesh_{:04d}.ply'.format(step))\n",
    "print(pl.camera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "203c876657988fdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T11:04:32.622022Z",
     "start_time": "2023-10-27T11:04:20.553768Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from render import render\n",
    "\n",
    "# Define rendering parameters\n",
    "camera_position = torch.tensor([0.0, 0.0, 2.0])\n",
    "max_ray_length = 4.\n",
    "\n",
    "# Define phong model constants\n",
    "ambient_coeff = 0.1\n",
    "diffuse_coeff = 0.6\n",
    "specular_coeff = 0.3\n",
    "shininess = 32.0\n",
    "\n",
    "# Define light inputs\n",
    "light_position = torch.tensor([2.0, 1.0, 3.0])\n",
    "\n",
    "def sdf_nphm(positions):\n",
    "    nphm_input = torch.reshape(positions, (1, -1, 3))\n",
    "    distance, _ = decoder_shape(nphm_input, torch.reshape(lat_rep, (1, 1, -1)), None)\n",
    "    return distance.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab188098",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = render(sdf_nphm, 50, camera_position, light_position, ambient_coeff, diffuse_coeff, specular_coeff, shininess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f4603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rendering parameters\n",
    "res = 50\n",
    "camera_position = torch.tensor([0.0, 0.0, 3.0])\n",
    "max_ray_length = 4 - 2.3\n",
    "\n",
    "# Define phong model constants\n",
    "ambient_coeff = 0.1\n",
    "diffuse_coeff = 0.6\n",
    "specular_coeff = 0.3\n",
    "shininess = 32.0\n",
    "\n",
    "# Define light inputs\n",
    "light_position = torch.tensor([2.0, 1.0, 3.0])\n",
    "\n",
    "# Create an empty image\n",
    "image = torch.zeros((res, res, 3))\n",
    "#image = 0.01 * torch.ones((res, res, 3))\n",
    "\n",
    "def phong_model(normal, light_dir, view_dir):\n",
    "    # Normalize all vectors\n",
    "    normal = normal / torch.norm(normal, dim=-1)\n",
    "    light_dir = light_dir / torch.norm(light_dir, dim=-1)\n",
    "    view_dir = view_dir / torch.norm(view_dir, dim=-1)\n",
    "    \n",
    "    ambient = ambient_coeff\n",
    "    diffuse = diffuse_coeff * torch.clamp(torch.sum(light_dir * normal, dim=-1), min=0.0)\n",
    "    reflect_dir = light_dir - 2 * normal * torch.clamp(torch.sum(light_dir * normal, dim=-1), min=0.0)\n",
    "    specular = specular_coeff * torch.pow(torch.clamp(torch.sum(reflect_dir * view_dir, dim=-1), min=0.0), shininess)\n",
    "\n",
    "    return ambient + diffuse + specular\n",
    "\n",
    "def estimate_normal(sdf, point, epsilon=1e-3):\n",
    "    # Calculate the SDF value at the given point\n",
    "    sdf_value = sdf(point)\n",
    "\n",
    "    # Calculate SDF values at neighboring points\n",
    "    sdf_dx = sdf(point + torch.tensor([epsilon, 0, 0]))\n",
    "    sdf_dy = sdf(point + torch.tensor([0, epsilon, 0]))\n",
    "    sdf_dz = sdf(point + torch.tensor([0, 0, epsilon]))\n",
    "\n",
    "    # Calculate the gradient using finite differences\n",
    "    gradient = torch.tensor([sdf_dx - sdf_value, sdf_dy - sdf_value, sdf_dz - sdf_value])\n",
    "\n",
    "    # Normalize the gradient to obtain the estimated normal\n",
    "    normal = gradient / torch.norm(gradient, p=2)\n",
    "    \n",
    "    return normal\n",
    "\n",
    "def sdf_sphere(position, radius=0.75):\n",
    "    return torch.norm(position, dim=-1) - radius\n",
    "\n",
    "def sdf_nphm(position):\n",
    "    position = position.unsqueeze(0).unsqueeze(0) # [1, N, 3], lat_rep [lat_dim]\n",
    "    distance, _ = decoder_shape(position, lat_rep.repeat(1, position.shape[1], 1), None)\n",
    "    return distance\n",
    "\n",
    "def ray_march(camera_position, direction, max_length):\n",
    "    position = camera_position + 2.3 * direction\n",
    "    step_size = 0.01\n",
    "\n",
    "    for _ in range(int(max_length / step_size)):\n",
    "        #distance = sdf_sphere(position)  # Replace with your SDF function\n",
    "        distance = sdf_nphm(position)\n",
    "        if distance < 0.01:\n",
    "            return position  # Ray hits the surface\n",
    "\n",
    "        position += step_size * direction\n",
    "\n",
    "    return None  # Ray misses the scene\n",
    "\n",
    "# Rendering loop\n",
    "for v in range(res):\n",
    "    for u in range(res):\n",
    "        # Normalize the xy value of the current pixel [-1, 1]\n",
    "        u_norm = (2.0 * (u + 0.5) / res - 1.0)\n",
    "        v_norm = 1.0 - 2.0 * (v + 0.5) / res\n",
    "        u_norm = torch.tensor([u_norm])\n",
    "        v_norm = torch.tensor([v_norm])\n",
    "         # Calculate the ray direction for the current pixel\n",
    "        direction_unn = torch.tensor([u_norm, v_norm, -3.0])\n",
    "        direction = direction_unn / torch.norm(direction_unn, dim=-1)\n",
    "\n",
    "        # Perform ray marching\n",
    "        hit_position = ray_march(camera_position, direction, max_ray_length)\n",
    "\n",
    "        # Color the pixel based on whether the ray hits an object\n",
    "        if hit_position is not None:\n",
    "            normal = estimate_normal(sdf_sphere, hit_position)\n",
    "            light_dir = - (hit_position - light_position) # umdrehen, damit L*V >0\n",
    "            view_dir = - (camera_position - hit_position) # umdrehen, damit dot product nicht kleienr null?\n",
    "            reflection = phong_model(normal, light_dir, view_dir)\n",
    "            # Assign a color for objects\n",
    "            image[v, u] = reflection * torch.tensor([1.0, 1.0, 1.0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b86b8f400259555",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T11:04:37.406375Z",
     "start_time": "2023-10-27T11:04:37.271427Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Display the image using Matplotlib\n",
    "plt.imshow(image.detach().numpy())\n",
    "plt.axis('off')  # Turn off axes\n",
    "plt.show()\n",
    "print(image[:, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f61807e95407c21",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
